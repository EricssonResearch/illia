{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from abc import abstractmethod\n",
    "from typing import Tuple, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seeds for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take care of paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Current working directory :{os.getcwd()}\")\n",
    "print(f\"Current sys path :{sys.path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_path=\"/mnt/c/Users/edanbaz/OneDrive - Ericsson/Documents/projects/illia\"\n",
    "if not os.path.exists(expected_path):\n",
    "    raise Exception(\"Please ensure correct path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(expected_path,'examples'))\n",
    "print(f\"Changed working directory to :{os.getcwd()}\")\n",
    "\n",
    "if expected_path not in sys.path:\n",
    "    sys.path.insert(0, expected_path)\n",
    "    print(\"Added {expected_path} to sys.path\")\n",
    "\n",
    "print(f\"Updated sys.path : {sys.path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test - Load illia and show available backends "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the backend is selected we can import illia, if we want to change the backend we need to restart the kernel. The backend can't be changed dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import illia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the available backends using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illia.show_available_backends()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test - Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test params and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (3, 2)\n",
    "mu_prior = 0.0\n",
    "std_prior = 0.1\n",
    "mu_init = 0.0\n",
    "rho_init = -7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_tensors(a, b, rtol=1e-1, atol=1e-1, name=\"\"):\n",
    "    are_close = np.allclose(a, b, rtol=rtol, atol=atol)\n",
    "    max_diff = np.max(np.abs(a - b))\n",
    "    print(f\"{name} are close: {are_close}\")\n",
    "    print(f\"Max absolute difference for {name}: {max_diff}\")\n",
    "    return are_close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize class distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from illia.torch.distributions.dynamic.gaussian import GaussianDistribution as TorchGaussianDistribution\n",
    "from illia.tf.distributions.dynamic.gaussian import GaussianDistribution  as TFGaussianDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dynamic_dist = TorchGaussianDistribution(\n",
    "    shape=shape, mu_init=mu_init, rho_init=rho_init\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_dynamic_dist = TFGaussianDistribution(\n",
    "    shape=shape, mu_init=mu_init, rho_init=rho_init\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1 - Distributions sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test 1: Sampling\")\n",
    "\n",
    "n_samples = 10000\n",
    "torch_samples = np.array([torch_dynamic_dist.sample().detach().cpu().numpy() for _ in range(n_samples)])\n",
    "tf_samples = np.array([tf_dynamic_dist.sample().numpy() for _ in range(n_samples)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_mean = np.mean(torch_samples, axis=0)\n",
    "tf_mean = np.mean(tf_samples, axis=0)\n",
    "compare_tensors(torch_mean, tf_mean, name=\"Means\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare standard deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_std = np.std(torch_samples, axis=0)\n",
    "tf_std = np.std(tf_samples, axis=0)\n",
    "compare_tensors(torch_std, tf_std, name=\"Standard deviations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2 - Distributions log probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTest 2: Log probability\")\n",
    "\n",
    "x = np.random.randn(*shape).astype(np.float32)\n",
    "torch_log_prob = torch_dynamic_dist.log_prob(torch.tensor(x, dtype=torch.float32)).detach().cpu().numpy()\n",
    "tf_log_prob = tf_dynamic_dist.log_prob(tf.constant(x, dtype=tf.float32)).numpy()\n",
    "compare_tensors(torch_log_prob, tf_log_prob, rtol=1e-1, atol=1e-1, name=\"Log probabilities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3 - Distributions num of params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Number of parameters\n",
    "print(\"\\nTest 3: Number of parameters\")\n",
    "print(\"PyTorch num params:\", torch_dynamic_dist.num_params)\n",
    "print(\"TensorFlow num params:\", tf_dynamic_dist.num_params)\n",
    "print(\"Num params are equal:\", torch_dynamic_dist.num_params == tf_dynamic_dist.num_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prove visually as well if implementations are correct or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.hist(torch_samples.flatten(), bins=50, alpha=0.5, label='PyTorch')\n",
    "plt.hist(tf_samples.flatten(), bins=50, alpha=0.5, label='TensorFlow')\n",
    "plt.legend()\n",
    "plt.xlabel(\"values\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.title(\"Distribution of Samples\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(torch_samples.flatten()[:1000], tf_samples.flatten()[:1000], alpha=0.1)\n",
    "plt.plot([-3, 3], [-3, 3], 'r--')\n",
    "plt.xlabel(\"PyTorch Samples\")\n",
    "plt.ylabel(\"TensorFlow Samples\")\n",
    "plt.title(\"PyTorch vs TensorFlow Samples\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test - Bayesian Module (nn.base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from illia.torch.nn.base import BayesianModule as TorchBayesianModule\n",
    "from illia.tf.nn.base import BayesianModule as TFBayesianModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class definitions - forward method to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchTestModule(TorchBayesianModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(10, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "    def kl_cost(self):\n",
    "        return torch.tensor(1.0), 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFTestModule(TFBayesianModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = tf.keras.layers.Dense(5)\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "    def kl_cost(self):\n",
    "        return tf.constant(1.0), 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_module=TorchTestModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_module=TFTestModule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1 - Bayesion Module freeze and unfreeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_freeze_unfreeze():\n",
    "    \n",
    "    print(\"Testing freeze and unfreeze...\")\n",
    "    \n",
    "    # Test PyTorch module\n",
    "    assert not torch_module.frozen, \"PyTorch module should not be frozen initially\"\n",
    "    torch_module.freeze()\n",
    "\n",
    "    assert torch_module.frozen, \"PyTorch module should be frozen after freeze()\"\n",
    "    torch_module.unfreeze()\n",
    "\n",
    "    assert not torch_module.frozen, \"PyTorch module should not be frozen after unfreeze()\"\n",
    "\n",
    "    # Test TensorFlow module\n",
    "    assert not tf_module.frozen, \"TensorFlow module should not be frozen initially\"\n",
    "    tf_module.freeze()\n",
    "\n",
    "    assert tf_module.frozen, \"TensorFlow module should be frozen after freeze()\"\n",
    "    tf_module.unfreeze()\n",
    "\n",
    "    assert not tf_module.frozen, \"TensorFlow module should not be frozen after unfreeze()\"\n",
    "    \n",
    "    print(\"Freeze and unfreeze test passed!\",'\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2 - Bayesion Module KL cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_kl_cost():\n",
    "\n",
    "    print(\"Testing KL cost...\")\n",
    "    \n",
    "    torch_kl, torch_n = torch_module.kl_cost()\n",
    "    tf_kl, tf_n = tf_module.kl_cost()\n",
    "\n",
    "    print(f'\\nPyTorch : {torch_kl.item()}, {torch_n}')\n",
    "    print(f'TensorFlow : {tf_kl.numpy()}, {tf_n}\\n')\n",
    "\n",
    "    assert torch_kl.item() == tf_kl.numpy(), f\"KL divergence mismatch: PyTorch {torch_kl.item()}, TensorFlow {tf_kl.numpy()}\"\n",
    "    assert torch_n == tf_n, f\"N mismatch: PyTorch {torch_n}, TensorFlow {tf_n}\"\n",
    "    \n",
    "    print(\"KL cost test passed!\",'\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3 - Bayesion Module forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_forward_pass():\n",
    "\n",
    "    print(\"Testing forward pass...\")\n",
    "    \n",
    "    # Input data\n",
    "    input_data = np.random.randn(1, 10).astype(np.float32)\n",
    "    \n",
    "    # PyTorch forward pass\n",
    "    torch_input = torch.from_numpy(input_data)\n",
    "    torch_output = torch_module(torch_input)\n",
    "\n",
    "    # TensorFlow forward pass\n",
    "    tf_input = tf.convert_to_tensor(input_data)\n",
    "    tf_output = tf_module(tf_input)\n",
    "\n",
    "    # Compare outputs\n",
    "    torch_np = torch_output.detach().numpy()\n",
    "    tf_np = tf_output.numpy()\n",
    "    \n",
    "    max_diff = np.max(np.abs(torch_np - tf_np))\n",
    "    print(f\"Maximum absolute difference: {max_diff}\")\n",
    "    \n",
    "    if max_diff > 1e-1:\n",
    "        print(\"\"\"\n",
    "              Warning-Ignore for now: Outputs differ slighlty,this might be due to different \n",
    "              initialization or computational differences between PyTorch and TensorFlow for \n",
    "              torch.nn.Linear && tf.keras.layers.Dense \n",
    "              \"\"\"\n",
    "        )\n",
    "        print(\"PyTorch output:\", torch_np)\n",
    "        print(\"TensorFlow output:\", tf_np)\n",
    "    else:\n",
    "        print(\"Outputs are close enough.\")\n",
    "\n",
    "    # Use a more lenient comparison\n",
    "    np.testing.assert_allclose(torch_np, tf_np, rtol=1, atol=1)\n",
    "    \n",
    "    print(\"Forward pass test passed!\",'\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run all tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_tests():\n",
    "    \n",
    "    test_freeze_unfreeze()\n",
    "    test_kl_cost()\n",
    "    test_forward_pass()\n",
    "\n",
    "run_all_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test - Bayesian Module (nn.losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import specific classes for each backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from illia.torch.nn.losses import (\n",
    "    KLDivergenceLoss as TorchKLDivergenceLoss, \n",
    "    ELBOLoss as TorchELBOLoss \n",
    ")\n",
    "from illia.torch.nn.linear import Linear as TorchLinear\n",
    "\n",
    "from illia.tf.nn.losses import (\n",
    "    KLDivergenceLoss as TFKLDivergenceLoss, \n",
    "    ELBOLoss as TFELBOLoss\n",
    ")\n",
    "from illia.tf.nn.linear import Linear as TFLinear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the classes for each backend defined previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchTestModule(TorchBayesianModule):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = TorchLinear(10, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.linear(x)\n",
    "\n",
    "    def kl_cost(self):\n",
    "\n",
    "        return torch.tensor(1.0), 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFTestModule(TFBayesianModule):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = TFLinear(10, 5)\n",
    "\n",
    "    def call(self, x):\n",
    "\n",
    "        return self.linear(x)\n",
    "\n",
    "    def kl_cost(self):\n",
    "\n",
    "        return tf.constant(1.0), 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_kl_divengence = TorchKLDivergenceLoss()\n",
    "torch_elbo_loss = TorchELBOLoss(loss_function=torch.nn.MSELoss())\n",
    "torch_module = TorchTestModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_kl_divengence = TFKLDivergenceLoss()\n",
    "tf_elbo_loss = TFELBOLoss(loss_function=tf.keras.losses.MeanSquaredError())\n",
    "tf_module = TFTestModule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1 - Check if parameters are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_parameters():\n",
    "\n",
    "    print(\"Check the existence of trainable parameters in the classes...\")\n",
    "\n",
    "    torch_list_parameters = list(torch_module.parameters())\n",
    "    assert len(torch_list_parameters) != 0, \"No parameters availables in TorchTestModule\"\n",
    "\n",
    "    trainable_variables = tf_module.trainable_variables\n",
    "    assert len(trainable_variables) != 0, \"No trainable parameters available in TFTestModule\"\n",
    "    \n",
    "    print(\"Test passed!\",'\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2 - Check losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_forward_losses():\n",
    "\n",
    "    print(\"Check the forward propagation of the loss functions...\")\n",
    "        \n",
    "    # Input data\n",
    "    input_data = np.random.randn(1, 10).astype(np.float32)\n",
    "    y_true = np.random.randn(1, 10).astype(np.float32)\n",
    "    y_pred = np.random.randn(1, 10).astype(np.float32)\n",
    "\n",
    "    # PyTorch forward pass\n",
    "    torch_input = torch.from_numpy(input_data)\n",
    "    torch_output = torch_module(torch_input)\n",
    "    torch_kl_divengence_output = torch_kl_divengence(torch_module)\n",
    "    torch_elbo_loss_output = torch_elbo_loss(torch.from_numpy(y_true), torch.from_numpy(y_pred), torch_module)\n",
    "\n",
    "    # Tensorflow forward pass\n",
    "    tf_input = tf.convert_to_tensor(input_data)\n",
    "    tf_output = tf_module(tf_input)\n",
    "    tf_kl_divergence_output = tf_kl_divengence(tf_module)\n",
    "    tf_elbo_loss_output = tf_elbo_loss(tf.convert_to_tensor(y_true), tf.convert_to_tensor(y_pred), tf_module)\n",
    "\n",
    "    # Assert that the outputs are similar\n",
    "    print(\"Torch output:\", torch_output)\n",
    "    print(\"TensorFlow output:\", tf_output)\n",
    "    print(\"Outputs are similar:\", np.allclose(torch_output.detach().numpy(), tf_output))\n",
    "\n",
    "    print(\"Torch KL divergence output:\", torch_kl_divengence_output)\n",
    "    print(\"TensorFlow KL divergence output:\", tf_kl_divergence_output)\n",
    "    print(\"KL divergence outputs are similar:\", np.allclose(torch_kl_divengence_output.detach().numpy(), tf_kl_divergence_output))\n",
    "\n",
    "    print(\"Torch ELBO loss output:\", torch_elbo_loss_output)\n",
    "    print(\"TensorFlow ELBO loss output:\", tf_elbo_loss_output)\n",
    "    print(\"ELBO loss outputs are similar:\", np.allclose(torch_elbo_loss_output.detach().numpy(), tf_elbo_loss_output))\n",
    "\n",
    "    print(\"Test passed!\",'\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run all tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_tests():\n",
    "    \n",
    "    check_parameters()\n",
    "    check_forward_losses()\n",
    "\n",
    "run_all_tests()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dani",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
