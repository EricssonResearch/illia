{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian base module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial explains the main features of a Bayesian base module. You'll learn how to perform essential tasks, including:\n",
    "\n",
    "+ Freezing and unfreezing layers: controlling which parts of the model are trainable.\n",
    "+ Calculating the KL divergence cost: measuring how much one probability distribution differs from a reference distribution.\n",
    "+ Performing a forward pass: processing input data through the model to get predictions.\n",
    "\n",
    "This guide is designed to help you understand these operations using Illia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, you'll need to import some essential libraries. The specific libraries you use will depend on the backend you've chosen, such as PyTorch, TensorFlow, or Jax. Additionally, you'll need to import NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `test_freeze_unfreeze` function confirms that layers can be accurately frozen and unfrozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_freeze_unfreeze():\n",
    "\n",
    "    print(\"Testing freeze and unfreeze...\")\n",
    "\n",
    "    # Test PyTorch module\n",
    "    assert not torch_module.frozen, \"PyTorch module should not be frozen initially\"\n",
    "    torch_module.freeze()\n",
    "\n",
    "    assert torch_module.frozen, \"PyTorch module should be frozen after freeze()\"\n",
    "    torch_module.unfreeze()\n",
    "\n",
    "    assert (\n",
    "        not torch_module.frozen\n",
    "    ), \"PyTorch module should not be frozen after unfreeze()\"\n",
    "\n",
    "    print(\"Freeze and unfreeze test passed!\", \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``test_kl_cost`` function verifies the calculation of the KL divergence cost, ensuring that all frameworks yield consistent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_kl_cost():\n",
    "\n",
    "    print(\"Testing KL cost...\")\n",
    "\n",
    "    torch_kl, torch_n = torch_module.kl_cost()\n",
    "\n",
    "    print(f\"\\nPyTorch : {torch_kl.item()}, {torch_n}\")\n",
    "    print(\"KL cost test passed!\", \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``test_forward_pass`` function ensures that the forward pass generates similar outputs across different framework models when provided with the same input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_forward_pass():\n",
    "\n",
    "    print(\"Testing forward pass...\")\n",
    "\n",
    "    # Input data\n",
    "    input_data = np.random.randn(1, 10).astype(np.float32)\n",
    "\n",
    "    # PyTorch forward pass\n",
    "    torch_input = torch.from_numpy(input_data)\n",
    "    torch_output = torch_module(torch_input)\n",
    "\n",
    "    print(\"PyTorch output:\", torch_output.detach().numpy())\n",
    "    print(\"Forward pass test passed!\", \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``run_all_tests`` function executes all test functions in sequence to validate the module's functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_tests():\n",
    "\n",
    "    test_freeze_unfreeze()\n",
    "    test_kl_cost()\n",
    "    test_forward_pass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seeds for reproducibility across different runs. This ensures that the results are consistent each time the code is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f97914e9470>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When setting the backend, we import the Illia library, which provides Bayesian module implementations. Note that backend selection requires a kernel restart and cannot be changed dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: 0.0.1, Backend: torch\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"/home/dani/Repositorios/illia/\")\n",
    "os.environ[\"ILLIA_BACKEND\"] = \"torch\"\n",
    "\n",
    "import illia\n",
    "from illia.nn import BayesianModule\n",
    "\n",
    "# Display available backends\n",
    "print(f\"Version: {illia.version()}, Backend: {illia.get_backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create test classes for various frameworks. Each class should implement a simple linear layer and include a method to calculate the KL divergence. These classes will be utilized in testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchTestModule(BayesianModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(10, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "    def kl_cost(self):\n",
    "        return torch.tensor(1.0), 1\n",
    "\n",
    "\n",
    "# PyTorch\n",
    "torch_module = TorchTestModule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run all tests to ensure that the module's functionalities work as expected across backends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing freeze and unfreeze...\n",
      "Freeze and unfreeze test passed! \n",
      "\n",
      "\n",
      "Testing KL cost...\n",
      "\n",
      "PyTorch : 1.0, 1\n",
      "KL cost test passed! \n",
      "\n",
      "\n",
      "Testing forward pass...\n",
      "PyTorch output: [[-0.8272763  -0.8433395   1.076015   -0.88353664 -0.528913  ]]\n",
      "Forward pass test passed! \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_all_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
