{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates the functionality of the distributions module. It covers operations like sampling from distributions, calculating log probabilities, and checking the number of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, you'll need to import some essential libraries. The specific libraries you use will depend on the backend you've chosen, such as PyTorch, TensorFlow, or Jax. Additionally, you'll need to import NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``compare_tensors`` function checks whether two tensors are close in value, given a relative and absolute tolerance. It prints the maximum absolute difference between the tensors and returns whether they are close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_tensors(a, b, rtol=1e-1, atol=1e-1, name=\"\"):\n",
    "    are_close = np.allclose(a, b, rtol=rtol, atol=atol)\n",
    "    max_diff = np.max(np.abs(a - b))\n",
    "    print(f\"{name} are close: {are_close}\")\n",
    "    print(f\"Max absolute difference for {name}: {max_diff}\")\n",
    "    return are_close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seeds for reproducibility. This ensures that the results are consistent each time the code is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When setting the backend, we import the Illia library, which provides Bayesian module implementations. Note that backend selection requires a kernel restart and cannot be changed dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import illia\n",
    "\n",
    "# Display available backends\n",
    "illia.show_available_backends()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test parameters and utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define parameters and utilities that will be used in the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (3, 2)  # Shape of the distribution\n",
    "mu_init = 0.0  # Initial mean\n",
    "rho_init = -7.0  # Initial rho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize class distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and initialize Gaussian distribution classes for the various frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from illia.torch.distributions.dynamic.gaussian import (\n",
    "    GaussianDistribution as TorchGaussianDistribution,\n",
    ")\n",
    "from illia.tf.distributions.dynamic.gaussian import (\n",
    "    GaussianDistribution as TFGaussianDistribution,\n",
    ")\n",
    "\n",
    "# PyTorch\n",
    "torch_dynamic_dist = TorchGaussianDistribution(\n",
    "    shape=shape, mu_init=mu_init, rho_init=rho_init\n",
    ")\n",
    "\n",
    "# Tensorflow\n",
    "tf_dynamic_dist = TFGaussianDistribution(\n",
    "    shape=shape, mu_init=mu_init, rho_init=rho_init\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample from the distributions and compare the means and standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test 1: Sampling\")\n",
    "\n",
    "n_samples = 10000  # Number of samples\n",
    "torch_samples = np.array(\n",
    "    [torch_dynamic_dist.sample().detach().cpu().numpy() for _ in range(n_samples)]\n",
    ")\n",
    "tf_samples = np.array([tf_dynamic_dist.sample().numpy() for _ in range(n_samples)])\n",
    "\n",
    "# Compare means\n",
    "torch_mean = np.mean(torch_samples, axis=0)\n",
    "tf_mean = np.mean(tf_samples, axis=0)\n",
    "compare_tensors(torch_mean, tf_mean, name=\"Means\")\n",
    "\n",
    "# Compare standard deviations\n",
    "torch_std = np.std(torch_samples, axis=0)\n",
    "tf_std = np.std(tf_samples, axis=0)\n",
    "compare_tensors(torch_std, tf_std, name=\"Standard deviations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate and compare the log probabilities of a sample for both distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTest 2: Log probability\")\n",
    "\n",
    "x = np.random.randn(*shape).astype(np.float32)\n",
    "torch_log_prob = (\n",
    "    torch_dynamic_dist.log_prob(torch.tensor(x, dtype=torch.float32))\n",
    "    .detach()\n",
    "    .cpu()\n",
    "    .numpy()\n",
    ")\n",
    "tf_log_prob = tf_dynamic_dist.log_prob(tf.constant(x, dtype=tf.float32)).numpy()\n",
    "compare_tensors(\n",
    "    torch_log_prob, tf_log_prob, rtol=1e-1, atol=1e-1, name=\"Log probabilities\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the number of parameters in both distributions to ensure they match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTest 3: Number of parameters\")\n",
    "print(\"PyTorch num params:\", torch_dynamic_dist.num_params)\n",
    "print(\"TensorFlow num params:\", tf_dynamic_dist.num_params)\n",
    "print(\n",
    "    \"Num params are equal:\", torch_dynamic_dist.num_params == tf_dynamic_dist.num_params\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
