"""
This module contains the code for the Losses.

Provides implementations for the KL divergence loss and the
Evidence Lower Bound (ELBO) loss, used in Bayesian neural
networks.
"""

# Standard libraries
from typing import Callable, Literal

# 3pps
import jax
import jax.numpy as jnp
from flax import nnx

# Own modules
from illia.nn.jax.base import BayesianModule


class KLDivergenceLoss(nnx.Module):
    """
    Computes the KL divergence loss across all Bayesian modules.

    Supports optional weighting and currently only "mean" reduction.
    """

    def __init__(
        self, reduction: Literal["mean"] = "mean", weight: float = 1.0
    ) -> None:
        """
        Initializes the KL divergence loss computation.

        Args:
            reduction: Reduction method for the loss. Only "mean"
                supported.
            weight: Scaling factor applied to the total KL loss.
        """

        # Call super class constructor
        super().__init__()

        # Set attributes
        self.reduction = reduction
        self.weight = weight

    def __call__(self, model: nnx.Module) -> jax.Array:
        """
        Computes KL divergence for all Bayesian modules in the model.

        Args:
            model: NNX model containing Bayesian submodules.

        Returns:
            Scaled KL divergence loss as a scalar array.
        """

        # Init kl cost and params
        kl_global_cost: jax.Array = jnp.array(0.0)
        num_params_global: int = 0

        # Iter over modules
        for _, module in model.iter_modules():
            if isinstance(module, BayesianModule):
                kl_cost, num_params = module.kl_cost()
                kl_global_cost += kl_cost
                num_params_global += num_params

        # Average by the number of parameters
        kl_global_cost /= num_params
        kl_global_cost *= self.weight

        return kl_global_cost


class ELBOLoss(nnx.Module):
    """
    Computes the Evidence Lower Bound (ELBO) loss function.

    Combines a reconstruction loss and the KL divergence using
    Monte Carlo sampling.
    """

    def __init__(
        self,
        loss_function: Callable[[jax.Array, jax.Array], jax.Array],
        num_samples: int = 1,
        kl_weight: float = 1e-3,
    ) -> None:
        """
        Initializes the ELBO loss with sampling and KL scaling.

        Args:
            loss_function: Module for computing reconstruction loss.
            num_samples: Number of MC samples for estimation.
            kl_weight: Weight applied to the KL loss.
        """

        # Call super class constructor
        super().__init__()

        # Set attributes
        self.loss_function = loss_function
        self.num_samples = num_samples
        self.kl_weight = kl_weight
        self.kl_loss = KLDivergenceLoss(weight=kl_weight)

    def __call__(
        self, outputs: jax.Array, targets: jax.Array, model: nnx.Module
    ) -> jax.Array:
        """
        Computes the ELBO loss using MC sampling and KL regularization.

        Args:
            outputs: Predictions generated by the model.
            targets: Ground truth values for training.
            model: Model containing Bayesian modules.

        Returns:
            Average ELBO loss as a scalar array.
        """

        loss_value: jax.Array = jnp.array(0.0)
        for _ in range(self.num_samples):
            loss_value += self.loss_function(outputs, targets) + self.kl_loss(model)

        loss_value /= self.num_samples

        return loss_value
