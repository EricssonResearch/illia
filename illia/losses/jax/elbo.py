# Standard libraries
from typing import Any, Callable

# 3pps
import jax
import jax.numpy as jnp
from flax import nnx

# Own modules
from illia.losses.jax.kl import KLDivergenceLoss


class ELBOLoss(nnx.Module):
    """
    Compute the Evidence Lower Bound (ELBO) loss for Bayesian
    networks. Combines a reconstruction loss with a KL divergence
    term. Monte Carlo sampling can estimate the expected
    reconstruction loss over stochastic layers.

    Notes:
        The KL term is weighted by `kl_weight`. The model is
        assumed to contain Bayesian layers compatible with
        `KLDivergenceLoss`.
    """

    def __init__(
        self,
        loss_function: Callable[[jax.Array, jax.Array], jax.Array],
        num_samples: int = 1,
        kl_weight: float = 1e-3,
        **kwargs: Any,
    ) -> None:
        """
        Initialize the ELBO loss with reconstruction and KL
        components.

        Args:
            loss_function: Function to compute reconstruction
                loss.
            num_samples: Number of Monte Carlo samples used for
                estimation.
            kl_weight: Weight applied to the KL divergence term.
            **kwargs: Extra arguments passed to the base class.

        Returns:
            None
        """

        # Call super class constructor
        super().__init__(**kwargs)

        # Set attributes
        self.loss_function = loss_function
        self.num_samples = num_samples
        self.kl_weight = kl_weight
        self.kl_loss = KLDivergenceLoss(weight=kl_weight)

    def __call__(
        self, outputs: jax.Array, targets: jax.Array, model: nnx.Module
    ) -> jax.Array:
        """
        Compute the ELBO loss with Monte Carlo sampling and KL
        regularization.

        Args:
            outputs: Predictions generated by the model.
            targets: Ground truth values for training.
            model: Model containing Bayesian layers.

        Returns:
            jax.Array: Scalar ELBO loss averaged over samples.

        Notes:
            The loss is averaged over `num_samples` Monte Carlo
            draws.
        """

        loss_value: jax.Array = jnp.array(0.0)
        for _ in range(self.num_samples):
            loss_value += self.loss_function(outputs, targets) + self.kl_loss(model)

        loss_value /= self.num_samples

        return loss_value
