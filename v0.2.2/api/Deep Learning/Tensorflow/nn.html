
<!DOCTYPE html>

<html class="no-js" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<meta content="Framework agnostic Bayesian Neural Network library" name="description"/>
<link href="losses.html" rel="prev"/>
<link href="../../Geometric%20Deep%20Learning/PyG/nn.html" rel="next"/>
<link href="../../../assets/images/white_logo_illia.png" rel="icon"/>
<meta content="mkdocs-1.6.1, mkdocs-material-9.6.16" name="generator"/>
<title>Neural Network Layers - illia</title>
<link href="../../../assets/stylesheets/main.7e37652d.min.css" rel="stylesheet"/>
<link href="../../../assets/stylesheets/palette.06af60db.min.css" rel="stylesheet"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&amp;display=fallback" rel="stylesheet"/>
<style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
<link href="../../../assets/_mkdocstrings.css" rel="stylesheet"/>
<link href="../../../assets/css/custom.css" rel="stylesheet"/>
<script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
</head>
<body data-md-color-accent="black" data-md-color-primary="black" data-md-color-scheme="default" dir="ltr">
<input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
<input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
<label class="md-overlay" for="__drawer"></label>
<div data-md-component="skip">
<a class="md-skip" href="#neural-network-layers">9. 
          Skip to content
        </a>
</div>
<div data-md-component="announce">
</div>
<div data-md-color-scheme="default" data-md-component="outdated" hidden="">
</div>
<header class="md-header" data-md-component="header">
<nav aria-label="Header" class="md-header__inner md-grid">
<a aria-label="illia" class="md-header__button md-logo" data-md-component="logo" href="../../../index.html" title="illia">
<img alt="logo" src="../../../assets/images/white_logo_illia.png"/>
</a>
<label class="md-header__button md-icon" for="__drawer">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg>
</label>
<div class="md-header__title" data-md-component="header-title">
<div class="md-header__ellipsis">
<div class="md-header__topic">
<span class="md-ellipsis">
            illia
          </span>
</div>
<div class="md-header__topic" data-md-component="header-topic">
<span class="md-ellipsis">
            
              Neural Network Layers
            
          </span>
</div>
</div>
</div>
<form class="md-header__option" data-md-component="palette">
<input aria-label="Switch to dark mode" class="md-option" data-md-color-accent="black" data-md-color-media="(prefers-color-scheme: light)" data-md-color-primary="black" data-md-color-scheme="default" id="__palette_0" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_1" hidden="" title="Switch to dark mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"></path></svg>
</label>
<input aria-label="Switch to light mode" class="md-option" data-md-color-accent="black" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-primary="black" data-md-color-scheme="slate" id="__palette_1" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_0" hidden="" title="Switch to light mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"></path></svg>
</label>
</form>
<script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
<label class="md-header__button md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
</label>
<div class="md-search" data-md-component="search" role="dialog">
<label class="md-search__overlay" for="__search"></label>
<div class="md-search__inner" role="search">
<form class="md-search__form" name="search">
<input aria-label="Search" autocapitalize="off" autocomplete="off" autocorrect="off" class="md-search__input" data-md-component="search-query" name="query" placeholder="Search" required="" spellcheck="false" type="text"/>
<label class="md-search__icon md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
</label>
<nav aria-label="Search" class="md-search__options">
<button aria-label="Clear" class="md-search__icon md-icon" tabindex="-1" title="Clear" type="reset">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg>
</button>
</nav>
<div class="md-search__suggest" data-md-component="search-suggest"></div>
</form>
<div class="md-search__output">
<div class="md-search__scrollwrap" data-md-scrollfix="" tabindex="0">
<div class="md-search-result" data-md-component="search-result">
<div class="md-search-result__meta">
            Initializing search
          </div>
<ol class="md-search-result__list" role="presentation"></ol>
</div>
</div>
</div>
</div>
</div>
<div class="md-header__source">
<a class="md-source" data-md-component="source" href="https://github.com/EricssonResearch/illia" title="Go to repository">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2" fill="currentColor"></path></svg>
</div>
<div class="md-source__repository">
    EricssonResearch/illia
  </div>
</a>
</div>
</nav>
</header>
<div class="md-container" data-md-component="container">
<nav aria-label="Tabs" class="md-tabs" data-md-component="tabs">
<div class="md-grid">
<ul class="md-tabs__list">
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../index.html">
        
  
  
    
  
  Home

      </a>
</li>
<li class="md-tabs__item md-tabs__item--active">
<a class="md-tabs__link" href="../Jax/distributions.html">
          
  
  
    
  
  API

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../examples/Computer%20Vision/MNIST%20Bayesian%20CNN.html">
          
  
  
    
  
  Examples

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../../reference/bayesian_neural_networks.html">
          
  
  
    
  
  Reference

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="https://ericsson.github.io/cognitive-labs/">
        
  
  
    
  
  Ericsson Cognitive Labs

      </a>
</li>
</ul>
</div>
</nav>
<main class="md-main" data-md-component="main">
<div class="md-main__inner md-grid">
<div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Navigation" class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" data-md-level="0">
<label class="md-nav__title" for="__drawer">
<a aria-label="illia" class="md-nav__button md-logo" data-md-component="logo" href="../../../index.html" title="illia">
<img alt="logo" src="../../../assets/images/white_logo_illia.png"/>
</a>
    illia
  </label>
<div class="md-nav__source">
<a class="md-source" data-md-component="source" href="https://github.com/EricssonResearch/illia" title="Go to repository">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2" fill="currentColor"></path></svg>
</div>
<div class="md-source__repository">
    EricssonResearch/illia
  </div>
</a>
</div>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../index.html">
<span class="md-ellipsis">
    Home
    
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_2" type="checkbox"/>
<label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
<span class="md-ellipsis">
    API
    
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="true" aria-labelledby="__nav_2_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_2">
<span class="md-nav__icon md-icon"></span>
            API
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_2_1" type="checkbox"/>
<label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="">
<span class="md-ellipsis">
    Deep Learning
    
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="true" aria-labelledby="__nav_2_1_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_2_1">
<span class="md-nav__icon md-icon"></span>
            Deep Learning
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_2_1_1" type="checkbox"/>
<label class="md-nav__link" for="__nav_2_1_1" id="__nav_2_1_1_label" tabindex="0">
<span class="md-ellipsis">
    Jax
    
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_2_1_1_label" class="md-nav" data-md-level="3">
<label class="md-nav__title" for="__nav_2_1_1">
<span class="md-nav__icon md-icon"></span>
            Jax
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../Jax/distributions.html">
<span class="md-ellipsis">
    Distributions
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../Jax/losses.html">
<span class="md-ellipsis">
    Losses
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../Jax/nn.html">
<span class="md-ellipsis">
    Neural Network Layers
    
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_2_1_2" type="checkbox"/>
<label class="md-nav__link" for="__nav_2_1_2" id="__nav_2_1_2_label" tabindex="0">
<span class="md-ellipsis">
    PyTorch
    
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_2_1_2_label" class="md-nav" data-md-level="3">
<label class="md-nav__title" for="__nav_2_1_2">
<span class="md-nav__icon md-icon"></span>
            PyTorch
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../PyTorch/distributions.html">
<span class="md-ellipsis">
    Distributions
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../PyTorch/losses.html">
<span class="md-ellipsis">
    Losses
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../PyTorch/nn.html">
<span class="md-ellipsis">
    Neural Network Layers
    
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_2_1_3" type="checkbox"/>
<label class="md-nav__link" for="__nav_2_1_3" id="__nav_2_1_3_label" tabindex="0">
<span class="md-ellipsis">
    Tensorflow
    
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="true" aria-labelledby="__nav_2_1_3_label" class="md-nav" data-md-level="3">
<label class="md-nav__title" for="__nav_2_1_3">
<span class="md-nav__icon md-icon"></span>
            Tensorflow
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="distributions.html">
<span class="md-ellipsis">
    Distributions
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="losses.html">
<span class="md-ellipsis">
    Losses
    
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--active">
<input class="md-nav__toggle md-toggle" id="__toc" type="checkbox"/>
<label class="md-nav__link md-nav__link--active" for="__toc">
<span class="md-ellipsis">
    Neural Network Layers
    
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<a class="md-nav__link md-nav__link--active" href="nn.html">
<span class="md-ellipsis">
    Neural Network Layers
    
  </span>
</a>
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#illia.nn.tf.base.BayesianModule">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-class"></code> BayesianModule
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#illia.nn.tf.conv1d.Conv1d">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-class"></code> Conv1d
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#illia.nn.tf.conv2d.Conv2d">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-class"></code> Conv2d
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#illia.nn.tf.embedding.Embedding">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-class"></code> Embedding
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#illia.nn.tf.linear.Linear">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-class"></code> Linear
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#illia.nn.tf.lstm.LSTM">
<span class="md-ellipsis">
<code class="doc-symbol doc-symbol-toc doc-symbol-class"></code> LSTM
    </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--section md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_2_2" type="checkbox"/>
<label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="">
<span class="md-ellipsis">
    Geometric Deep Learning
    
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_2_2_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_2_2">
<span class="md-nav__icon md-icon"></span>
            Geometric Deep Learning
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_2_2_1" type="checkbox"/>
<label class="md-nav__link" for="__nav_2_2_1" id="__nav_2_2_1_label" tabindex="0">
<span class="md-ellipsis">
    PyG
    
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_2_2_1_label" class="md-nav" data-md-level="3">
<label class="md-nav__title" for="__nav_2_2_1">
<span class="md-nav__icon md-icon"></span>
            PyG
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../Geometric%20Deep%20Learning/PyG/nn.html">
<span class="md-ellipsis">
    Graph Neural Network Layers
    
  </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_3" type="checkbox"/>
<label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
<span class="md-ellipsis">
    Examples
    
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_3_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_3">
<span class="md-nav__icon md-icon"></span>
            Examples
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_3_1" type="checkbox"/>
<label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
<span class="md-ellipsis">
    Computer Vision
    
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_3_1_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_3_1">
<span class="md-nav__icon md-icon"></span>
            Computer Vision
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../examples/Computer%20Vision/MNIST%20Bayesian%20CNN.html">
<span class="md-ellipsis">
    MNIST Bayesian CNN
    
  </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_4" type="checkbox"/>
<label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
<span class="md-ellipsis">
    Reference
    
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_4_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_4">
<span class="md-nav__icon md-icon"></span>
            Reference
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../reference/bayesian_neural_networks.html">
<span class="md-ellipsis">
    Bayesian Neural Networks
    
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../reference/bibliography.html">
<span class="md-ellipsis">
    Bibliography
    
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="https://ericsson.github.io/cognitive-labs/">
<span class="md-ellipsis">
    Ericsson Cognitive Labs
    
  </span>
</a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-content" data-md-component="content">
<article class="md-content__inner md-typeset">
<h1 id="neural-network-layers"><span class="enumerate-headings-plugin enumerate-heading-plugin">9.</span> Neural Network Layers</h1>
<div class="doc doc-object doc-module">
<div class="doc doc-contents first">
<div class="doc doc-children">
<div class="doc doc-object doc-class">
<h2 class="doc doc-heading" id="illia.nn.tf.base.BayesianModule"><span class="enumerate-headings-plugin enumerate-heading-plugin">9.1</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code> <code>BayesianModule</code>
</h2>
<div class="doc doc-contents">
<p>Abstract base for Bayesian-aware modules in Tensorflow.
Provides mechanisms to track if a module is Bayesian and control
parameter updates through freezing/unfreezing.</p>
<details class="notes" open="">
<summary>Notes</summary>
<p>All derived classes must implement <code>freeze</code> and <code>kl_cost</code> to
handle parameter management and compute the KL divergence cost.</p>
</details>
<details class="quote">
<summary>Source code in <code>illia/nn/tf/base.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@saving</span><span class="o">.</span><span class="n">register_keras_serializable</span><span class="p">(</span><span class="n">package</span><span class="o">=</span><span class="s2">"illia"</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"BayesianModule"</span><span class="p">)</span>
<span class="k">class</span><span class="w"> </span><span class="nc">BayesianModule</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">,</span> <span class="n">ABC</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Abstract base for Bayesian-aware modules in Tensorflow.</span>
<span class="sd">    Provides mechanisms to track if a module is Bayesian and control</span>
<span class="sd">    parameter updates through freezing/unfreezing.</span>

<span class="sd">    Notes:</span>
<span class="sd">        All derived classes must implement `freeze` and `kl_cost` to</span>
<span class="sd">        handle parameter management and compute the KL divergence cost.</span>
<span class="sd">    """</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Initialize the Bayesian module with default flags.</span>
<span class="sd">        Sets `frozen` to False and `is_bayesian` to True.</span>

<span class="sd">        Args:</span>
<span class="sd">            **kwargs: Extra arguments passed to the base class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>
<span class="sd">        """</span>

        <span class="c1"># Call super class constructor</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># Set attributes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">frozen</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_bayesian</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">freeze</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Freeze the module's parameters to stop gradient computation.</span>
<span class="sd">        If weights or biases are not sampled yet, they are sampled first.</span>
<span class="sd">        Once frozen, parameters are not resampled or updated.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Notes:</span>
<span class="sd">            Must be implemented by all subclasses.</span>
<span class="sd">        """</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">unfreeze</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Unfreeze the module by setting its `frozen` flag to False.</span>
<span class="sd">        Allows parameters to be sampled and updated again.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>
<span class="sd">        """</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">frozen</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">kl_cost</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Compute the KL divergence cost for all Bayesian parameters.</span>

<span class="sd">        Returns:</span>
<span class="sd">            tuple[tf.Tensor, int]: A tuple containing the KL divergence</span>
<span class="sd">                cost and the total number of parameters in the layer.</span>

<span class="sd">        Notes:</span>
<span class="sd">            Must be implemented by all subclasses.</span>
<span class="sd">        """</span>
</code></pre></div></td></tr></table></div>
</details>
<div class="doc doc-children">
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="illia.nn.tf.base.BayesianModule.__init__"><span class="enumerate-headings-plugin enumerate-heading-plugin">9.1.1</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>
</h3>
<div class="doc doc-contents">
<p>Initialize the Bayesian module with default flags.
Sets <code>frozen</code> to False and <code>is_bayesian</code> to True.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>**kwargs</code>
</td>
<td>
<code><span title="typing.Any">Any</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Extra arguments passed to the base class.</p>
</div>
</td>
<td>
<code>{}</code>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>None</code>
</td>
<td>
<div class="doc-md-description">
<p>None.</p>
</div>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>illia/nn/tf/base.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Initialize the Bayesian module with default flags.</span>
<span class="sd">    Sets `frozen` to False and `is_bayesian` to True.</span>

<span class="sd">    Args:</span>
<span class="sd">        **kwargs: Extra arguments passed to the base class.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>
<span class="sd">    """</span>

    <span class="c1"># Call super class constructor</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="c1"># Set attributes</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">frozen</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">is_bayesian</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="illia.nn.tf.base.BayesianModule.freeze"><span class="enumerate-headings-plugin enumerate-heading-plugin">9.1.2</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <code class="highlight language-python"><span class="n">freeze</span><span class="p">()</span></code>
<span class="doc doc-labels">
<small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
</span>
</h3>
<div class="doc doc-contents">
<p>Freeze the module's parameters to stop gradient computation.
If weights or biases are not sampled yet, they are sampled first.
Once frozen, parameters are not resampled or updated.</p>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>None</code>
</td>
<td>
<div class="doc-md-description">
<p>None.</p>
</div>
</td>
</tr>
</tbody>
</table>
<details class="notes" open="">
<summary>Notes</summary>
<p>Must be implemented by all subclasses.</p>
</details>
<details class="quote">
<summary>Source code in <code>illia/nn/tf/base.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@abstractmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">freeze</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Freeze the module's parameters to stop gradient computation.</span>
<span class="sd">    If weights or biases are not sampled yet, they are sampled first.</span>
<span class="sd">    Once frozen, parameters are not resampled or updated.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Notes:</span>
<span class="sd">        Must be implemented by all subclasses.</span>
<span class="sd">    """</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="illia.nn.tf.base.BayesianModule.kl_cost"><span class="enumerate-headings-plugin enumerate-heading-plugin">9.1.3</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <code class="highlight language-python"><span class="n">kl_cost</span><span class="p">()</span></code>
<span class="doc doc-labels">
<small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
</span>
</h3>
<div class="doc doc-contents">
<p>Compute the KL divergence cost for all Bayesian parameters.</p>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code><span title="tuple">tuple</span>[<span title="tensorflow.Tensor">Tensor</span>, <span title="int">int</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>tuple[tf.Tensor, int]: A tuple containing the KL divergence
cost and the total number of parameters in the layer.</p>
</div>
</td>
</tr>
</tbody>
</table>
<details class="notes" open="">
<summary>Notes</summary>
<p>Must be implemented by all subclasses.</p>
</details>
<details class="quote">
<summary>Source code in <code>illia/nn/tf/base.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@abstractmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">kl_cost</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Compute the KL divergence cost for all Bayesian parameters.</span>

<span class="sd">    Returns:</span>
<span class="sd">        tuple[tf.Tensor, int]: A tuple containing the KL divergence</span>
<span class="sd">            cost and the total number of parameters in the layer.</span>

<span class="sd">    Notes:</span>
<span class="sd">        Must be implemented by all subclasses.</span>
<span class="sd">    """</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="illia.nn.tf.base.BayesianModule.unfreeze"><span class="enumerate-headings-plugin enumerate-heading-plugin">9.1.4</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <code class="highlight language-python"><span class="n">unfreeze</span><span class="p">()</span></code>
</h3>
<div class="doc doc-contents">
<p>Unfreeze the module by setting its <code>frozen</code> flag to False.
Allows parameters to be sampled and updated again.</p>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>None</code>
</td>
<td>
<div class="doc-md-description">
<p>None.</p>
</div>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>illia/nn/tf/base.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">unfreeze</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Unfreeze the module by setting its `frozen` flag to False.</span>
<span class="sd">    Allows parameters to be sampled and updated again.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>
<span class="sd">    """</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">frozen</span> <span class="o">=</span> <span class="kc">False</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="doc doc-object doc-module">
<div class="doc doc-contents first">
<div class="doc doc-children">
<div class="doc doc-object doc-class">
<h2 class="doc doc-heading" id="illia.nn.tf.conv1d.Conv1d"><span class="enumerate-headings-plugin enumerate-heading-plugin">9.2</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code> <code>Conv1d</code>
</h2>
<div class="doc doc-contents">
<p>Bayesian 1D convolutional layer with optional weight and bias priors.
Behaves like a standard Conv1d but treats weights and bias as random
variables sampled from specified distributions. Parameters become fixed
when the layer is frozen.</p>
<details class="quote">
<summary>Source code in <code>illia/nn/tf/conv1d.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@saving</span><span class="o">.</span><span class="n">register_keras_serializable</span><span class="p">(</span><span class="n">package</span><span class="o">=</span><span class="s2">"illia"</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"Conv1d"</span><span class="p">)</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Conv1d</span><span class="p">(</span><span class="n">BayesianModule</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Bayesian 1D convolutional layer with optional weight and bias priors.</span>
<span class="sd">    Behaves like a standard Conv1d but treats weights and bias as random</span>
<span class="sd">    variables sampled from specified distributions. Parameters become fixed</span>
<span class="sd">    when the layer is frozen.</span>
<span class="sd">    """</span>

    <span class="n">bias_distribution</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GaussianDistribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">output_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"VALID"</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">data_format</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"NWC"</span><span class="p">,</span>
        <span class="n">weights_distribution</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GaussianDistribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">bias_distribution</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GaussianDistribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Initializes a Bayesian 1D convolutional layer.</span>

<span class="sd">        Args:</span>
<span class="sd">            input_channels: Number of channels in the input.</span>
<span class="sd">            output_channels: Number of channels produced by the conv.</span>
<span class="sd">            kernel_size: Size of the convolution kernel.</span>
<span class="sd">            stride: Stride of the convolution.</span>
<span class="sd">            padding: Padding type, 'VALID' or 'SAME'.</span>
<span class="sd">            dilation: Spacing between kernel elements.</span>
<span class="sd">            groups: Number of blocked connections between input/output.</span>
<span class="sd">            data_format: 'NWC' or 'NCW' format for input data.</span>
<span class="sd">            weights_distribution: Distribution for weights sampling.</span>
<span class="sd">            bias_distribution: Distribution for bias sampling.</span>
<span class="sd">            use_bias: Whether to include a bias term.</span>
<span class="sd">            **kwargs: Extra arguments passed to the base class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Notes:</span>
<span class="sd">            Gaussian distributions are used by default if none are</span>
<span class="sd">            provided.</span>
<span class="sd">        """</span>

        <span class="c1"># Call super class constructor</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># Check data format</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_params</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">groups</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">data_format</span><span class="p">)</span>

        <span class="c1"># Set attributes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_channels</span> <span class="o">=</span> <span class="n">input_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_channels</span> <span class="o">=</span> <span class="n">output_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">kernel_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">padding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">dilation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="o">=</span> <span class="n">groups</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="o">=</span> <span class="n">use_bias</span>

        <span class="c1"># Adjust the weights distribution based on the channel format</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">"NWC"</span> <span class="k">if</span> <span class="n">data_format</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">data_format</span> <span class="o">==</span> <span class="s2">"NWC"</span> <span class="k">else</span> <span class="s2">"NCW"</span>
        <span class="p">)</span>

        <span class="c1"># Get the weights distribution shape, needs to be channel last</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_weights_distribution_shape</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">input_channels</span> <span class="o">//</span> <span class="n">groups</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">output_channels</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Set weights distribution</span>
        <span class="k">if</span> <span class="n">weights_distribution</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span> <span class="o">=</span> <span class="n">GaussianDistribution</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_weights_distribution_shape</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span> <span class="o">=</span> <span class="n">weights_distribution</span>

        <span class="c1"># Set bias distribution</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">bias_distribution</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="o">=</span> <span class="n">GaussianDistribution</span><span class="p">((</span><span class="n">output_channels</span><span class="p">,))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="o">=</span> <span class="n">bias_distribution</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_check_params</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">data_format</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Validates convolution parameters for correctness.</span>

<span class="sd">        Args:</span>
<span class="sd">            kernel_size: Convolution kernel size.</span>
<span class="sd">            groups: Number of blocked connections.</span>
<span class="sd">            stride: Convolution stride.</span>
<span class="sd">            dilation: Spacing between kernel elements.</span>
<span class="sd">            data_format: 'NWC' or 'NCW' for input tensor.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If any parameter is invalid.</span>
<span class="sd">        """</span>

        <span class="k">if</span> <span class="n">kernel_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">kernel_size</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">kernel_size</span> <span class="o">%</span> <span class="n">groups</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">"Invalid `kernel_size`: </span><span class="si">{</span><span class="n">kernel_size</span><span class="si">}</span><span class="s2">. Must be &gt; 0 "</span>
                <span class="sa">f</span><span class="s2">"and divisible by `groups` </span><span class="si">{</span><span class="n">groups</span><span class="si">}</span><span class="s2">."</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">groups</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Invalid `groups`: </span><span class="si">{</span><span class="n">groups</span><span class="si">}</span><span class="s2">. Must be &gt; 0."</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">s</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">stride</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">"`stride` </span><span class="si">{</span><span class="n">stride</span><span class="si">}</span><span class="s2"> cannot contain 0."</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">max</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dilation</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">max</span><span class="p">(</span><span class="n">dilation</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">"`stride` </span><span class="si">{</span><span class="n">stride</span><span class="si">}</span><span class="s2"> &gt; 1 not allowed with `dilation` </span><span class="si">{</span><span class="n">dilation</span><span class="si">}</span><span class="s2"> &gt; 1."</span>
                <span class="p">)</span>
        <span class="k">if</span> <span class="n">data_format</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">{</span><span class="s2">"NWC"</span><span class="p">,</span> <span class="s2">"NCW"</span><span class="p">}:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">"Invalid `data_format`: </span><span class="si">{</span><span class="n">data_format</span><span class="si">}</span><span class="s2">. Must be 'NWC' or 'NCW'."</span>
            <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Build trainable and non-trainable parameters.</span>

<span class="sd">        Args:</span>
<span class="sd">            input_shape: Input shape used to trigger layer build.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>
<span class="sd">        """</span>

        <span class="c1"># Register non-trainable variables</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">"weights"</span><span class="p">,</span>
            <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="p">),</span>
            <span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_weights_distribution_shape</span><span class="p">,</span>
            <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
                <span class="n">name</span><span class="o">=</span><span class="s2">"bias"</span><span class="p">,</span>
                <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                <span class="p">),</span>
                <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_channels</span><span class="p">,),</span>
                <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Call super-class build method</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Return the configuration dictionary for serialization.</span>

<span class="sd">        Returns:</span>
<span class="sd">            dict: Dictionary with the layer configuration.</span>
<span class="sd">        """</span>

        <span class="c1"># Get the base configuration</span>
        <span class="n">base_config</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>

        <span class="c1"># Add the custom configurations</span>
        <span class="n">custom_config</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">"input_channels"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_channels</span><span class="p">,</span>
            <span class="s2">"output_channels"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_channels</span><span class="p">,</span>
            <span class="s2">"kernel_size"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span>
            <span class="s2">"stride"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
            <span class="s2">"padding"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
            <span class="s2">"dilation"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
            <span class="s2">"groups"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
            <span class="s2">"data_format"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="c1"># Combine both configurations</span>
        <span class="k">return</span> <span class="p">{</span><span class="o">**</span><span class="n">base_config</span><span class="p">,</span> <span class="o">**</span><span class="n">custom_config</span><span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_conv1d</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">weight</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">padding</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">data_format</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"NWC"</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Performs a 1D convolution using provided weights.</span>

<span class="sd">        Args:</span>
<span class="sd">            inputs: Input tensor.</span>
<span class="sd">            weight: Convolutional kernel tensor.</span>
<span class="sd">            stride: Convolution stride.</span>
<span class="sd">            padding: Padding strategy 'VALID' or 'SAME'.</span>
<span class="sd">            data_format: 'NWC' or 'NCW' input format.</span>
<span class="sd">            dilation: Spacing between kernel elements.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor after 1D convolution.</span>
<span class="sd">        """</span>

        <span class="n">output</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">conv1d</span><span class="p">(</span>
            <span class="nb">input</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
            <span class="n">filters</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
            <span class="n">data_format</span><span class="o">=</span><span class="n">data_format</span><span class="p">,</span>
            <span class="n">dilations</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">freeze</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Freeze the module's parameters to stop gradient computation.</span>
<span class="sd">        If weights or biases are not sampled yet, they are sampled first.</span>
<span class="sd">        Once frozen, parameters are not resampled or updated.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>
<span class="sd">        """</span>

        <span class="c1"># Set indicator</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">frozen</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># Sample weights if they are undefined</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

        <span class="c1"># Sample bias is they are undefined</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

        <span class="c1"># Stop gradient computation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">kl_cost</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Compute the KL divergence cost for all Bayesian parameters.</span>

<span class="sd">        Returns:</span>
<span class="sd">            tuple[tf.Tensor, int]: A tuple containing the KL divergence</span>
<span class="sd">                cost and the total number of parameters in the layer.</span>
<span class="sd">        """</span>

        <span class="c1"># Compute log probs</span>
        <span class="n">log_probs</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>

        <span class="c1"># Add bias log probs only if using bias</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">log_probs</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>

        <span class="c1"># Compute number of parameters</span>
        <span class="n">num_params</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span><span class="o">.</span><span class="n">num_params</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">num_params</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span><span class="o">.</span><span class="n">num_params</span>

        <span class="k">return</span> <span class="n">log_probs</span><span class="p">,</span> <span class="n">num_params</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Performs a forward pass through the Bayesian Convolution 1D</span>
<span class="sd">        layer. If the layer is not frozen, it samples weights and bias</span>
<span class="sd">        from their respective distributions. If the layer is frozen</span>
<span class="sd">        and the weights or bias are not initialized, it also performs</span>
<span class="sd">        sampling.</span>

<span class="sd">        Args:</span>
<span class="sd">            inputs: Input tensor to the layer with shape</span>
<span class="sd">                (batch, length, output_channels) if 'data_format' is</span>
<span class="sd">                'NWC' or (batch, output_channels, length) if</span>
<span class="sd">                'data_format' is 'NCW'</span>

<span class="sd">        Returns:</span>
<span class="sd">            Output tensor after convolution with optional bias added.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the layer is frozen but weights or bias are</span>
<span class="sd">                undefined.</span>
<span class="sd">        """</span>

        <span class="c1"># Check if layer is frozen</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">frozen</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

            <span class="c1"># Sample bias only if using bias</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">"Module has been frozen with undefined weights and/or bias."</span>
            <span class="p">)</span>

        <span class="c1"># Compute outputs</span>
        <span class="n">outputs</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_conv1d</span><span class="p">(</span>
            <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
            <span class="n">weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
            <span class="n">data_format</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">data_format</span><span class="p">,</span>
            <span class="n">dilation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Add bias only if using bias</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span>
                <span class="n">value</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span>
                <span class="n">bias</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">,</span>
                <span class="n">data_format</span><span class="o">=</span><span class="s2">"N..C"</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span> <span class="o">==</span> <span class="s2">"NWC"</span> <span class="k">else</span> <span class="s2">"NC.."</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
</details>
<div class="doc doc-children">
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="illia.nn.tf.conv1d.Conv1d.__init__"><span class="enumerate-headings-plugin enumerate-heading-plugin">9.2.1</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">'VALID'</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s1">'NWC'</span><span class="p">,</span> <span class="n">weights_distribution</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bias_distribution</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>
</h3>
<div class="doc doc-contents">
<p>Initializes a Bayesian 1D convolutional layer.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>input_channels</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Number of channels in the input.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>output_channels</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Number of channels produced by the conv.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>kernel_size</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Size of the convolution kernel.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>stride</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Stride of the convolution.</p>
</div>
</td>
<td>
<code>1</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>padding</code>
</td>
<td>
<code><span title="str">str</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Padding type, 'VALID' or 'SAME'.</p>
</div>
</td>
<td>
<code>'VALID'</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>dilation</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Spacing between kernel elements.</p>
</div>
</td>
<td>
<code>1</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>groups</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Number of blocked connections between input/output.</p>
</div>
</td>
<td>
<code>1</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>data_format</code>
</td>
<td>
<code><span title="typing.Optional">Optional</span>[<span title="str">str</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>'NWC' or 'NCW' format for input data.</p>
</div>
</td>
<td>
<code>'NWC'</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>weights_distribution</code>
</td>
<td>
<code><span title="typing.Optional">Optional</span>[<a class="autorefs autorefs-internal" href="distributions.html#illia.distributions.tf.gaussian.GaussianDistribution">GaussianDistribution</a>]</code>
</td>
<td>
<div class="doc-md-description">
<p>Distribution for weights sampling.</p>
</div>
</td>
<td>
<code>None</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>bias_distribution</code>
</td>
<td>
<code><span title="typing.Optional">Optional</span>[<a class="autorefs autorefs-internal" href="distributions.html#illia.distributions.tf.gaussian.GaussianDistribution">GaussianDistribution</a>]</code>
</td>
<td>
<div class="doc-md-description">
<p>Distribution for bias sampling.</p>
</div>
</td>
<td>
<code>None</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>use_bias</code>
</td>
<td>
<code><span title="bool">bool</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Whether to include a bias term.</p>
</div>
</td>
<td>
<code>True</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>**kwargs</code>
</td>
<td>
<code><span title="typing.Any">Any</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Extra arguments passed to the base class.</p>
</div>
</td>
<td>
<code>{}</code>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>None</code>
</td>
<td>
<div class="doc-md-description">
<p>None.</p>
</div>
</td>
</tr>
</tbody>
</table>
<details class="notes" open="">
<summary>Notes</summary>
<p>Gaussian distributions are used by default if none are
provided.</p>
</details>
<details class="quote">
<summary>Source code in <code>illia/nn/tf/conv1d.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">output_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">padding</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"VALID"</span><span class="p">,</span>
    <span class="n">dilation</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">data_format</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"NWC"</span><span class="p">,</span>
    <span class="n">weights_distribution</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GaussianDistribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">bias_distribution</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GaussianDistribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Initializes a Bayesian 1D convolutional layer.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_channels: Number of channels in the input.</span>
<span class="sd">        output_channels: Number of channels produced by the conv.</span>
<span class="sd">        kernel_size: Size of the convolution kernel.</span>
<span class="sd">        stride: Stride of the convolution.</span>
<span class="sd">        padding: Padding type, 'VALID' or 'SAME'.</span>
<span class="sd">        dilation: Spacing between kernel elements.</span>
<span class="sd">        groups: Number of blocked connections between input/output.</span>
<span class="sd">        data_format: 'NWC' or 'NCW' format for input data.</span>
<span class="sd">        weights_distribution: Distribution for weights sampling.</span>
<span class="sd">        bias_distribution: Distribution for bias sampling.</span>
<span class="sd">        use_bias: Whether to include a bias term.</span>
<span class="sd">        **kwargs: Extra arguments passed to the base class.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Notes:</span>
<span class="sd">        Gaussian distributions are used by default if none are</span>
<span class="sd">        provided.</span>
<span class="sd">    """</span>

    <span class="c1"># Call super class constructor</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="c1"># Check data format</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_check_params</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">groups</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">data_format</span><span class="p">)</span>

    <span class="c1"># Set attributes</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">input_channels</span> <span class="o">=</span> <span class="n">input_channels</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">output_channels</span> <span class="o">=</span> <span class="n">output_channels</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">kernel_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">padding</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">dilation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="o">=</span> <span class="n">groups</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="o">=</span> <span class="n">use_bias</span>

    <span class="c1"># Adjust the weights distribution based on the channel format</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">"NWC"</span> <span class="k">if</span> <span class="n">data_format</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">data_format</span> <span class="o">==</span> <span class="s2">"NWC"</span> <span class="k">else</span> <span class="s2">"NCW"</span>
    <span class="p">)</span>

    <span class="c1"># Get the weights distribution shape, needs to be channel last</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_weights_distribution_shape</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">input_channels</span> <span class="o">//</span> <span class="n">groups</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">output_channels</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Set weights distribution</span>
    <span class="k">if</span> <span class="n">weights_distribution</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span> <span class="o">=</span> <span class="n">GaussianDistribution</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_weights_distribution_shape</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span> <span class="o">=</span> <span class="n">weights_distribution</span>

    <span class="c1"># Set bias distribution</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">bias_distribution</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="o">=</span> <span class="n">GaussianDistribution</span><span class="p">((</span><span class="n">output_channels</span><span class="p">,))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="o">=</span> <span class="n">bias_distribution</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="illia.nn.tf.conv1d.Conv1d.call"><span class="enumerate-headings-plugin enumerate-heading-plugin">9.2.2</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <code class="highlight language-python"><span class="n">call</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span></code>
</h3>
<div class="doc doc-contents">
<p>Performs a forward pass through the Bayesian Convolution 1D
layer. If the layer is not frozen, it samples weights and bias
from their respective distributions. If the layer is frozen
and the weights or bias are not initialized, it also performs
sampling.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>inputs</code>
</td>
<td>
<code><span title="tensorflow.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Input tensor to the layer with shape
(batch, length, output_channels) if 'data_format' is
'NWC' or (batch, output_channels, length) if
'data_format' is 'NCW'</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code><span title="tensorflow.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Output tensor after convolution with optional bias added.</p>
</div>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Raises:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code><span title="ValueError">ValueError</span></code>
</td>
<td>
<div class="doc-md-description">
<p>If the layer is frozen but weights or bias are
undefined.</p>
</div>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>illia/nn/tf/conv1d.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Performs a forward pass through the Bayesian Convolution 1D</span>
<span class="sd">    layer. If the layer is not frozen, it samples weights and bias</span>
<span class="sd">    from their respective distributions. If the layer is frozen</span>
<span class="sd">    and the weights or bias are not initialized, it also performs</span>
<span class="sd">    sampling.</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs: Input tensor to the layer with shape</span>
<span class="sd">            (batch, length, output_channels) if 'data_format' is</span>
<span class="sd">            'NWC' or (batch, output_channels, length) if</span>
<span class="sd">            'data_format' is 'NCW'</span>

<span class="sd">    Returns:</span>
<span class="sd">        Output tensor after convolution with optional bias added.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the layer is frozen but weights or bias are</span>
<span class="sd">            undefined.</span>
<span class="sd">    """</span>

    <span class="c1"># Check if layer is frozen</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">frozen</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

        <span class="c1"># Sample bias only if using bias</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">"Module has been frozen with undefined weights and/or bias."</span>
        <span class="p">)</span>

    <span class="c1"># Compute outputs</span>
    <span class="n">outputs</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_conv1d</span><span class="p">(</span>
        <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
        <span class="n">weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">,</span>
        <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
        <span class="n">data_format</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">data_format</span><span class="p">,</span>
        <span class="n">dilation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Add bias only if using bias</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span>
            <span class="n">value</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">,</span>
            <span class="n">data_format</span><span class="o">=</span><span class="s2">"N..C"</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span> <span class="o">==</span> <span class="s2">"NWC"</span> <span class="k">else</span> <span class="s2">"NC.."</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="illia.nn.tf.conv1d.Conv1d.freeze"><span class="enumerate-headings-plugin enumerate-heading-plugin">9.2.3</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <code class="highlight language-python"><span class="n">freeze</span><span class="p">()</span></code>
</h3>
<div class="doc doc-contents">
<p>Freeze the module's parameters to stop gradient computation.
If weights or biases are not sampled yet, they are sampled first.
Once frozen, parameters are not resampled or updated.</p>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>None</code>
</td>
<td>
<div class="doc-md-description">
<p>None.</p>
</div>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>illia/nn/tf/conv1d.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">freeze</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Freeze the module's parameters to stop gradient computation.</span>
<span class="sd">    If weights or biases are not sampled yet, they are sampled first.</span>
<span class="sd">    Once frozen, parameters are not resampled or updated.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>
<span class="sd">    """</span>

    <span class="c1"># Set indicator</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">frozen</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="c1"># Sample weights if they are undefined</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

    <span class="c1"># Sample bias is they are undefined</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

    <span class="c1"># Stop gradient computation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="illia.nn.tf.conv1d.Conv1d.kl_cost"><span class="enumerate-headings-plugin enumerate-heading-plugin">9.2.4</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <code class="highlight language-python"><span class="n">kl_cost</span><span class="p">()</span></code>
</h3>
<div class="doc doc-contents">
<p>Compute the KL divergence cost for all Bayesian parameters.</p>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code><span title="tuple">tuple</span>[<span title="tensorflow.Tensor">Tensor</span>, <span title="int">int</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>tuple[tf.Tensor, int]: A tuple containing the KL divergence
cost and the total number of parameters in the layer.</p>
</div>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>illia/nn/tf/conv1d.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">kl_cost</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Compute the KL divergence cost for all Bayesian parameters.</span>

<span class="sd">    Returns:</span>
<span class="sd">        tuple[tf.Tensor, int]: A tuple containing the KL divergence</span>
<span class="sd">            cost and the total number of parameters in the layer.</span>
<span class="sd">    """</span>

    <span class="c1"># Compute log probs</span>
    <span class="n">log_probs</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>

    <span class="c1"># Add bias log probs only if using bias</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">log_probs</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>

    <span class="c1"># Compute number of parameters</span>
    <span class="n">num_params</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span><span class="o">.</span><span class="n">num_params</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">num_params</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span><span class="o">.</span><span class="n">num_params</span>

    <span class="k">return</span> <span class="n">log_probs</span><span class="p">,</span> <span class="n">num_params</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="doc doc-object doc-module">
<div class="doc doc-contents first">
<div class="doc doc-children">
<div class="doc doc-object doc-class">
<h2 class="doc doc-heading" id="illia.nn.tf.conv2d.Conv2d"><span class="enumerate-headings-plugin enumerate-heading-plugin">9.3</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code> <code>Conv2d</code>
</h2>
<div class="doc doc-contents">
<p>Bayesian 2D convolutional layer with optional weight and bias priors.
Behaves like a standard Conv2d but treats weights and bias as random
variables sampled from specified distributions. Parameters become fixed
when the layer is frozen.</p>
<details class="quote">
<summary>Source code in <code>illia/nn/tf/conv2d.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@saving</span><span class="o">.</span><span class="n">register_keras_serializable</span><span class="p">(</span><span class="n">package</span><span class="o">=</span><span class="s2">"illia"</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"Conv2d"</span><span class="p">)</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">BayesianModule</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Bayesian 2D convolutional layer with optional weight and bias priors.</span>
<span class="sd">    Behaves like a standard Conv2d but treats weights and bias as random</span>
<span class="sd">    variables sampled from specified distributions. Parameters become fixed</span>
<span class="sd">    when the layer is frozen.</span>
<span class="sd">    """</span>

    <span class="n">bias_distribution</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GaussianDistribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">output_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"VALID"</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">data_format</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"NHWC"</span><span class="p">,</span>
        <span class="n">weights_distribution</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GaussianDistribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">bias_distribution</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GaussianDistribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Initializes a Bayesian 2D convolutional layer.</span>

<span class="sd">        Args:</span>
<span class="sd">            input_channels: Number of channels in the input image.</span>
<span class="sd">            output_channels: Number of channels produced by the conv.</span>
<span class="sd">            kernel_size: Convolution kernel size as int or list.</span>
<span class="sd">            stride: Convolution stride as int or list.</span>
<span class="sd">            padding: Padding type 'VALID', 'SAME', or list of ints.</span>
<span class="sd">            dilation: Spacing between kernel elements as int or list.</span>
<span class="sd">            groups: Number of blocked connections between input/output.</span>
<span class="sd">            data_format: 'NHWC' or 'NCHW' format for input data.</span>
<span class="sd">            weights_distribution: Distribution for weights sampling.</span>
<span class="sd">            bias_distribution: Distribution for bias sampling.</span>
<span class="sd">            use_bias: Whether to include a bias term.</span>
<span class="sd">            **kwargs: Extra arguments passed to the base class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Notes:</span>
<span class="sd">            Gaussian distributions are used by default if none are</span>
<span class="sd">            provided.</span>
<span class="sd">        """</span>

        <span class="c1"># Call super class constructor</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># Check data format</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_params</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">groups</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">data_format</span><span class="p">)</span>

        <span class="c1"># Set attributes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_channels</span> <span class="o">=</span> <span class="n">input_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_channels</span> <span class="o">=</span> <span class="n">output_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">kernel_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">padding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">dilation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="o">=</span> <span class="n">groups</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="o">=</span> <span class="n">use_bias</span>

        <span class="c1"># Check if kernel_size is a list and unpack it if necessary</span>
        <span class="n">kernel_shape</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">kernel_size</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="k">else</span> <span class="p">[</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="c1"># Adjust the weights distribution based on the channel format</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">"NHWC"</span> <span class="k">if</span> <span class="n">data_format</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">data_format</span> <span class="o">==</span> <span class="s2">"NHWC"</span> <span class="k">else</span> <span class="s2">"NCHW"</span>
        <span class="p">)</span>

        <span class="c1"># Set the weights distribution shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_weights_distribution_shape</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">input_channels</span> <span class="o">//</span> <span class="n">groups</span><span class="p">,</span>
            <span class="o">*</span><span class="n">kernel_shape</span><span class="p">,</span>
            <span class="n">output_channels</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Set weights distribution</span>
        <span class="k">if</span> <span class="n">weights_distribution</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span> <span class="o">=</span> <span class="n">GaussianDistribution</span><span class="p">(</span>
                <span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_weights_distribution_shape</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span> <span class="o">=</span> <span class="n">weights_distribution</span>

        <span class="c1"># Set bias distribution</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">bias_distribution</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="o">=</span> <span class="n">GaussianDistribution</span><span class="p">((</span><span class="n">output_channels</span><span class="p">,))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="o">=</span> <span class="n">bias_distribution</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_check_params</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
        <span class="n">data_format</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Validates parameters for the 2D convolution operation.</span>

<span class="sd">        Args:</span>
<span class="sd">            kernel_size: Convolution kernel size.</span>
<span class="sd">            groups: Number of blocked connections.</span>
<span class="sd">            stride: Convolution stride as int or list.</span>
<span class="sd">            dilation: Kernel spacing as int or list.</span>
<span class="sd">            data_format: 'NHWC' or 'NCHW' for input tensor.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If any parameter is invalid.</span>
<span class="sd">        """</span>

        <span class="k">if</span> <span class="n">kernel_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">kernel_size</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">kernel_size</span> <span class="o">%</span> <span class="n">groups</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">"Invalid `kernel_size`: </span><span class="si">{</span><span class="n">kernel_size</span><span class="si">}</span><span class="s2">. Must "</span>
                    <span class="sa">f</span><span class="s2">"be &gt; 0 and divisible by `groups` </span><span class="si">{</span><span class="n">groups</span><span class="si">}</span><span class="s2">."</span>
                <span class="p">)</span>
        <span class="k">if</span> <span class="n">groups</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Invalid `groups`: </span><span class="si">{</span><span class="n">groups</span><span class="si">}</span><span class="s2">. Must be &gt; 0."</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">s</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">stride</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">"`stride` </span><span class="si">{</span><span class="n">stride</span><span class="si">}</span><span class="s2"> cannot contain 0."</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">max</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dilation</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">max</span><span class="p">(</span><span class="n">dilation</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">"`stride` </span><span class="si">{</span><span class="n">stride</span><span class="si">}</span><span class="s2"> &gt; 1 not allowed with "</span>
                    <span class="sa">f</span><span class="s2">"`dilation` </span><span class="si">{</span><span class="n">dilation</span><span class="si">}</span><span class="s2"> &gt; 1."</span>
                <span class="p">)</span>
        <span class="k">if</span> <span class="n">data_format</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">{</span><span class="s2">"NHWC"</span><span class="p">,</span> <span class="s2">"NCHW"</span><span class="p">}:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">"Invalid `data_format`: </span><span class="si">{</span><span class="n">data_format</span><span class="si">}</span><span class="s2">. Must be 'NHWC' or 'NCHW'."</span>
            <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Build trainable and non-trainable parameters.</span>

<span class="sd">        Args:</span>
<span class="sd">            input_shape: Input shape used to trigger layer build.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>
<span class="sd">        """</span>

        <span class="c1"># Register non-trainable variables</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">"weights"</span><span class="p">,</span>
            <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="p">),</span>
            <span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_weights_distribution_shape</span><span class="p">,</span>
            <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
                <span class="n">name</span><span class="o">=</span><span class="s2">"bias"</span><span class="p">,</span>
                <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                <span class="p">),</span>
                <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_channels</span><span class="p">,),</span>
                <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Call super-class build method</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Return the configuration dictionary for serialization.</span>

<span class="sd">        Returns:</span>
<span class="sd">            dict: Dictionary with the layer configuration.</span>
<span class="sd">        """</span>

        <span class="c1"># Get the base configuration</span>
        <span class="n">base_config</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>

        <span class="c1"># Add the custom configurations</span>
        <span class="n">custom_config</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">"input_channels"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_channels</span><span class="p">,</span>
            <span class="s2">"output_channels"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_channels</span><span class="p">,</span>
            <span class="s2">"kernel_size"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span>
            <span class="s2">"stride"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
            <span class="s2">"padding"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
            <span class="s2">"dilation"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
            <span class="s2">"groups"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
            <span class="s2">"data_format"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="c1"># Combine both configurations</span>
        <span class="k">return</span> <span class="p">{</span><span class="o">**</span><span class="n">base_config</span><span class="p">,</span> <span class="o">**</span><span class="n">custom_config</span><span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_conv2d</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">weight</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">padding</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">data_format</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"NHWC"</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Performs a 2D convolution using provided weights.</span>

<span class="sd">        Args:</span>
<span class="sd">            inputs: Input tensor.</span>
<span class="sd">            weight: Convolutional kernel tensor.</span>
<span class="sd">            stride: Convolution stride as int or list.</span>
<span class="sd">            padding: Padding type 'VALID', 'SAME', or list of ints.</span>
<span class="sd">            data_format: 'NHWC' or 'NCHW' input format.</span>
<span class="sd">            dilation: Kernel spacing as int or list.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor after 2D convolution.</span>
<span class="sd">        """</span>

        <span class="n">output</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span>
            <span class="nb">input</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
            <span class="n">filters</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span>
            <span class="n">strides</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
            <span class="n">data_format</span><span class="o">=</span><span class="n">data_format</span><span class="p">,</span>
            <span class="n">dilations</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">freeze</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Freeze the module's parameters to stop gradient computation.</span>
<span class="sd">        If weights or biases are not sampled yet, they are sampled first.</span>
<span class="sd">        Once frozen, parameters are not resampled or updated.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>
<span class="sd">        """</span>

        <span class="c1"># Set indicator</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">frozen</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># Sample weights if they are undefined</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

        <span class="c1"># Sample bias is they are undefined</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

        <span class="c1"># Stop gradient computation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">kl_cost</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Compute the KL divergence cost for all Bayesian parameters.</span>

<span class="sd">        Returns:</span>
<span class="sd">            tuple[tf.Tensor, int]: A tuple containing the KL divergence</span>
<span class="sd">                cost and the total number of parameters in the layer.</span>
<span class="sd">        """</span>

        <span class="c1"># Compute log probs</span>
        <span class="n">log_probs</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>

        <span class="c1"># Add bias log probs only if using bias</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">log_probs</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>

        <span class="c1"># Compute number of parameters</span>
        <span class="n">num_params</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span><span class="o">.</span><span class="n">num_params</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">num_params</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span><span class="o">.</span><span class="n">num_params</span>

        <span class="k">return</span> <span class="n">log_probs</span><span class="p">,</span> <span class="n">num_params</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Performs a forward pass through the Bayesian Convolution 2D</span>
<span class="sd">        layer. If the layer is not frozen, it samples weights and bias</span>
<span class="sd">        from their respective distributions. If the layer is frozen</span>
<span class="sd">        and the weights or bias are not initialized, it also performs</span>
<span class="sd">        sampling.</span>

<span class="sd">        Args:</span>
<span class="sd">            inputs: Input tensor with shape [batch, height, width,</span>
<span class="sd">                channels] if NHWC or [batch, channels, height, width] if NCHW.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Output tensor after convolution with optional bias added.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the layer is frozen but weights or bias are</span>
<span class="sd">                undefined.</span>
<span class="sd">        """</span>

        <span class="c1"># Check if layer is frozen</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">frozen</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

            <span class="c1"># Sample bias only if using bias</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">"Module has been frozen with undefined weights and/or bias."</span>
            <span class="p">)</span>

        <span class="c1"># Compute outputs</span>
        <span class="n">outputs</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_conv2d</span><span class="p">(</span>
            <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
            <span class="n">weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
            <span class="n">data_format</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">data_format</span><span class="p">,</span>
            <span class="n">dilation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Add bias only if using bias</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span>
                <span class="n">value</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span>
                <span class="n">bias</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">,</span>
                <span class="n">data_format</span><span class="o">=</span><span class="s2">"N..C"</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span> <span class="o">==</span> <span class="s2">"NHWC"</span> <span class="k">else</span> <span class="s2">"NC.."</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
</details>
<div class="doc doc-children">
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="illia.nn.tf.conv2d.Conv2d.__init__"><span class="enumerate-headings-plugin enumerate-heading-plugin">9.3.1</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">'VALID'</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s1">'NHWC'</span><span class="p">,</span> <span class="n">weights_distribution</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bias_distribution</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>
</h3>
<div class="doc doc-contents">
<p>Initializes a Bayesian 2D convolutional layer.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>input_channels</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Number of channels in the input image.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>output_channels</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Number of channels produced by the conv.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>kernel_size</code>
</td>
<td>
<code><span title="int">int</span> | <span title="list">list</span>[<span title="int">int</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>Convolution kernel size as int or list.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>stride</code>
</td>
<td>
<code><span title="int">int</span> | <span title="list">list</span>[<span title="int">int</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>Convolution stride as int or list.</p>
</div>
</td>
<td>
<code>1</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>padding</code>
</td>
<td>
<code><span title="str">str</span> | <span title="list">list</span>[<span title="int">int</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>Padding type 'VALID', 'SAME', or list of ints.</p>
</div>
</td>
<td>
<code>'VALID'</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>dilation</code>
</td>
<td>
<code><span title="typing.Optional">Optional</span>[<span title="int">int</span> | <span title="list">list</span>[<span title="int">int</span>]]</code>
</td>
<td>
<div class="doc-md-description">
<p>Spacing between kernel elements as int or list.</p>
</div>
</td>
<td>
<code>None</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>groups</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Number of blocked connections between input/output.</p>
</div>
</td>
<td>
<code>1</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>data_format</code>
</td>
<td>
<code><span title="typing.Optional">Optional</span>[<span title="str">str</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>'NHWC' or 'NCHW' format for input data.</p>
</div>
</td>
<td>
<code>'NHWC'</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>weights_distribution</code>
</td>
<td>
<code><span title="typing.Optional">Optional</span>[<a class="autorefs autorefs-internal" href="distributions.html#illia.distributions.tf.gaussian.GaussianDistribution">GaussianDistribution</a>]</code>
</td>
<td>
<div class="doc-md-description">
<p>Distribution for weights sampling.</p>
</div>
</td>
<td>
<code>None</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>bias_distribution</code>
</td>
<td>
<code><span title="typing.Optional">Optional</span>[<a class="autorefs autorefs-internal" href="distributions.html#illia.distributions.tf.gaussian.GaussianDistribution">GaussianDistribution</a>]</code>
</td>
<td>
<div class="doc-md-description">
<p>Distribution for bias sampling.</p>
</div>
</td>
<td>
<code>None</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>use_bias</code>
</td>
<td>
<code><span title="bool">bool</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Whether to include a bias term.</p>
</div>
</td>
<td>
<code>True</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>**kwargs</code>
</td>
<td>
<code><span title="typing.Any">Any</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Extra arguments passed to the base class.</p>
</div>
</td>
<td>
<code>{}</code>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>None</code>
</td>
<td>
<div class="doc-md-description">
<p>None.</p>
</div>
</td>
</tr>
</tbody>
</table>
<details class="notes" open="">
<summary>Notes</summary>
<p>Gaussian distributions are used by default if none are
provided.</p>
</details>
<details class="quote">
<summary>Source code in <code>illia/nn/tf/conv2d.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">output_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">padding</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"VALID"</span><span class="p">,</span>
    <span class="n">dilation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">data_format</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"NHWC"</span><span class="p">,</span>
    <span class="n">weights_distribution</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GaussianDistribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">bias_distribution</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GaussianDistribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Initializes a Bayesian 2D convolutional layer.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_channels: Number of channels in the input image.</span>
<span class="sd">        output_channels: Number of channels produced by the conv.</span>
<span class="sd">        kernel_size: Convolution kernel size as int or list.</span>
<span class="sd">        stride: Convolution stride as int or list.</span>
<span class="sd">        padding: Padding type 'VALID', 'SAME', or list of ints.</span>
<span class="sd">        dilation: Spacing between kernel elements as int or list.</span>
<span class="sd">        groups: Number of blocked connections between input/output.</span>
<span class="sd">        data_format: 'NHWC' or 'NCHW' format for input data.</span>
<span class="sd">        weights_distribution: Distribution for weights sampling.</span>
<span class="sd">        bias_distribution: Distribution for bias sampling.</span>
<span class="sd">        use_bias: Whether to include a bias term.</span>
<span class="sd">        **kwargs: Extra arguments passed to the base class.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Notes:</span>
<span class="sd">        Gaussian distributions are used by default if none are</span>
<span class="sd">        provided.</span>
<span class="sd">    """</span>

    <span class="c1"># Call super class constructor</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="c1"># Check data format</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_check_params</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">groups</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">data_format</span><span class="p">)</span>

    <span class="c1"># Set attributes</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">input_channels</span> <span class="o">=</span> <span class="n">input_channels</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">output_channels</span> <span class="o">=</span> <span class="n">output_channels</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">kernel_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">padding</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">dilation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="o">=</span> <span class="n">groups</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="o">=</span> <span class="n">use_bias</span>

    <span class="c1"># Check if kernel_size is a list and unpack it if necessary</span>
    <span class="n">kernel_shape</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">kernel_size</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="k">else</span> <span class="p">[</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">]</span>
    <span class="p">)</span>

    <span class="c1"># Adjust the weights distribution based on the channel format</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">"NHWC"</span> <span class="k">if</span> <span class="n">data_format</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">data_format</span> <span class="o">==</span> <span class="s2">"NHWC"</span> <span class="k">else</span> <span class="s2">"NCHW"</span>
    <span class="p">)</span>

    <span class="c1"># Set the weights distribution shape</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_weights_distribution_shape</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">input_channels</span> <span class="o">//</span> <span class="n">groups</span><span class="p">,</span>
        <span class="o">*</span><span class="n">kernel_shape</span><span class="p">,</span>
        <span class="n">output_channels</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Set weights distribution</span>
    <span class="k">if</span> <span class="n">weights_distribution</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span> <span class="o">=</span> <span class="n">GaussianDistribution</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_weights_distribution_shape</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span> <span class="o">=</span> <span class="n">weights_distribution</span>

    <span class="c1"># Set bias distribution</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">bias_distribution</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="o">=</span> <span class="n">GaussianDistribution</span><span class="p">((</span><span class="n">output_channels</span><span class="p">,))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="o">=</span> <span class="n">bias_distribution</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="illia.nn.tf.conv2d.Conv2d.call"><span class="enumerate-headings-plugin enumerate-heading-plugin">9.3.2</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <code class="highlight language-python"><span class="n">call</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span></code>
</h3>
<div class="doc doc-contents">
<p>Performs a forward pass through the Bayesian Convolution 2D
layer. If the layer is not frozen, it samples weights and bias
from their respective distributions. If the layer is frozen
and the weights or bias are not initialized, it also performs
sampling.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>inputs</code>
</td>
<td>
<code><span title="tensorflow.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Input tensor with shape [batch, height, width,
channels] if NHWC or [batch, channels, height, width] if NCHW.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code><span title="tensorflow.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Output tensor after convolution with optional bias added.</p>
</div>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Raises:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code><span title="ValueError">ValueError</span></code>
</td>
<td>
<div class="doc-md-description">
<p>If the layer is frozen but weights or bias are
undefined.</p>
</div>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>illia/nn/tf/conv2d.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Performs a forward pass through the Bayesian Convolution 2D</span>
<span class="sd">    layer. If the layer is not frozen, it samples weights and bias</span>
<span class="sd">    from their respective distributions. If the layer is frozen</span>
<span class="sd">    and the weights or bias are not initialized, it also performs</span>
<span class="sd">    sampling.</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs: Input tensor with shape [batch, height, width,</span>
<span class="sd">            channels] if NHWC or [batch, channels, height, width] if NCHW.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Output tensor after convolution with optional bias added.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the layer is frozen but weights or bias are</span>
<span class="sd">            undefined.</span>
<span class="sd">    """</span>

    <span class="c1"># Check if layer is frozen</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">frozen</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

        <span class="c1"># Sample bias only if using bias</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">"Module has been frozen with undefined weights and/or bias."</span>
        <span class="p">)</span>

    <span class="c1"># Compute outputs</span>
    <span class="n">outputs</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_conv2d</span><span class="p">(</span>
        <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
        <span class="n">weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">,</span>
        <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
        <span class="n">data_format</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">data_format</span><span class="p">,</span>
        <span class="n">dilation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Add bias only if using bias</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span>
            <span class="n">value</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">,</span>
            <span class="n">data_format</span><span class="o">=</span><span class="s2">"N..C"</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span> <span class="o">==</span> <span class="s2">"NHWC"</span> <span class="k">else</span> <span class="s2">"NC.."</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="illia.nn.tf.conv2d.Conv2d.freeze"><span class="enumerate-headings-plugin enumerate-heading-plugin">9.3.3</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <code class="highlight language-python"><span class="n">freeze</span><span class="p">()</span></code>
</h3>
<div class="doc doc-contents">
<p>Freeze the module's parameters to stop gradient computation.
If weights or biases are not sampled yet, they are sampled first.
Once frozen, parameters are not resampled or updated.</p>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>None</code>
</td>
<td>
<div class="doc-md-description">
<p>None.</p>
</div>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>illia/nn/tf/conv2d.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">freeze</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Freeze the module's parameters to stop gradient computation.</span>
<span class="sd">    If weights or biases are not sampled yet, they are sampled first.</span>
<span class="sd">    Once frozen, parameters are not resampled or updated.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>
<span class="sd">    """</span>

    <span class="c1"># Set indicator</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">frozen</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="c1"># Sample weights if they are undefined</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

    <span class="c1"># Sample bias is they are undefined</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

    <span class="c1"># Stop gradient computation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="illia.nn.tf.conv2d.Conv2d.kl_cost"><span class="enumerate-headings-plugin enumerate-heading-plugin">9.3.4</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <code class="highlight language-python"><span class="n">kl_cost</span><span class="p">()</span></code>
</h3>
<div class="doc doc-contents">
<p>Compute the KL divergence cost for all Bayesian parameters.</p>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code><span title="tuple">tuple</span>[<span title="tensorflow.Tensor">Tensor</span>, <span title="int">int</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>tuple[tf.Tensor, int]: A tuple containing the KL divergence
cost and the total number of parameters in the layer.</p>
</div>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>illia/nn/tf/conv2d.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">kl_cost</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Compute the KL divergence cost for all Bayesian parameters.</span>

<span class="sd">    Returns:</span>
<span class="sd">        tuple[tf.Tensor, int]: A tuple containing the KL divergence</span>
<span class="sd">            cost and the total number of parameters in the layer.</span>
<span class="sd">    """</span>

    <span class="c1"># Compute log probs</span>
    <span class="n">log_probs</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>

    <span class="c1"># Add bias log probs only if using bias</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">log_probs</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>

    <span class="c1"># Compute number of parameters</span>
    <span class="n">num_params</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span><span class="o">.</span><span class="n">num_params</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">num_params</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span><span class="o">.</span><span class="n">num_params</span>

    <span class="k">return</span> <span class="n">log_probs</span><span class="p">,</span> <span class="n">num_params</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="doc doc-object doc-module">
<div class="doc doc-contents first">
<div class="doc doc-children">
<div class="doc doc-object doc-class">
<h2 class="doc doc-heading" id="illia.nn.tf.embedding.Embedding"><span class="enumerate-headings-plugin enumerate-heading-plugin">9.4</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code> <code>Embedding</code>
</h2>
<div class="doc doc-contents">
<p>Bayesian embedding layer with optional padding and max-norm. Each
embedding vector is sampled from a specified distribution. Can be
frozen to fix embeddings and stop gradients.</p>
<details class="quote">
<summary>Source code in <code>illia/nn/tf/embedding.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@saving</span><span class="o">.</span><span class="n">register_keras_serializable</span><span class="p">(</span><span class="n">package</span><span class="o">=</span><span class="s2">"illia"</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"Embedding"</span><span class="p">)</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Embedding</span><span class="p">(</span><span class="n">BayesianModule</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Bayesian embedding layer with optional padding and max-norm. Each</span>
<span class="sd">    embedding vector is sampled from a specified distribution. Can be</span>
<span class="sd">    frozen to fix embeddings and stop gradients.</span>
<span class="sd">    """</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">num_embeddings</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">embeddings_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">padding_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">norm_type</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">2.0</span><span class="p">,</span>
        <span class="n">scale_grad_by_freq</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">sparse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">weights_distribution</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GaussianDistribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Initializes a Bayesian Embedding layer.</span>

<span class="sd">        Args:</span>
<span class="sd">            num_embeddings: Size of the embedding dictionary.</span>
<span class="sd">            embeddings_dim: Dimensionality of each embedding vector.</span>
<span class="sd">            padding_idx: Index to exclude from gradient computation.</span>
<span class="sd">            max_norm: Maximum norm for embedding vectors.</span>
<span class="sd">            norm_type: p of the p-norm for max_norm.</span>
<span class="sd">            scale_grad_by_freq: Scale gradient by inverse frequency.</span>
<span class="sd">            sparse: Use sparse gradient updates.</span>
<span class="sd">            weights_distribution: Distribution for embedding weights.</span>
<span class="sd">            **kwargs: Extra arguments passed to the base class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Notes:</span>
<span class="sd">            Gaussian distributions are used by default if none are</span>
<span class="sd">            provided.</span>
<span class="sd">        """</span>

        <span class="c1"># Call super class constructor</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># Set atributtes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_embeddings</span> <span class="o">=</span> <span class="n">num_embeddings</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embeddings_dim</span> <span class="o">=</span> <span class="n">embeddings_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">=</span> <span class="n">padding_idx</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_norm</span> <span class="o">=</span> <span class="n">max_norm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm_type</span> <span class="o">=</span> <span class="n">norm_type</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale_grad_by_freq</span> <span class="o">=</span> <span class="n">scale_grad_by_freq</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sparse</span> <span class="o">=</span> <span class="n">sparse</span>

        <span class="c1"># Set weights distribution</span>
        <span class="k">if</span> <span class="n">weights_distribution</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span> <span class="o">=</span> <span class="n">GaussianDistribution</span><span class="p">(</span>
                <span class="p">(</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="n">embeddings_dim</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span> <span class="o">=</span> <span class="n">weights_distribution</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Build trainable and non-trainable parameters.</span>

<span class="sd">        Args:</span>
<span class="sd">            input_shape: Input shape used to trigger layer build.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>
<span class="sd">        """</span>

        <span class="c1"># Create a variable for weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">"weights"</span><span class="p">,</span>
            <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="p">),</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings_dim</span><span class="p">),</span>
            <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Call super-class build method</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Return the configuration dictionary for serialization.</span>

<span class="sd">        Returns:</span>
<span class="sd">            dict: Dictionary with the layer configuration.</span>
<span class="sd">        """</span>

        <span class="c1"># Get the base configuration</span>
        <span class="n">base_config</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>

        <span class="c1"># Add the custom configurations</span>
        <span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">"num_embeddings"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_embeddings</span><span class="p">,</span>
            <span class="s2">"embeddings_dim"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings_dim</span><span class="p">,</span>
            <span class="s2">"padding_idx"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">,</span>
            <span class="s2">"max_norm"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_norm</span><span class="p">,</span>
            <span class="s2">"norm_type"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_type</span><span class="p">,</span>
            <span class="s2">"scale_grad_by_freq"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_grad_by_freq</span><span class="p">,</span>
            <span class="s2">"sparse"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="c1"># Combine both configurations</span>
        <span class="k">return</span> <span class="p">{</span><span class="o">**</span><span class="n">base_config</span><span class="p">,</span> <span class="o">**</span><span class="n">config</span><span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_embedding</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">weight</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">padding_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">norm_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">2.0</span><span class="p">,</span>
        <span class="n">sparse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Computes embedding lookup with optional padding and normalization.</span>

<span class="sd">        Args:</span>
<span class="sd">            inputs: Input tensor of indices.</span>
<span class="sd">            weight: Embedding weight tensor.</span>
<span class="sd">            padding_idx: Index to mask out.</span>
<span class="sd">            max_norm: Maximum norm for embeddings.</span>
<span class="sd">            norm_type: Norm type for max_norm.</span>
<span class="sd">            sparse: Use sparse lookup if True.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor of embeddings.</span>
<span class="sd">        """</span>

        <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">sparse</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">embeddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">embeddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup_sparse</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">sp_weights</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">padding_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">not_equal</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">padding_idx</span><span class="p">)</span>
            <span class="n">embeddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">padding_mask</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">embeddings</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">max_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">norms</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="n">norm_type</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">desired</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">norms</span><span class="p">,</span> <span class="n">clip_value_min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">clip_value_max</span><span class="o">=</span><span class="n">max_norm</span><span class="p">)</span>
            <span class="n">scale</span> <span class="o">=</span> <span class="n">desired</span> <span class="o">/</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">norms</span><span class="p">,</span> <span class="mf">1e-7</span><span class="p">))</span>
            <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span> <span class="o">*</span> <span class="n">scale</span>

        <span class="k">return</span> <span class="n">embeddings</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">freeze</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Freeze the module's parameters to stop gradient computation.</span>
<span class="sd">        If weights or biases are not sampled yet, they are sampled first.</span>
<span class="sd">        Once frozen, parameters are not resampled or updated.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>
<span class="sd">        """</span>

        <span class="c1"># Set indicator</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">frozen</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># Sample weights if they are undefined</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

        <span class="c1"># Stop gradient computation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">kl_cost</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Compute the KL divergence cost for all Bayesian parameters.</span>

<span class="sd">        Returns:</span>
<span class="sd">            tuple[tf.Tensor, int]: A tuple containing the KL divergence</span>
<span class="sd">                cost and the total number of parameters in the layer.</span>
<span class="sd">        """</span>

        <span class="c1"># Get log probs</span>
        <span class="n">log_probs</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>

        <span class="c1"># Get number of parameters</span>
        <span class="n">num_params</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span><span class="o">.</span><span class="n">num_params</span>

        <span class="k">return</span> <span class="n">log_probs</span><span class="p">,</span> <span class="n">num_params</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Performs embedding lookup using current weights.</span>

<span class="sd">        Args:</span>
<span class="sd">            inputs: Input tensor of indices with shape [batch, *].</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tensor of embeddings.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the layer is frozen but weights are</span>
<span class="sd">                undefined.</span>
<span class="sd">        """</span>

        <span class="c1"># Check if layer is frozen</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">frozen</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"Module has been frozen with undefined weights."</span><span class="p">)</span>

        <span class="c1"># Compute outputs</span>
        <span class="n">outputs</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_embedding</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_norm</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">norm_type</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sparse</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
</details>
<div class="doc doc-children">
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="illia.nn.tf.embedding.Embedding.__init__"><span class="enumerate-headings-plugin enumerate-heading-plugin">9.4.1</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="n">embeddings_dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">norm_type</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">scale_grad_by_freq</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">weights_distribution</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>
</h3>
<div class="doc doc-contents">
<p>Initializes a Bayesian Embedding layer.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>num_embeddings</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Size of the embedding dictionary.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>embeddings_dim</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Dimensionality of each embedding vector.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>padding_idx</code>
</td>
<td>
<code><span title="typing.Optional">Optional</span>[<span title="int">int</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>Index to exclude from gradient computation.</p>
</div>
</td>
<td>
<code>None</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>max_norm</code>
</td>
<td>
<code><span title="typing.Optional">Optional</span>[<span title="float">float</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>Maximum norm for embedding vectors.</p>
</div>
</td>
<td>
<code>None</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>norm_type</code>
</td>
<td>
<code><span title="float">float</span></code>
</td>
<td>
<div class="doc-md-description">
<p>p of the p-norm for max_norm.</p>
</div>
</td>
<td>
<code>2.0</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>scale_grad_by_freq</code>
</td>
<td>
<code><span title="bool">bool</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Scale gradient by inverse frequency.</p>
</div>
</td>
<td>
<code>False</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>sparse</code>
</td>
<td>
<code><span title="bool">bool</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Use sparse gradient updates.</p>
</div>
</td>
<td>
<code>False</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>weights_distribution</code>
</td>
<td>
<code><span title="typing.Optional">Optional</span>[<a class="autorefs autorefs-internal" href="distributions.html#illia.distributions.tf.gaussian.GaussianDistribution">GaussianDistribution</a>]</code>
</td>
<td>
<div class="doc-md-description">
<p>Distribution for embedding weights.</p>
</div>
</td>
<td>
<code>None</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>**kwargs</code>
</td>
<td>
<code><span title="typing.Any">Any</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Extra arguments passed to the base class.</p>
</div>
</td>
<td>
<code>{}</code>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>None</code>
</td>
<td>
<div class="doc-md-description">
<p>None.</p>
</div>
</td>
</tr>
</tbody>
</table>
<details class="notes" open="">
<summary>Notes</summary>
<p>Gaussian distributions are used by default if none are
provided.</p>
</details>
<details class="quote">
<summary>Source code in <code>illia/nn/tf/embedding.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_embeddings</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">embeddings_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">padding_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">norm_type</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">2.0</span><span class="p">,</span>
    <span class="n">scale_grad_by_freq</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">sparse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">weights_distribution</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GaussianDistribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Initializes a Bayesian Embedding layer.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_embeddings: Size of the embedding dictionary.</span>
<span class="sd">        embeddings_dim: Dimensionality of each embedding vector.</span>
<span class="sd">        padding_idx: Index to exclude from gradient computation.</span>
<span class="sd">        max_norm: Maximum norm for embedding vectors.</span>
<span class="sd">        norm_type: p of the p-norm for max_norm.</span>
<span class="sd">        scale_grad_by_freq: Scale gradient by inverse frequency.</span>
<span class="sd">        sparse: Use sparse gradient updates.</span>
<span class="sd">        weights_distribution: Distribution for embedding weights.</span>
<span class="sd">        **kwargs: Extra arguments passed to the base class.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Notes:</span>
<span class="sd">        Gaussian distributions are used by default if none are</span>
<span class="sd">        provided.</span>
<span class="sd">    """</span>

    <span class="c1"># Call super class constructor</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="c1"># Set atributtes</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_embeddings</span> <span class="o">=</span> <span class="n">num_embeddings</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embeddings_dim</span> <span class="o">=</span> <span class="n">embeddings_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">=</span> <span class="n">padding_idx</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_norm</span> <span class="o">=</span> <span class="n">max_norm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">norm_type</span> <span class="o">=</span> <span class="n">norm_type</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">scale_grad_by_freq</span> <span class="o">=</span> <span class="n">scale_grad_by_freq</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">sparse</span> <span class="o">=</span> <span class="n">sparse</span>

    <span class="c1"># Set weights distribution</span>
    <span class="k">if</span> <span class="n">weights_distribution</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span> <span class="o">=</span> <span class="n">GaussianDistribution</span><span class="p">(</span>
            <span class="p">(</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="n">embeddings_dim</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span> <span class="o">=</span> <span class="n">weights_distribution</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="illia.nn.tf.embedding.Embedding.call"><span class="enumerate-headings-plugin enumerate-heading-plugin">9.4.2</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <code class="highlight language-python"><span class="n">call</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span></code>
</h3>
<div class="doc doc-contents">
<p>Performs embedding lookup using current weights.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>inputs</code>
</td>
<td>
<code><span title="tensorflow.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Input tensor of indices with shape [batch, *].</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code><span title="tensorflow.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Tensor of embeddings.</p>
</div>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Raises:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code><span title="ValueError">ValueError</span></code>
</td>
<td>
<div class="doc-md-description">
<p>If the layer is frozen but weights are
undefined.</p>
</div>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>illia/nn/tf/embedding.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Performs embedding lookup using current weights.</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs: Input tensor of indices with shape [batch, *].</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor of embeddings.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the layer is frozen but weights are</span>
<span class="sd">            undefined.</span>
<span class="sd">    """</span>

    <span class="c1"># Check if layer is frozen</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">frozen</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"Module has been frozen with undefined weights."</span><span class="p">)</span>

    <span class="c1"># Compute outputs</span>
    <span class="n">outputs</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_embedding</span><span class="p">(</span>
        <span class="n">inputs</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_norm</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm_type</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sparse</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="illia.nn.tf.embedding.Embedding.freeze"><span class="enumerate-headings-plugin enumerate-heading-plugin">9.4.3</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <code class="highlight language-python"><span class="n">freeze</span><span class="p">()</span></code>
</h3>
<div class="doc doc-contents">
<p>Freeze the module's parameters to stop gradient computation.
If weights or biases are not sampled yet, they are sampled first.
Once frozen, parameters are not resampled or updated.</p>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>None</code>
</td>
<td>
<div class="doc-md-description">
<p>None.</p>
</div>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>illia/nn/tf/embedding.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">freeze</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Freeze the module's parameters to stop gradient computation.</span>
<span class="sd">    If weights or biases are not sampled yet, they are sampled first.</span>
<span class="sd">    Once frozen, parameters are not resampled or updated.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>
<span class="sd">    """</span>

    <span class="c1"># Set indicator</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">frozen</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="c1"># Sample weights if they are undefined</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

    <span class="c1"># Stop gradient computation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="illia.nn.tf.embedding.Embedding.kl_cost"><span class="enumerate-headings-plugin enumerate-heading-plugin">9.4.4</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <code class="highlight language-python"><span class="n">kl_cost</span><span class="p">()</span></code>
</h3>
<div class="doc doc-contents">
<p>Compute the KL divergence cost for all Bayesian parameters.</p>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code><span title="tuple">tuple</span>[<span title="tensorflow.Tensor">Tensor</span>, <span title="int">int</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>tuple[tf.Tensor, int]: A tuple containing the KL divergence
cost and the total number of parameters in the layer.</p>
</div>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>illia/nn/tf/embedding.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">kl_cost</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Compute the KL divergence cost for all Bayesian parameters.</span>

<span class="sd">    Returns:</span>
<span class="sd">        tuple[tf.Tensor, int]: A tuple containing the KL divergence</span>
<span class="sd">            cost and the total number of parameters in the layer.</span>
<span class="sd">    """</span>

    <span class="c1"># Get log probs</span>
    <span class="n">log_probs</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>

    <span class="c1"># Get number of parameters</span>
    <span class="n">num_params</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span><span class="o">.</span><span class="n">num_params</span>

    <span class="k">return</span> <span class="n">log_probs</span><span class="p">,</span> <span class="n">num_params</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="doc doc-object doc-module">
<div class="doc doc-contents first">
<div class="doc doc-children">
<div class="doc doc-object doc-class">
<h2 class="doc doc-heading" id="illia.nn.tf.linear.Linear"><span class="enumerate-headings-plugin enumerate-heading-plugin">9.5</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code> <code>Linear</code>
</h2>
<div class="doc doc-contents">
<p>Bayesian linear layer (fully connected) with optional weight and bias
distributions. Can be frozen to stop gradient updates and fix
parameters.</p>
<details class="quote">
<summary>Source code in <code>illia/nn/tf/linear.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@saving</span><span class="o">.</span><span class="n">register_keras_serializable</span><span class="p">(</span><span class="n">package</span><span class="o">=</span><span class="s2">"illia"</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"Linear"</span><span class="p">)</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Linear</span><span class="p">(</span><span class="n">BayesianModule</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Bayesian linear layer (fully connected) with optional weight and bias</span>
<span class="sd">    distributions. Can be frozen to stop gradient updates and fix</span>
<span class="sd">    parameters.</span>
<span class="sd">    """</span>

    <span class="n">bias_distribution</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GaussianDistribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">output_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">weights_distribution</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GaussianDistribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">bias_distribution</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GaussianDistribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Initializes a Bayesian Linear layer.</span>

<span class="sd">        Args:</span>
<span class="sd">            input_size: Number of input features.</span>
<span class="sd">            output_size: Number of output features.</span>
<span class="sd">            weights_distribution: Distribution for the weights.</span>
<span class="sd">            bias_distribution: Distribution for the bias.</span>
<span class="sd">            use_bias: Whether to include a bias term.</span>
<span class="sd">            **kwargs: Extra arguments passed to the base class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Notes:</span>
<span class="sd">            Gaussian distributions are used by default if none are</span>
<span class="sd">            provided.</span>
<span class="sd">        """</span>

        <span class="c1"># Call super-class constructor</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># Set parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_size</span> <span class="o">=</span> <span class="n">input_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="o">=</span> <span class="n">use_bias</span>

        <span class="c1"># Set weights distribution</span>
        <span class="k">if</span> <span class="n">weights_distribution</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span> <span class="o">=</span> <span class="n">GaussianDistribution</span><span class="p">((</span><span class="n">output_size</span><span class="p">,</span> <span class="n">input_size</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span> <span class="o">=</span> <span class="n">weights_distribution</span>

        <span class="c1"># Set bias distribution</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">bias_distribution</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="o">=</span> <span class="n">GaussianDistribution</span><span class="p">((</span><span class="n">output_size</span><span class="p">,))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="o">=</span> <span class="n">bias_distribution</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Build trainable and non-trainable parameters.</span>

<span class="sd">        Args:</span>
<span class="sd">            input_shape: Input shape used to trigger layer build.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>
<span class="sd">        """</span>

        <span class="c1"># Register non-trainable variables</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">"weights"</span><span class="p">,</span>
            <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="p">),</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_size</span><span class="p">),</span>
            <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
                <span class="n">name</span><span class="o">=</span><span class="s2">"bias"</span><span class="p">,</span>
                <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                <span class="p">),</span>
                <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">,),</span>
                <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Call super-class build method</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Return the configuration dictionary for serialization.</span>

<span class="sd">        Returns:</span>
<span class="sd">            dict: Dictionary with the layer configuration.</span>
<span class="sd">        """</span>

        <span class="c1"># Get the base configuration</span>
        <span class="n">base_config</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>

        <span class="c1"># Add the custom configurations</span>
        <span class="n">custom_config</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">"input_size"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_size</span><span class="p">,</span>
            <span class="s2">"output_size"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="c1"># Combine both configurations</span>
        <span class="k">return</span> <span class="p">{</span><span class="o">**</span><span class="n">base_config</span><span class="p">,</span> <span class="o">**</span><span class="n">custom_config</span><span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">freeze</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Freeze the module's parameters to stop gradient computation.</span>
<span class="sd">        If weights or biases are not sampled yet, they are sampled first.</span>
<span class="sd">        Once frozen, parameters are not resampled or updated.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>
<span class="sd">        """</span>

        <span class="c1"># Set indicator</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">frozen</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># Sample weights if they are undefined</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

        <span class="c1"># Sample bias is they are undefined</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

        <span class="c1"># Stop gradient computation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">kl_cost</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Compute the KL divergence cost for all Bayesian parameters.</span>

<span class="sd">        Returns:</span>
<span class="sd">            tuple[tf.Tensor, int]: A tuple containing the KL divergence</span>
<span class="sd">                cost and the total number of parameters in the layer.</span>
<span class="sd">        """</span>

        <span class="c1"># Compute log probs</span>
        <span class="n">log_probs</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>

        <span class="c1"># Add bias log probs only if using bias</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">log_probs</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>

        <span class="c1"># Compute number of parameters</span>
        <span class="n">num_params</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span><span class="o">.</span><span class="n">num_params</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">num_params</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span><span class="o">.</span><span class="n">num_params</span>

        <span class="k">return</span> <span class="n">log_probs</span><span class="p">,</span> <span class="n">num_params</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Performs forward pass using current weights and bias.</span>

<span class="sd">        Samples parameters if layer is not frozen. Raises an error if</span>
<span class="sd">        frozen weights are undefined.</span>

<span class="sd">        Args:</span>
<span class="sd">            inputs: Input tensor of shape [batch, features].</span>

<span class="sd">        Returns:</span>
<span class="sd">            Output tensor after linear transformation.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the layer is frozen but weights or bias are</span>
<span class="sd">                undefined.</span>
<span class="sd">        """</span>

        <span class="c1"># Check if layer is frozen</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">frozen</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

            <span class="c1"># Sample bias only if using bias</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">"Module has been frozen with undefined weights and/or bias."</span>
            <span class="p">)</span>

        <span class="c1"># Compute outputs</span>
        <span class="n">outputs</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">,</span> <span class="n">transpose_b</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Add bias only if using bias</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
</details>
<div class="doc doc-children">
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="illia.nn.tf.linear.Linear.__init__"><span class="enumerate-headings-plugin enumerate-heading-plugin">9.5.1</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">weights_distribution</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bias_distribution</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>
</h3>
<div class="doc doc-contents">
<p>Initializes a Bayesian Linear layer.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>input_size</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Number of input features.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>output_size</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Number of output features.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>weights_distribution</code>
</td>
<td>
<code><span title="typing.Optional">Optional</span>[<a class="autorefs autorefs-internal" href="distributions.html#illia.distributions.tf.gaussian.GaussianDistribution">GaussianDistribution</a>]</code>
</td>
<td>
<div class="doc-md-description">
<p>Distribution for the weights.</p>
</div>
</td>
<td>
<code>None</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>bias_distribution</code>
</td>
<td>
<code><span title="typing.Optional">Optional</span>[<a class="autorefs autorefs-internal" href="distributions.html#illia.distributions.tf.gaussian.GaussianDistribution">GaussianDistribution</a>]</code>
</td>
<td>
<div class="doc-md-description">
<p>Distribution for the bias.</p>
</div>
</td>
<td>
<code>None</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>use_bias</code>
</td>
<td>
<code><span title="bool">bool</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Whether to include a bias term.</p>
</div>
</td>
<td>
<code>True</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>**kwargs</code>
</td>
<td>
<code><span title="typing.Any">Any</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Extra arguments passed to the base class.</p>
</div>
</td>
<td>
<code>{}</code>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>None</code>
</td>
<td>
<div class="doc-md-description">
<p>None.</p>
</div>
</td>
</tr>
</tbody>
</table>
<details class="notes" open="">
<summary>Notes</summary>
<p>Gaussian distributions are used by default if none are
provided.</p>
</details>
<details class="quote">
<summary>Source code in <code>illia/nn/tf/linear.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">output_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">weights_distribution</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GaussianDistribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">bias_distribution</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GaussianDistribution</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Initializes a Bayesian Linear layer.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_size: Number of input features.</span>
<span class="sd">        output_size: Number of output features.</span>
<span class="sd">        weights_distribution: Distribution for the weights.</span>
<span class="sd">        bias_distribution: Distribution for the bias.</span>
<span class="sd">        use_bias: Whether to include a bias term.</span>
<span class="sd">        **kwargs: Extra arguments passed to the base class.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Notes:</span>
<span class="sd">        Gaussian distributions are used by default if none are</span>
<span class="sd">        provided.</span>
<span class="sd">    """</span>

    <span class="c1"># Call super-class constructor</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="c1"># Set parameters</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">input_size</span> <span class="o">=</span> <span class="n">input_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="o">=</span> <span class="n">use_bias</span>

    <span class="c1"># Set weights distribution</span>
    <span class="k">if</span> <span class="n">weights_distribution</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span> <span class="o">=</span> <span class="n">GaussianDistribution</span><span class="p">((</span><span class="n">output_size</span><span class="p">,</span> <span class="n">input_size</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span> <span class="o">=</span> <span class="n">weights_distribution</span>

    <span class="c1"># Set bias distribution</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">bias_distribution</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="o">=</span> <span class="n">GaussianDistribution</span><span class="p">((</span><span class="n">output_size</span><span class="p">,))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="o">=</span> <span class="n">bias_distribution</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="illia.nn.tf.linear.Linear.call"><span class="enumerate-headings-plugin enumerate-heading-plugin">9.5.2</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <code class="highlight language-python"><span class="n">call</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span></code>
</h3>
<div class="doc doc-contents">
<p>Performs forward pass using current weights and bias.</p>
<p>Samples parameters if layer is not frozen. Raises an error if
frozen weights are undefined.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>inputs</code>
</td>
<td>
<code><span title="tensorflow.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Input tensor of shape [batch, features].</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code><span title="tensorflow.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Output tensor after linear transformation.</p>
</div>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Raises:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code><span title="ValueError">ValueError</span></code>
</td>
<td>
<div class="doc-md-description">
<p>If the layer is frozen but weights or bias are
undefined.</p>
</div>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>illia/nn/tf/linear.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Performs forward pass using current weights and bias.</span>

<span class="sd">    Samples parameters if layer is not frozen. Raises an error if</span>
<span class="sd">    frozen weights are undefined.</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs: Input tensor of shape [batch, features].</span>

<span class="sd">    Returns:</span>
<span class="sd">        Output tensor after linear transformation.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the layer is frozen but weights or bias are</span>
<span class="sd">            undefined.</span>
<span class="sd">    """</span>

    <span class="c1"># Check if layer is frozen</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">frozen</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

        <span class="c1"># Sample bias only if using bias</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">"Module has been frozen with undefined weights and/or bias."</span>
        <span class="p">)</span>

    <span class="c1"># Compute outputs</span>
    <span class="n">outputs</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">,</span> <span class="n">transpose_b</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Add bias only if using bias</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="illia.nn.tf.linear.Linear.freeze"><span class="enumerate-headings-plugin enumerate-heading-plugin">9.5.3</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <code class="highlight language-python"><span class="n">freeze</span><span class="p">()</span></code>
</h3>
<div class="doc doc-contents">
<p>Freeze the module's parameters to stop gradient computation.
If weights or biases are not sampled yet, they are sampled first.
Once frozen, parameters are not resampled or updated.</p>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>None</code>
</td>
<td>
<div class="doc-md-description">
<p>None.</p>
</div>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>illia/nn/tf/linear.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">freeze</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Freeze the module's parameters to stop gradient computation.</span>
<span class="sd">    If weights or biases are not sampled yet, they are sampled first.</span>
<span class="sd">    Once frozen, parameters are not resampled or updated.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>
<span class="sd">    """</span>

    <span class="c1"># Set indicator</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">frozen</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="c1"># Sample weights if they are undefined</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

    <span class="c1"># Sample bias is they are undefined</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

    <span class="c1"># Stop gradient computation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="illia.nn.tf.linear.Linear.kl_cost"><span class="enumerate-headings-plugin enumerate-heading-plugin">9.5.4</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <code class="highlight language-python"><span class="n">kl_cost</span><span class="p">()</span></code>
</h3>
<div class="doc doc-contents">
<p>Compute the KL divergence cost for all Bayesian parameters.</p>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code><span title="tuple">tuple</span>[<span title="tensorflow.Tensor">Tensor</span>, <span title="int">int</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>tuple[tf.Tensor, int]: A tuple containing the KL divergence
cost and the total number of parameters in the layer.</p>
</div>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>illia/nn/tf/linear.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">kl_cost</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Compute the KL divergence cost for all Bayesian parameters.</span>

<span class="sd">    Returns:</span>
<span class="sd">        tuple[tf.Tensor, int]: A tuple containing the KL divergence</span>
<span class="sd">            cost and the total number of parameters in the layer.</span>
<span class="sd">    """</span>

    <span class="c1"># Compute log probs</span>
    <span class="n">log_probs</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>

    <span class="c1"># Add bias log probs only if using bias</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">log_probs</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>

    <span class="c1"># Compute number of parameters</span>
    <span class="n">num_params</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_distribution</span><span class="o">.</span><span class="n">num_params</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">num_params</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_distribution</span><span class="o">.</span><span class="n">num_params</span>

    <span class="k">return</span> <span class="n">log_probs</span><span class="p">,</span> <span class="n">num_params</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="doc doc-object doc-module">
<div class="doc doc-contents first">
<div class="doc doc-children">
<div class="doc doc-object doc-class">
<h2 class="doc doc-heading" id="illia.nn.tf.lstm.LSTM"><span class="enumerate-headings-plugin enumerate-heading-plugin">9.6</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code> <code>LSTM</code>
</h2>
<div class="doc doc-contents">
<p>Bayesian LSTM layer with embedding and probabilistic weights.
All weights and biases are sampled from Gaussian distributions.
Freezing the layer fixes parameters and stops gradient computation.</p>
<details class="quote">
<summary>Source code in <code>illia/nn/tf/lstm.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@saving</span><span class="o">.</span><span class="n">register_keras_serializable</span><span class="p">(</span><span class="n">package</span><span class="o">=</span><span class="s2">"illia"</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"LSTM"</span><span class="p">)</span>
<span class="k">class</span><span class="w"> </span><span class="nc">LSTM</span><span class="p">(</span><span class="n">BayesianModule</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Bayesian LSTM layer with embedding and probabilistic weights.</span>
<span class="sd">    All weights and biases are sampled from Gaussian distributions.</span>
<span class="sd">    Freezing the layer fixes parameters and stops gradient computation.</span>
<span class="sd">    """</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">num_embeddings</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">embeddings_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">output_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">padding_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">norm_type</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">2.0</span><span class="p">,</span>
        <span class="n">scale_grad_by_freq</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">sparse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Initializes the Bayesian LSTM layer.</span>

<span class="sd">        Args:</span>
<span class="sd">            num_embeddings: Size of the embedding dictionary.</span>
<span class="sd">            embeddings_dim: Dimensionality of each embedding vector.</span>
<span class="sd">            hidden_size: Number of hidden units in the LSTM.</span>
<span class="sd">            output_size: Size of the final output.</span>
<span class="sd">            padding_idx: Index to ignore in embeddings.</span>
<span class="sd">            max_norm: Maximum norm for embedding vectors.</span>
<span class="sd">            norm_type: Norm type used for max_norm.</span>
<span class="sd">            scale_grad_by_freq: Scale gradient by inverse frequency.</span>
<span class="sd">            sparse: Use sparse embedding updates.</span>
<span class="sd">            **kwargs: Extra arguments passed to the base class.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>

<span class="sd">        Notes:</span>
<span class="sd">            Gaussian distributions are used by default if none are</span>
<span class="sd">            provided.</span>
<span class="sd">        """</span>

        <span class="c1"># Call super-class constructor</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># Set attributes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_embeddings</span> <span class="o">=</span> <span class="n">num_embeddings</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embeddings_dim</span> <span class="o">=</span> <span class="n">embeddings_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">=</span> <span class="n">padding_idx</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_norm</span> <span class="o">=</span> <span class="n">max_norm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm_type</span> <span class="o">=</span> <span class="n">norm_type</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale_grad_by_freq</span> <span class="o">=</span> <span class="n">scale_grad_by_freq</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sparse</span> <span class="o">=</span> <span class="n">sparse</span>

        <span class="c1"># Define the Embedding layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span>
            <span class="n">num_embeddings</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_embeddings</span><span class="p">,</span>
            <span class="n">embeddings_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">embeddings_dim</span><span class="p">,</span>
            <span class="n">padding_idx</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">,</span>
            <span class="n">max_norm</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_norm</span><span class="p">,</span>
            <span class="n">norm_type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">norm_type</span><span class="p">,</span>
            <span class="n">scale_grad_by_freq</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">scale_grad_by_freq</span><span class="p">,</span>
            <span class="n">sparse</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sparse</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Initialize weight distributions</span>
        <span class="c1"># Forget gate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wf_distribution</span> <span class="o">=</span> <span class="n">GaussianDistribution</span><span class="p">(</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embeddings_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bf_distribution</span> <span class="o">=</span> <span class="n">GaussianDistribution</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,))</span>

        <span class="c1"># Input gate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wi_distribution</span> <span class="o">=</span> <span class="n">GaussianDistribution</span><span class="p">(</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embeddings_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bi_distribution</span> <span class="o">=</span> <span class="n">GaussianDistribution</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,))</span>

        <span class="c1"># Candidate gate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wc_distribution</span> <span class="o">=</span> <span class="n">GaussianDistribution</span><span class="p">(</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embeddings_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bc_distribution</span> <span class="o">=</span> <span class="n">GaussianDistribution</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,))</span>

        <span class="c1"># Output gate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wo_distribution</span> <span class="o">=</span> <span class="n">GaussianDistribution</span><span class="p">(</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embeddings_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bo_distribution</span> <span class="o">=</span> <span class="n">GaussianDistribution</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,))</span>

        <span class="c1"># Final output layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wv_distribution</span> <span class="o">=</span> <span class="n">GaussianDistribution</span><span class="p">(</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bv_distribution</span> <span class="o">=</span> <span class="n">GaussianDistribution</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">,))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Build trainable and non-trainable parameters.</span>

<span class="sd">        Args:</span>
<span class="sd">            input_shape: Input shape used to trigger layer build.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>
<span class="sd">        """</span>

        <span class="c1"># Forget gate weights and bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wf</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">"forget_gate_weights"</span><span class="p">,</span>
            <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wf_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()),</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embeddings_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">),</span>
            <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">bf</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">"forget_gate_bias"</span><span class="p">,</span>
            <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bf_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()),</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,),</span>
            <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Input gate weights and bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wi</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">"input_gate_weights"</span><span class="p">,</span>
            <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wi_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()),</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embeddings_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">),</span>
            <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">bi</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">"input_gate_bias"</span><span class="p">,</span>
            <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bi_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()),</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,),</span>
            <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Candidate gate weights and bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">"candidate_gate_weights"</span><span class="p">,</span>
            <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wc_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()),</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embeddings_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">),</span>
            <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">bc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">"candidate_gate_bias"</span><span class="p">,</span>
            <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bc_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()),</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,),</span>
            <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Output gate weights and bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wo</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">"output_gate_weights"</span><span class="p">,</span>
            <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wo_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()),</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embeddings_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">),</span>
            <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">bo</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">"output_gate_bias"</span><span class="p">,</span>
            <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bo_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()),</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,),</span>
            <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Final output layer weights and bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">"final_output_weights"</span><span class="p">,</span>
            <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wv_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()),</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">),</span>
            <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">bv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">"final_output_bias"</span><span class="p">,</span>
            <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bv_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()),</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">,),</span>
            <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Call super-class build method</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Return the configuration dictionary for serialization.</span>

<span class="sd">        Returns:</span>
<span class="sd">            dict: Dictionary with the layer configuration.</span>
<span class="sd">        """</span>

        <span class="c1"># Get the base configuration</span>
        <span class="n">base_config</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>

        <span class="c1"># Add the custom configurations</span>
        <span class="n">custom_config</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">"num_embeddings"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_embeddings</span><span class="p">,</span>
            <span class="s2">"embeddings_dim"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings_dim</span><span class="p">,</span>
            <span class="s2">"hidden_size"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="s2">"output_size"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">,</span>
            <span class="s2">"padding_idx"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">,</span>
            <span class="s2">"max_norm"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_norm</span><span class="p">,</span>
            <span class="s2">"norm_type"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_type</span><span class="p">,</span>
            <span class="s2">"scale_grad_by_freq"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_grad_by_freq</span><span class="p">,</span>
            <span class="s2">"sparse"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="c1"># Combine both configurations</span>
        <span class="k">return</span> <span class="p">{</span><span class="o">**</span><span class="n">base_config</span><span class="p">,</span> <span class="o">**</span><span class="n">custom_config</span><span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">freeze</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Freeze the module's parameters to stop gradient computation.</span>
<span class="sd">        If weights or biases are not sampled yet, they are sampled first.</span>
<span class="sd">        Once frozen, parameters are not resampled or updated.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None.</span>
<span class="sd">        """</span>

        <span class="c1"># Set indicator</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">frozen</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># Freeze embedding layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>

        <span class="c1"># Forget gate</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">wf</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">wf</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wf_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bf</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bf</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bf_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wf</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wf</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bf</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bf</span><span class="p">)</span>

        <span class="c1"># Input gate</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">wi</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">wi</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wi_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bi</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bi</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bi_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wi</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wi</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bi</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bi</span><span class="p">)</span>

        <span class="c1"># Candidate gate</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">wc</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">wc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wc_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bc</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bc_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wc</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wc</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bc</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bc</span><span class="p">)</span>

        <span class="c1"># Output gate</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">wo</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">wo</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wo_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bo</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bo</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bo_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wo</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wo</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bo</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bo</span><span class="p">)</span>

        <span class="c1"># Final output layer</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">wv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bv</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bv_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wv</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bv</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bv</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">kl_cost</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Compute the KL divergence cost for all Bayesian parameters.</span>

<span class="sd">        Returns:</span>
<span class="sd">            tuple[tf.Tensor, int]: A tuple containing the KL divergence</span>
<span class="sd">                cost and the total number of parameters in the layer.</span>
<span class="sd">        """</span>

        <span class="c1"># Compute log probs for each pair of weights and bias</span>
        <span class="n">log_probs_f</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wf_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">wf</span>
        <span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bf_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bf</span><span class="p">)</span>

        <span class="n">log_probs_i</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wi_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">wi</span>
        <span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bi_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bi</span><span class="p">)</span>

        <span class="n">log_probs_c</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wc_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">wc</span>
        <span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bc_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bc</span><span class="p">)</span>

        <span class="n">log_probs_o</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wo_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">wo</span>
        <span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bo_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bo</span><span class="p">)</span>

        <span class="n">log_probs_v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">wv</span>
        <span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bv_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bv</span><span class="p">)</span>

        <span class="c1"># Compute the total loss</span>
        <span class="n">log_probs</span> <span class="o">=</span> <span class="n">log_probs_f</span> <span class="o">+</span> <span class="n">log_probs_i</span> <span class="o">+</span> <span class="n">log_probs_c</span> <span class="o">+</span> <span class="n">log_probs_o</span> <span class="o">+</span> <span class="n">log_probs_v</span>

        <span class="c1"># Compute number of parameters</span>
        <span class="n">num_params</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">wf_distribution</span><span class="o">.</span><span class="n">num_params</span>
            <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bf_distribution</span><span class="o">.</span><span class="n">num_params</span>
            <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">wi_distribution</span><span class="o">.</span><span class="n">num_params</span>
            <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bi_distribution</span><span class="o">.</span><span class="n">num_params</span>
            <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">wc_distribution</span><span class="o">.</span><span class="n">num_params</span>
            <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bc_distribution</span><span class="o">.</span><span class="n">num_params</span>
            <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">wo_distribution</span><span class="o">.</span><span class="n">num_params</span>
            <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bo_distribution</span><span class="o">.</span><span class="n">num_params</span>
            <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv_distribution</span><span class="o">.</span><span class="n">num_params</span>
            <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bv_distribution</span><span class="o">.</span><span class="n">num_params</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">log_probs</span><span class="p">,</span> <span class="n">num_params</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">call</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">inputs</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">init_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Performs a forward pass through the Bayesian LSTM.</span>

<span class="sd">        Args:</span>
<span class="sd">            inputs: Input tensor of token indices. Shape: [batch, seq_len, 1].</span>
<span class="sd">            init_states: Optional tuple of initial (hidden, cell) states.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple containing:</span>
<span class="sd">                - Output tensor after final linear transformation.</span>
<span class="sd">                - Tuple of final hidden and cell states.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the layer is frozen but weights are</span>
<span class="sd">                undefined.</span>
<span class="sd">        """</span>

        <span class="c1"># Sample weights if not frozen</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">frozen</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">wf</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wf_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bf</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bf_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">wi</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wi_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bi</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bi_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">wc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wc_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bc_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">wo</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wo_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bo</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bo_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">wv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bv_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="k">elif</span> <span class="nb">any</span><span class="p">(</span>
            <span class="n">p</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="p">[</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">wf</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bf</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">wi</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bi</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">wc</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bc</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">wo</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bo</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bv</span><span class="p">,</span>
            <span class="p">]</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">"Module has been frozen with undefined weights and/or bias."</span>
            <span class="p">)</span>

        <span class="c1"># Apply embedding layer to input indices</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">seq_len</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Initialize h_t and c_t if init_states is None</span>
        <span class="k">if</span> <span class="n">init_states</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">h_t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">c_t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">h_t</span><span class="p">,</span> <span class="n">c_t</span> <span class="o">=</span> <span class="n">init_states</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">init_states</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Process sequence</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">):</span>
            <span class="c1"># Shape: (batch_size, embedding_dim)</span>
            <span class="n">x_t</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[:,</span> <span class="n">t</span><span class="p">,</span> <span class="p">:]</span>

            <span class="c1"># Concatenate input and hidden state</span>
            <span class="c1"># Shape: (batch_size, embedding_dim + hidden_size)</span>
            <span class="n">z_t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">x_t</span><span class="p">,</span> <span class="n">h_t</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># Forget gate</span>
            <span class="n">ft</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">z_t</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">wf</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bf</span><span class="p">)</span>

            <span class="c1"># Input gate</span>
            <span class="n">it</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">z_t</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">wi</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bi</span><span class="p">)</span>

            <span class="c1"># Candidate cell state</span>
            <span class="n">can</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">z_t</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">wc</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bc</span><span class="p">)</span>

            <span class="c1"># Output gate</span>
            <span class="n">ot</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">z_t</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">wo</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bo</span><span class="p">)</span>

            <span class="c1"># Update cell state</span>
            <span class="n">c_t</span> <span class="o">=</span> <span class="n">c_t</span> <span class="o">*</span> <span class="n">ft</span> <span class="o">+</span> <span class="n">can</span> <span class="o">*</span> <span class="n">it</span>

            <span class="c1"># Update hidden state</span>
            <span class="n">h_t</span> <span class="o">=</span> <span class="n">ot</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">c_t</span><span class="p">)</span>

        <span class="c1"># Compute final output</span>
        <span class="n">y_t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_t</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bv</span>

        <span class="k">return</span> <span class="n">y_t</span><span class="p">,</span> <span class="p">(</span><span class="n">h_t</span><span class="p">,</span> <span class="n">c_t</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
</details>
<div class="doc doc-children">
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="illia.nn.tf.lstm.LSTM.__init__"><span class="enumerate-headings-plugin enumerate-heading-plugin">9.6.1</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="n">embeddings_dim</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">norm_type</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">scale_grad_by_freq</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>
</h3>
<div class="doc doc-contents">
<p>Initializes the Bayesian LSTM layer.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>num_embeddings</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Size of the embedding dictionary.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>embeddings_dim</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Dimensionality of each embedding vector.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>hidden_size</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Number of hidden units in the LSTM.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>output_size</code>
</td>
<td>
<code><span title="int">int</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Size of the final output.</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>padding_idx</code>
</td>
<td>
<code><span title="typing.Optional">Optional</span>[<span title="int">int</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>Index to ignore in embeddings.</p>
</div>
</td>
<td>
<code>None</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>max_norm</code>
</td>
<td>
<code><span title="typing.Optional">Optional</span>[<span title="float">float</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>Maximum norm for embedding vectors.</p>
</div>
</td>
<td>
<code>None</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>norm_type</code>
</td>
<td>
<code><span title="float">float</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Norm type used for max_norm.</p>
</div>
</td>
<td>
<code>2.0</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>scale_grad_by_freq</code>
</td>
<td>
<code><span title="bool">bool</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Scale gradient by inverse frequency.</p>
</div>
</td>
<td>
<code>False</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>sparse</code>
</td>
<td>
<code><span title="bool">bool</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Use sparse embedding updates.</p>
</div>
</td>
<td>
<code>False</code>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>**kwargs</code>
</td>
<td>
<code><span title="typing.Any">Any</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Extra arguments passed to the base class.</p>
</div>
</td>
<td>
<code>{}</code>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>None</code>
</td>
<td>
<div class="doc-md-description">
<p>None.</p>
</div>
</td>
</tr>
</tbody>
</table>
<details class="notes" open="">
<summary>Notes</summary>
<p>Gaussian distributions are used by default if none are
provided.</p>
</details>
<details class="quote">
<summary>Source code in <code>illia/nn/tf/lstm.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_embeddings</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">embeddings_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">output_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">padding_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">norm_type</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">2.0</span><span class="p">,</span>
    <span class="n">scale_grad_by_freq</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">sparse</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Initializes the Bayesian LSTM layer.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_embeddings: Size of the embedding dictionary.</span>
<span class="sd">        embeddings_dim: Dimensionality of each embedding vector.</span>
<span class="sd">        hidden_size: Number of hidden units in the LSTM.</span>
<span class="sd">        output_size: Size of the final output.</span>
<span class="sd">        padding_idx: Index to ignore in embeddings.</span>
<span class="sd">        max_norm: Maximum norm for embedding vectors.</span>
<span class="sd">        norm_type: Norm type used for max_norm.</span>
<span class="sd">        scale_grad_by_freq: Scale gradient by inverse frequency.</span>
<span class="sd">        sparse: Use sparse embedding updates.</span>
<span class="sd">        **kwargs: Extra arguments passed to the base class.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>

<span class="sd">    Notes:</span>
<span class="sd">        Gaussian distributions are used by default if none are</span>
<span class="sd">        provided.</span>
<span class="sd">    """</span>

    <span class="c1"># Call super-class constructor</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="c1"># Set attributes</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_embeddings</span> <span class="o">=</span> <span class="n">num_embeddings</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embeddings_dim</span> <span class="o">=</span> <span class="n">embeddings_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">=</span> <span class="n">padding_idx</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_norm</span> <span class="o">=</span> <span class="n">max_norm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">norm_type</span> <span class="o">=</span> <span class="n">norm_type</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">scale_grad_by_freq</span> <span class="o">=</span> <span class="n">scale_grad_by_freq</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">sparse</span> <span class="o">=</span> <span class="n">sparse</span>

    <span class="c1"># Define the Embedding layer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span>
        <span class="n">num_embeddings</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_embeddings</span><span class="p">,</span>
        <span class="n">embeddings_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">embeddings_dim</span><span class="p">,</span>
        <span class="n">padding_idx</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">,</span>
        <span class="n">max_norm</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_norm</span><span class="p">,</span>
        <span class="n">norm_type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">norm_type</span><span class="p">,</span>
        <span class="n">scale_grad_by_freq</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">scale_grad_by_freq</span><span class="p">,</span>
        <span class="n">sparse</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sparse</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Initialize weight distributions</span>
    <span class="c1"># Forget gate</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">wf_distribution</span> <span class="o">=</span> <span class="n">GaussianDistribution</span><span class="p">(</span>
        <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embeddings_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bf_distribution</span> <span class="o">=</span> <span class="n">GaussianDistribution</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,))</span>

    <span class="c1"># Input gate</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">wi_distribution</span> <span class="o">=</span> <span class="n">GaussianDistribution</span><span class="p">(</span>
        <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embeddings_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bi_distribution</span> <span class="o">=</span> <span class="n">GaussianDistribution</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,))</span>

    <span class="c1"># Candidate gate</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">wc_distribution</span> <span class="o">=</span> <span class="n">GaussianDistribution</span><span class="p">(</span>
        <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embeddings_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bc_distribution</span> <span class="o">=</span> <span class="n">GaussianDistribution</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,))</span>

    <span class="c1"># Output gate</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">wo_distribution</span> <span class="o">=</span> <span class="n">GaussianDistribution</span><span class="p">(</span>
        <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embeddings_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bo_distribution</span> <span class="o">=</span> <span class="n">GaussianDistribution</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,))</span>

    <span class="c1"># Final output layer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">wv_distribution</span> <span class="o">=</span> <span class="n">GaussianDistribution</span><span class="p">(</span>
        <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bv_distribution</span> <span class="o">=</span> <span class="n">GaussianDistribution</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">,))</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="illia.nn.tf.lstm.LSTM.call"><span class="enumerate-headings-plugin enumerate-heading-plugin">9.6.2</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <code class="highlight language-python"><span class="n">call</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">init_states</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>
</h3>
<div class="doc doc-contents">
<p>Performs a forward pass through the Bayesian LSTM.</p>
<p><span class="doc-section-title">Parameters:</span></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>inputs</code>
</td>
<td>
<code><span title="tensorflow.Tensor">Tensor</span></code>
</td>
<td>
<div class="doc-md-description">
<p>Input tensor of token indices. Shape: [batch, seq_len, 1].</p>
</div>
</td>
<td>
<em>required</em>
</td>
</tr>
<tr class="doc-section-item">
<td>
<code>init_states</code>
</td>
<td>
<code><span title="typing.Optional">Optional</span>[<span title="tuple">tuple</span>[<span title="tensorflow.Tensor">Tensor</span>, <span title="tensorflow.Tensor">Tensor</span>]]</code>
</td>
<td>
<div class="doc-md-description">
<p>Optional tuple of initial (hidden, cell) states.</p>
</div>
</td>
<td>
<code>None</code>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code><span title="tuple">tuple</span>[<span title="tensorflow.Tensor">Tensor</span>, <span title="tuple">tuple</span>[<span title="tensorflow.Tensor">Tensor</span>, <span title="tensorflow.Tensor">Tensor</span>]]</code>
</td>
<td>
<div class="doc-md-description">
<p>Tuple containing:
- Output tensor after final linear transformation.
- Tuple of final hidden and cell states.</p>
</div>
</td>
</tr>
</tbody>
</table>
<p><span class="doc-section-title">Raises:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code><span title="ValueError">ValueError</span></code>
</td>
<td>
<div class="doc-md-description">
<p>If the layer is frozen but weights are
undefined.</p>
</div>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>illia/nn/tf/lstm.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">call</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">inputs</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">init_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Performs a forward pass through the Bayesian LSTM.</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs: Input tensor of token indices. Shape: [batch, seq_len, 1].</span>
<span class="sd">        init_states: Optional tuple of initial (hidden, cell) states.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple containing:</span>
<span class="sd">            - Output tensor after final linear transformation.</span>
<span class="sd">            - Tuple of final hidden and cell states.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the layer is frozen but weights are</span>
<span class="sd">            undefined.</span>
<span class="sd">    """</span>

    <span class="c1"># Sample weights if not frozen</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">frozen</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wf</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wf_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bf</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bf_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wi</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wi_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bi</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bi_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wc_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bc_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wo</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wo_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bo</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bo_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bv_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="k">elif</span> <span class="nb">any</span><span class="p">(</span>
        <span class="n">p</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">wf</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bf</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">wi</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bi</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">wc</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bc</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">wo</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bo</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bv</span><span class="p">,</span>
        <span class="p">]</span>
    <span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">"Module has been frozen with undefined weights and/or bias."</span>
        <span class="p">)</span>

    <span class="c1"># Apply embedding layer to input indices</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">seq_len</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Initialize h_t and c_t if init_states is None</span>
    <span class="k">if</span> <span class="n">init_states</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">h_t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">c_t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">h_t</span><span class="p">,</span> <span class="n">c_t</span> <span class="o">=</span> <span class="n">init_states</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">init_states</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Process sequence</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">):</span>
        <span class="c1"># Shape: (batch_size, embedding_dim)</span>
        <span class="n">x_t</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[:,</span> <span class="n">t</span><span class="p">,</span> <span class="p">:]</span>

        <span class="c1"># Concatenate input and hidden state</span>
        <span class="c1"># Shape: (batch_size, embedding_dim + hidden_size)</span>
        <span class="n">z_t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">x_t</span><span class="p">,</span> <span class="n">h_t</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Forget gate</span>
        <span class="n">ft</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">z_t</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">wf</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bf</span><span class="p">)</span>

        <span class="c1"># Input gate</span>
        <span class="n">it</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">z_t</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">wi</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bi</span><span class="p">)</span>

        <span class="c1"># Candidate cell state</span>
        <span class="n">can</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">z_t</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">wc</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bc</span><span class="p">)</span>

        <span class="c1"># Output gate</span>
        <span class="n">ot</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">z_t</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">wo</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bo</span><span class="p">)</span>

        <span class="c1"># Update cell state</span>
        <span class="n">c_t</span> <span class="o">=</span> <span class="n">c_t</span> <span class="o">*</span> <span class="n">ft</span> <span class="o">+</span> <span class="n">can</span> <span class="o">*</span> <span class="n">it</span>

        <span class="c1"># Update hidden state</span>
        <span class="n">h_t</span> <span class="o">=</span> <span class="n">ot</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">c_t</span><span class="p">)</span>

    <span class="c1"># Compute final output</span>
    <span class="n">y_t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_t</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bv</span>

    <span class="k">return</span> <span class="n">y_t</span><span class="p">,</span> <span class="p">(</span><span class="n">h_t</span><span class="p">,</span> <span class="n">c_t</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="illia.nn.tf.lstm.LSTM.freeze"><span class="enumerate-headings-plugin enumerate-heading-plugin">9.6.3</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <code class="highlight language-python"><span class="n">freeze</span><span class="p">()</span></code>
</h3>
<div class="doc doc-contents">
<p>Freeze the module's parameters to stop gradient computation.
If weights or biases are not sampled yet, they are sampled first.
Once frozen, parameters are not resampled or updated.</p>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code>None</code>
</td>
<td>
<div class="doc-md-description">
<p>None.</p>
</div>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>illia/nn/tf/lstm.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">freeze</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Freeze the module's parameters to stop gradient computation.</span>
<span class="sd">    If weights or biases are not sampled yet, they are sampled first.</span>
<span class="sd">    Once frozen, parameters are not resampled or updated.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None.</span>
<span class="sd">    """</span>

    <span class="c1"># Set indicator</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">frozen</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="c1"># Freeze embedding layer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>

    <span class="c1"># Forget gate</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">wf</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wf</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wf_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bf</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bf</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bf_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">wf</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wf</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bf</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bf</span><span class="p">)</span>

    <span class="c1"># Input gate</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">wi</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wi</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wi_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bi</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bi</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bi_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">wi</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wi</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bi</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bi</span><span class="p">)</span>

    <span class="c1"># Candidate gate</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">wc</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wc_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bc</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bc_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">wc</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wc</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bc</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bc</span><span class="p">)</span>

    <span class="c1"># Output gate</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">wo</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wo</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wo_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bo</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bo</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bo_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">wo</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wo</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bo</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bo</span><span class="p">)</span>

    <span class="c1"># Final output layer</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bv</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bv_distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">wv</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wv</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bv</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bv</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="illia.nn.tf.lstm.LSTM.kl_cost"><span class="enumerate-headings-plugin enumerate-heading-plugin">9.6.4</span> 
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code> <code class="highlight language-python"><span class="n">kl_cost</span><span class="p">()</span></code>
</h3>
<div class="doc doc-contents">
<p>Compute the KL divergence cost for all Bayesian parameters.</p>
<p><span class="doc-section-title">Returns:</span></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="doc-section-item">
<td>
<code><span title="tuple">tuple</span>[<span title="tensorflow.Tensor">Tensor</span>, <span title="int">int</span>]</code>
</td>
<td>
<div class="doc-md-description">
<p>tuple[tf.Tensor, int]: A tuple containing the KL divergence
cost and the total number of parameters in the layer.</p>
</div>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>illia/nn/tf/lstm.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">kl_cost</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Compute the KL divergence cost for all Bayesian parameters.</span>

<span class="sd">    Returns:</span>
<span class="sd">        tuple[tf.Tensor, int]: A tuple containing the KL divergence</span>
<span class="sd">            cost and the total number of parameters in the layer.</span>
<span class="sd">    """</span>

    <span class="c1"># Compute log probs for each pair of weights and bias</span>
    <span class="n">log_probs_f</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wf_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wf</span>
    <span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bf_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bf</span><span class="p">)</span>

    <span class="n">log_probs_i</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wi_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wi</span>
    <span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bi_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bi</span><span class="p">)</span>

    <span class="n">log_probs_c</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wc_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wc</span>
    <span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bc_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bc</span><span class="p">)</span>

    <span class="n">log_probs_o</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wo_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wo</span>
    <span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bo_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bo</span><span class="p">)</span>

    <span class="n">log_probs_v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wv</span>
    <span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bv_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bv</span><span class="p">)</span>

    <span class="c1"># Compute the total loss</span>
    <span class="n">log_probs</span> <span class="o">=</span> <span class="n">log_probs_f</span> <span class="o">+</span> <span class="n">log_probs_i</span> <span class="o">+</span> <span class="n">log_probs_c</span> <span class="o">+</span> <span class="n">log_probs_o</span> <span class="o">+</span> <span class="n">log_probs_v</span>

    <span class="c1"># Compute number of parameters</span>
    <span class="n">num_params</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wf_distribution</span><span class="o">.</span><span class="n">num_params</span>
        <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bf_distribution</span><span class="o">.</span><span class="n">num_params</span>
        <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">wi_distribution</span><span class="o">.</span><span class="n">num_params</span>
        <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bi_distribution</span><span class="o">.</span><span class="n">num_params</span>
        <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">wc_distribution</span><span class="o">.</span><span class="n">num_params</span>
        <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bc_distribution</span><span class="o">.</span><span class="n">num_params</span>
        <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">wo_distribution</span><span class="o">.</span><span class="n">num_params</span>
        <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bo_distribution</span><span class="o">.</span><span class="n">num_params</span>
        <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">wv_distribution</span><span class="o">.</span><span class="n">num_params</span>
        <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bv_distribution</span><span class="o">.</span><span class="n">num_params</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">log_probs</span><span class="p">,</span> <span class="n">num_params</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<aside class="md-source-file">
<span class="md-source-file__fact">
<span class="md-icon" title="Last update">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"></path></svg>
</span>
<span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="September 18, 2025 16:49:42 UTC">September 18, 2025</span>
</span>
<span class="md-source-file__fact">
<span class="md-icon" title="Contributors">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 4a4 4 0 0 1 4 4 4 4 0 0 1-4 4 4 4 0 0 1-4-4 4 4 0 0 1 4-4m0 10c4.42 0 8 1.79 8 4v2H4v-2c0-2.21 3.58-4 8-4"></path></svg>
</span>
<nav>
      Daniel Bazo Correa
    </nav>
</span>
</aside>
</article>
</div>
<script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
</div>
<button class="md-top md-icon" data-md-component="top" hidden="" type="button">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg>
  Back to top
</button>
</main>
<footer class="md-footer">
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-copyright">
<div class="md-copyright__highlight">
      © Telefonaktiebolaget LM Ericsson 1994-2025
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" rel="noopener" target="_blank">
      Material for MkDocs
    </a>
</div>
<div class="md-social">
<a class="md-social__link" href="https://www.linkedin.com/company/ericsson/" rel="noopener" target="_blank" title="www.linkedin.com">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zM102.2 96a38.5 38.5 0 1 1 0 77 38.5 38.5 0 1 1 0-77m282.1 320h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z" fill="currentColor"></path></svg>
</a>
<a class="md-social__link" href="https://www.youtube.com/@ericsson" rel="noopener" target="_blank" title="www.youtube.com">
<svg viewbox="0 0 576 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M549.7 124.1c-6.2-23.7-24.8-42.3-48.3-48.6C458.9 64 288.1 64 288.1 64S117.3 64 74.7 75.5c-23.5 6.3-42 24.9-48.3 48.6C15 167 15 256.4 15 256.4s0 89.4 11.4 132.3c6.3 23.6 24.8 41.5 48.3 47.8C117.3 448 288.1 448 288.1 448s170.8 0 213.4-11.5c23.5-6.3 42-24.2 48.3-47.8 11.4-42.9 11.4-132.3 11.4-132.3s0-89.4-11.4-132.3zM232.2 337.6V175.2l142.7 81.2z" fill="currentColor"></path></svg>
</a>
<a class="md-social__link" href="https://x.com/ericsson" rel="noopener" target="_blank" title="x.com">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M357.2 48h70.6L273.6 224.2 455 464H313L201.7 318.6 74.5 464H3.8l164.9-188.5L-5.2 48h145.6l100.5 132.9zm-24.8 373.8h39.1L119.1 88h-42z" fill="currentColor"></path></svg>
</a>
</div>
</div>
</div>
</footer>
</div>
<div class="md-dialog" data-md-component="dialog">
<div class="md-dialog__inner md-typeset"></div>
</div>
<div class="md-progress" data-md-component="progress" role="progressbar"></div>
<script id="__config" type="application/json">{"base": "../../..", "features": ["content.tabs.link", "content.code.annotate", "content.code.copy", "content.tooltips", "announce.dismiss", "navigation.tabs", "navigation.instant", "navigation.instant.prefetch", "navigation.instant.preview", "navigation.instant.progress", "navigation.sections", "navigation.top", "navigation.tracking", "navigation.indexes", "search.suggest", "toc.follow", "toc.integrate"], "search": "../../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"default": "latest", "provider": "mike"}}</script>
<script src="../../../assets/javascripts/bundle.50899def.min.js"></script>
<script src="../../../javascripts/mathjax.js"></script>
<script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>