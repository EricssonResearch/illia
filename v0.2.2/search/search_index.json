{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":"<p>Warning</p> <p>illia is under active development. The library is evolving rapidly to ensure stable support across all frameworks. Expect ongoing changes as we improve functionality and performance.</p>"},{"location":"index.html#introduction","title":"Introduction","text":"<p>illia is a library for Bayesian Neural Networks that brings uncertainty quantification to deep learning, a capability that is critical in sectors such as telecommunications, medicine, and beyond. Designed with flexibility in mind, it integrates seamlessly with multiple backends and popular frameworks, enabling a single codebase to support multiple backends with minimal modifications.</p> <p>For full documentation, please visit the site: https://ericssonresearch.github.io/illia/</p>"},{"location":"index.html#why-choose-illia","title":"Why Choose illia?","text":"<ul> <li>Multi-Backend Support: Works with PyTorch, TensorFlow, and JAX.</li> <li>Graph Neural Networks: Currently integrated with PyTorch Geometric, with planned   support for DGL and/or Spektral in future releases.</li> <li>Developer Friendly: Intuitive API design and comprehensive documentation.</li> </ul>"},{"location":"index.html#quick-start","title":"Quick Start","text":"<p>To show how easy it is to use illia, here\u2019s a quick example to get started. In this case, we explicitly choose the backend PyTorch, the underlying framework, and define a convolutional layer:</p> <pre><code>import os\nimport torch\n\n# Configure backend (PyTorch is default)\nos.environ[\"ILLIA_BACKEND\"] = \"torch\"\n\nimport illia\nfrom illia.nn import Conv2d\n\n# Create a Bayesian convolutional layer\nconv_layer = Conv2d(\n    input_channels=1,\n    output_channels=1,\n    kernel_size=3,\n)\n\n# Define input tensor\ninput_tensor = torch.rand(1, 1, 4, 4)\n\n# Define the number of iterations to apply the forward pass\nnum_passes = 10\noutputs = [conv_layer(input_tensor) for _ in range(num_passes)]\n\n# Stack outputs into a single tensor\noutputs = torch.stack(outputs)\n\nprint(f\"Output shape: {outputs.shape}\")\nprint(f\"Output std: {outputs.std()}\")\nprint(f\"Output var: {outputs.var()}\")\n</code></pre>"},{"location":"index.html#contributing","title":"Contributing","text":"<p>We welcome contributions from the community! Whether you're fixing bugs, adding features, or improving documentation:</p> <ol> <li>Read our    contributing guide    for development setup.</li> <li>Check open issues for ways to    help.</li> <li>Submit bug reports using our issue templates.</li> </ol>"},{"location":"index.html#license","title":"License","text":"<p>illia is released under the MIT License. We hope you find it useful and inspiring for your projects!</p>"},{"location":"api/Deep%20Learning/Jax/distributions.html","title":"Distributions","text":""},{"location":"api/Deep%20Learning/Jax/distributions.html#illia.distributions.jax.base.DistributionModule","title":"<code>DistributionModule</code>","text":"<p>Abstract base for probabilistic distribution modules in JAX. Defines the required interface for sampling, computing log-probabilities, and counting learnable parameters.</p> Notes <p>This class is abstract and cannot be instantiated directly. All abstract methods must be implemented by subclasses.</p> Source code in <code>illia/distributions/jax/base.py</code> <pre><code>class DistributionModule(nnx.Module, ABC):\n    \"\"\"\n    Abstract base for probabilistic distribution modules in JAX.\n    Defines the required interface for sampling, computing\n    log-probabilities, and counting learnable parameters.\n\n    Notes:\n        This class is abstract and cannot be instantiated directly.\n        All abstract methods must be implemented by subclasses.\n    \"\"\"\n\n    @abstractmethod\n    def sample(self, rngs: Rngs = nnx.Rngs(0)) -&gt; jax.Array:\n        \"\"\"\n        Draw a sample from the distribution using the given RNG.\n\n        Args:\n            rngs: RNG container used for sampling.\n\n        Returns:\n            jax.Array: A sample drawn from the distribution.\n\n        Notes:\n            Sampling should be reproducible given the same RNG.\n        \"\"\"\n\n    @abstractmethod\n    def log_prob(self, x: Optional[jax.Array] = None) -&gt; jax.Array:\n        \"\"\"\n        Compute the log-probability of a provided sample. If no\n        sample is passed, one is drawn internally.\n\n        Args:\n            x: Optional sample to evaluate. If None, a new sample is\n                drawn from the distribution.\n\n        Returns:\n            jax.Array: Scalar log-probability value.\n\n        Notes:\n            Works with both user-supplied and internally drawn\n            samples.\n        \"\"\"\n\n    @property\n    @abstractmethod\n    def num_params(self) -&gt; int:\n        \"\"\"\n        Return the number of learnable parameters in the\n        distribution.\n\n        Returns:\n            int: Total count of learnable parameters.\n        \"\"\"\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/distributions.html#illia.distributions.jax.base.DistributionModule.num_params","title":"<code>num_params</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Return the number of learnable parameters in the distribution.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Total count of learnable parameters.</p>"},{"location":"api/Deep%20Learning/Jax/distributions.html#illia.distributions.jax.base.DistributionModule.log_prob","title":"<code>log_prob(x=None)</code>  <code>abstractmethod</code>","text":"<p>Compute the log-probability of a provided sample. If no sample is passed, one is drawn internally.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Optional[Array]</code> <p>Optional sample to evaluate. If None, a new sample is drawn from the distribution.</p> <code>None</code> <p>Returns:</p> Type Description <code>Array</code> <p>jax.Array: Scalar log-probability value.</p> Notes <p>Works with both user-supplied and internally drawn samples.</p> Source code in <code>illia/distributions/jax/base.py</code> <pre><code>@abstractmethod\ndef log_prob(self, x: Optional[jax.Array] = None) -&gt; jax.Array:\n    \"\"\"\n    Compute the log-probability of a provided sample. If no\n    sample is passed, one is drawn internally.\n\n    Args:\n        x: Optional sample to evaluate. If None, a new sample is\n            drawn from the distribution.\n\n    Returns:\n        jax.Array: Scalar log-probability value.\n\n    Notes:\n        Works with both user-supplied and internally drawn\n        samples.\n    \"\"\"\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/distributions.html#illia.distributions.jax.base.DistributionModule.sample","title":"<code>sample(rngs=nnx.Rngs(0))</code>  <code>abstractmethod</code>","text":"<p>Draw a sample from the distribution using the given RNG.</p> <p>Parameters:</p> Name Type Description Default <code>rngs</code> <code>Rngs</code> <p>RNG container used for sampling.</p> <code>Rngs(0)</code> <p>Returns:</p> Type Description <code>Array</code> <p>jax.Array: A sample drawn from the distribution.</p> Notes <p>Sampling should be reproducible given the same RNG.</p> Source code in <code>illia/distributions/jax/base.py</code> <pre><code>@abstractmethod\ndef sample(self, rngs: Rngs = nnx.Rngs(0)) -&gt; jax.Array:\n    \"\"\"\n    Draw a sample from the distribution using the given RNG.\n\n    Args:\n        rngs: RNG container used for sampling.\n\n    Returns:\n        jax.Array: A sample drawn from the distribution.\n\n    Notes:\n        Sampling should be reproducible given the same RNG.\n    \"\"\"\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/distributions.html#illia.distributions.jax.gaussian.GaussianDistribution","title":"<code>GaussianDistribution</code>","text":"<p>Learnable Gaussian distribution with diagonal covariance. Represents a Gaussian with trainable mean and standard deviation. The standard deviation is derived from <code>rho</code> using a softplus transformation to ensure positivity.</p> Notes <p>Assumes diagonal covariance. KL divergence can be estimated via log-probability differences from <code>log_prob</code>.</p> Source code in <code>illia/distributions/jax/gaussian.py</code> <pre><code>class GaussianDistribution(DistributionModule):\n    \"\"\"\n    Learnable Gaussian distribution with diagonal covariance.\n    Represents a Gaussian with trainable mean and standard\n    deviation. The standard deviation is derived from `rho`\n    using a softplus transformation to ensure positivity.\n\n    Notes:\n        Assumes diagonal covariance. KL divergence can be\n        estimated via log-probability differences from\n        `log_prob`.\n    \"\"\"\n\n    def __init__(\n        self,\n        shape: tuple[int, ...],\n        mu_prior: float = 0.0,\n        std_prior: float = 0.1,\n        mu_init: float = 0.0,\n        rho_init: float = -7.0,\n        rngs: Rngs = nnx.Rngs(0),\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initialize a learnable Gaussian distribution module.\n\n        Args:\n            shape: Shape of the learnable parameters.\n            mu_prior: Mean of the Gaussian prior.\n            std_prior: Standard deviation of the prior.\n            mu_init: Initial value for the learnable mean.\n            rho_init: Initial value for the learnable rho.\n            rngs: RNG container for parameter initialization.\n            **kwargs: Extra arguments passed to the base class.\n\n        Returns:\n            None\n        \"\"\"\n\n        # Call super-class constructor\n        super().__init__(**kwargs)\n\n        # Set attributes\n        self.shape = shape\n        self.mu_prior = mu_prior\n        self.std_prior = std_prior\n        self.mu_init = mu_init\n        self.rho_init = rho_init\n\n        # Define initial mu and rho\n        self.mu = nnx.Param(\n            self.mu_init + 0.1 * jax.random.normal(rngs.params(), self.shape)\n        )\n        self.rho = nnx.Param(\n            self.rho_init + 0.1 * jax.random.normal(rngs.params(), self.shape)\n        )\n\n    def sample(self, rngs: Rngs = nnx.Rngs(0)) -&gt; jax.Array:\n        \"\"\"\n        Draw a sample from the Gaussian distribution.\n\n        Args:\n            rngs: RNG container used for sampling.\n\n        Returns:\n            jax.Array: A sample drawn from the distribution.\n\n        Notes:\n            Sampling is reproducible with the same RNG.\n        \"\"\"\n\n        # Compute epsilon and sigma\n        eps: jax.Array = jax.random.normal(rngs.params(), self.rho.shape)\n        sigma: jax.Array = jnp.log1p(jnp.exp(jnp.asarray(self.rho)))\n\n        return self.mu + sigma * eps\n\n    def log_prob(self, x: Optional[jax.Array] = None) -&gt; jax.Array:\n        \"\"\"\n        Compute the log-probability of a given sample. If no\n        sample is provided, one is drawn internally.\n\n        Args:\n            x: Optional input sample to evaluate. If None,\n                a new sample is drawn from the distribution.\n\n        Returns:\n            jax.Array: Scalar log-probability value.\n\n        Notes:\n            Supports both user-supplied and internally drawn\n            samples.\n        \"\"\"\n\n        # Sample if x is None\n        if x is None:\n            x = self.sample()\n\n        # Define pi variable\n        pi: jax.Array = jnp.acos(jnp.zeros(1)) * 2\n\n        # Compute log priors\n        log_prior = (\n            -jnp.log(jnp.sqrt(2 * pi))\n            - jnp.log(self.std_prior)\n            - (((x - self.mu_prior) ** 2) / (2 * self.std_prior**2))\n            - 0.5\n        )\n\n        # Compute sigma\n        sigma: jax.Array = jnp.log1p(jnp.exp(jnp.asarray(self.rho)))\n\n        # Compute log posteriors\n        log_posteriors = (\n            -jnp.log(jnp.sqrt(2 * pi))\n            - jnp.log(sigma)\n            - (((x - self.mu) ** 2) / (2 * sigma**2))\n            - 0.5\n        )\n\n        # Compute final log probs\n        log_probs = log_posteriors.sum() - log_prior.sum()\n\n        return log_probs\n\n    @property\n    def num_params(self) -&gt; int:\n        \"\"\"\n        Return the number of learnable parameters in the\n        distribution.\n\n        Returns:\n            int: Total count of learnable parameters.\n        \"\"\"\n\n        return len(self.mu.reshape(-1))\n\n    def __call__(self) -&gt; jax.Array:\n        \"\"\"\n        Perform a forward pass by drawing a sample.\n\n        Returns:\n            jax.Array: A sample from the distribution.\n        \"\"\"\n\n        return self.sample()\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/distributions.html#illia.distributions.jax.gaussian.GaussianDistribution.num_params","title":"<code>num_params</code>  <code>property</code>","text":"<p>Return the number of learnable parameters in the distribution.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Total count of learnable parameters.</p>"},{"location":"api/Deep%20Learning/Jax/distributions.html#illia.distributions.jax.gaussian.GaussianDistribution.__call__","title":"<code>__call__()</code>","text":"<p>Perform a forward pass by drawing a sample.</p> <p>Returns:</p> Type Description <code>Array</code> <p>jax.Array: A sample from the distribution.</p> Source code in <code>illia/distributions/jax/gaussian.py</code> <pre><code>def __call__(self) -&gt; jax.Array:\n    \"\"\"\n    Perform a forward pass by drawing a sample.\n\n    Returns:\n        jax.Array: A sample from the distribution.\n    \"\"\"\n\n    return self.sample()\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/distributions.html#illia.distributions.jax.gaussian.GaussianDistribution.__init__","title":"<code>__init__(shape, mu_prior=0.0, std_prior=0.1, mu_init=0.0, rho_init=-7.0, rngs=nnx.Rngs(0), **kwargs)</code>","text":"<p>Initialize a learnable Gaussian distribution module.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>tuple[int, ...]</code> <p>Shape of the learnable parameters.</p> required <code>mu_prior</code> <code>float</code> <p>Mean of the Gaussian prior.</p> <code>0.0</code> <code>std_prior</code> <code>float</code> <p>Standard deviation of the prior.</p> <code>0.1</code> <code>mu_init</code> <code>float</code> <p>Initial value for the learnable mean.</p> <code>0.0</code> <code>rho_init</code> <code>float</code> <p>Initial value for the learnable rho.</p> <code>-7.0</code> <code>rngs</code> <code>Rngs</code> <p>RNG container for parameter initialization.</p> <code>Rngs(0)</code> <code>**kwargs</code> <code>Any</code> <p>Extra arguments passed to the base class.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>illia/distributions/jax/gaussian.py</code> <pre><code>def __init__(\n    self,\n    shape: tuple[int, ...],\n    mu_prior: float = 0.0,\n    std_prior: float = 0.1,\n    mu_init: float = 0.0,\n    rho_init: float = -7.0,\n    rngs: Rngs = nnx.Rngs(0),\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialize a learnable Gaussian distribution module.\n\n    Args:\n        shape: Shape of the learnable parameters.\n        mu_prior: Mean of the Gaussian prior.\n        std_prior: Standard deviation of the prior.\n        mu_init: Initial value for the learnable mean.\n        rho_init: Initial value for the learnable rho.\n        rngs: RNG container for parameter initialization.\n        **kwargs: Extra arguments passed to the base class.\n\n    Returns:\n        None\n    \"\"\"\n\n    # Call super-class constructor\n    super().__init__(**kwargs)\n\n    # Set attributes\n    self.shape = shape\n    self.mu_prior = mu_prior\n    self.std_prior = std_prior\n    self.mu_init = mu_init\n    self.rho_init = rho_init\n\n    # Define initial mu and rho\n    self.mu = nnx.Param(\n        self.mu_init + 0.1 * jax.random.normal(rngs.params(), self.shape)\n    )\n    self.rho = nnx.Param(\n        self.rho_init + 0.1 * jax.random.normal(rngs.params(), self.shape)\n    )\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/distributions.html#illia.distributions.jax.gaussian.GaussianDistribution.log_prob","title":"<code>log_prob(x=None)</code>","text":"<p>Compute the log-probability of a given sample. If no sample is provided, one is drawn internally.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Optional[Array]</code> <p>Optional input sample to evaluate. If None, a new sample is drawn from the distribution.</p> <code>None</code> <p>Returns:</p> Type Description <code>Array</code> <p>jax.Array: Scalar log-probability value.</p> Notes <p>Supports both user-supplied and internally drawn samples.</p> Source code in <code>illia/distributions/jax/gaussian.py</code> <pre><code>def log_prob(self, x: Optional[jax.Array] = None) -&gt; jax.Array:\n    \"\"\"\n    Compute the log-probability of a given sample. If no\n    sample is provided, one is drawn internally.\n\n    Args:\n        x: Optional input sample to evaluate. If None,\n            a new sample is drawn from the distribution.\n\n    Returns:\n        jax.Array: Scalar log-probability value.\n\n    Notes:\n        Supports both user-supplied and internally drawn\n        samples.\n    \"\"\"\n\n    # Sample if x is None\n    if x is None:\n        x = self.sample()\n\n    # Define pi variable\n    pi: jax.Array = jnp.acos(jnp.zeros(1)) * 2\n\n    # Compute log priors\n    log_prior = (\n        -jnp.log(jnp.sqrt(2 * pi))\n        - jnp.log(self.std_prior)\n        - (((x - self.mu_prior) ** 2) / (2 * self.std_prior**2))\n        - 0.5\n    )\n\n    # Compute sigma\n    sigma: jax.Array = jnp.log1p(jnp.exp(jnp.asarray(self.rho)))\n\n    # Compute log posteriors\n    log_posteriors = (\n        -jnp.log(jnp.sqrt(2 * pi))\n        - jnp.log(sigma)\n        - (((x - self.mu) ** 2) / (2 * sigma**2))\n        - 0.5\n    )\n\n    # Compute final log probs\n    log_probs = log_posteriors.sum() - log_prior.sum()\n\n    return log_probs\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/distributions.html#illia.distributions.jax.gaussian.GaussianDistribution.sample","title":"<code>sample(rngs=nnx.Rngs(0))</code>","text":"<p>Draw a sample from the Gaussian distribution.</p> <p>Parameters:</p> Name Type Description Default <code>rngs</code> <code>Rngs</code> <p>RNG container used for sampling.</p> <code>Rngs(0)</code> <p>Returns:</p> Type Description <code>Array</code> <p>jax.Array: A sample drawn from the distribution.</p> Notes <p>Sampling is reproducible with the same RNG.</p> Source code in <code>illia/distributions/jax/gaussian.py</code> <pre><code>def sample(self, rngs: Rngs = nnx.Rngs(0)) -&gt; jax.Array:\n    \"\"\"\n    Draw a sample from the Gaussian distribution.\n\n    Args:\n        rngs: RNG container used for sampling.\n\n    Returns:\n        jax.Array: A sample drawn from the distribution.\n\n    Notes:\n        Sampling is reproducible with the same RNG.\n    \"\"\"\n\n    # Compute epsilon and sigma\n    eps: jax.Array = jax.random.normal(rngs.params(), self.rho.shape)\n    sigma: jax.Array = jnp.log1p(jnp.exp(jnp.asarray(self.rho)))\n\n    return self.mu + sigma * eps\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/losses.html","title":"Losses","text":""},{"location":"api/Deep%20Learning/Jax/losses.html#illia.losses.jax.kl.KLDivergenceLoss","title":"<code>KLDivergenceLoss</code>","text":"<p>Compute Kullback-Leibler divergence across Bayesian modules. This loss sums the KL divergence from all Bayesian layers in the model. It can be reduced by averaging and scaled by a weight factor.</p> Notes <p>Assumes the model contains submodules derived from <code>BayesianModule</code>.</p> Source code in <code>illia/losses/jax/kl.py</code> <pre><code>class KLDivergenceLoss(nnx.Module):\n    \"\"\"\n    Compute Kullback-Leibler divergence across Bayesian modules.\n    This loss sums the KL divergence from all Bayesian layers in\n    the model. It can be reduced by averaging and scaled by a\n    weight factor.\n\n    Notes:\n        Assumes the model contains submodules derived from\n        `BayesianModule`.\n    \"\"\"\n\n    def __init__(\n        self,\n        reduction: Literal[\"mean\"] = \"mean\",\n        weight: float = 1.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the KL divergence loss.\n\n        Args:\n            reduction: Method used to reduce the KL loss.\n            weight: Scaling factor for the KL divergence.\n            **kwargs: Extra arguments passed to the base class.\n\n        Returns:\n            None\n        \"\"\"\n\n        # Call super class constructor\n        super().__init__(**kwargs)\n\n        # Set attributes\n        self.reduction = reduction\n        self.weight = weight\n\n    def __call__(self, model: nnx.Module) -&gt; jax.Array:\n        \"\"\"\n        Compute KL divergence for all Bayesian modules in a model.\n\n        Args:\n            model: Model containing Bayesian submodules.\n\n        Returns:\n            jax.Array: Weighted KL divergence loss.\n\n        Notes:\n            The KL loss is averaged over the number of parameters\n            and scaled by the `weight` attribute.\n        \"\"\"\n\n        # Init kl cost and params\n        kl_global_cost: jax.Array = jnp.array(0.0)\n        num_params_global: int = 0\n\n        # Iter over modules\n        for _, module in model.iter_modules():\n            if isinstance(module, BayesianModule):\n                kl_cost, num_params = module.kl_cost()\n                kl_global_cost += kl_cost\n                num_params_global += num_params\n\n        # Average by the number of parameters\n        kl_global_cost /= num_params\n        kl_global_cost *= self.weight\n\n        return kl_global_cost\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/losses.html#illia.losses.jax.kl.KLDivergenceLoss.__call__","title":"<code>__call__(model)</code>","text":"<p>Compute KL divergence for all Bayesian modules in a model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Model containing Bayesian submodules.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>jax.Array: Weighted KL divergence loss.</p> Notes <p>The KL loss is averaged over the number of parameters and scaled by the <code>weight</code> attribute.</p> Source code in <code>illia/losses/jax/kl.py</code> <pre><code>def __call__(self, model: nnx.Module) -&gt; jax.Array:\n    \"\"\"\n    Compute KL divergence for all Bayesian modules in a model.\n\n    Args:\n        model: Model containing Bayesian submodules.\n\n    Returns:\n        jax.Array: Weighted KL divergence loss.\n\n    Notes:\n        The KL loss is averaged over the number of parameters\n        and scaled by the `weight` attribute.\n    \"\"\"\n\n    # Init kl cost and params\n    kl_global_cost: jax.Array = jnp.array(0.0)\n    num_params_global: int = 0\n\n    # Iter over modules\n    for _, module in model.iter_modules():\n        if isinstance(module, BayesianModule):\n            kl_cost, num_params = module.kl_cost()\n            kl_global_cost += kl_cost\n            num_params_global += num_params\n\n    # Average by the number of parameters\n    kl_global_cost /= num_params\n    kl_global_cost *= self.weight\n\n    return kl_global_cost\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/losses.html#illia.losses.jax.kl.KLDivergenceLoss.__init__","title":"<code>__init__(reduction='mean', weight=1.0, **kwargs)</code>","text":"<p>Initialize the KL divergence loss.</p> <p>Parameters:</p> Name Type Description Default <code>reduction</code> <code>Literal['mean']</code> <p>Method used to reduce the KL loss.</p> <code>'mean'</code> <code>weight</code> <code>float</code> <p>Scaling factor for the KL divergence.</p> <code>1.0</code> <code>**kwargs</code> <code>Any</code> <p>Extra arguments passed to the base class.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>illia/losses/jax/kl.py</code> <pre><code>def __init__(\n    self,\n    reduction: Literal[\"mean\"] = \"mean\",\n    weight: float = 1.0,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialize the KL divergence loss.\n\n    Args:\n        reduction: Method used to reduce the KL loss.\n        weight: Scaling factor for the KL divergence.\n        **kwargs: Extra arguments passed to the base class.\n\n    Returns:\n        None\n    \"\"\"\n\n    # Call super class constructor\n    super().__init__(**kwargs)\n\n    # Set attributes\n    self.reduction = reduction\n    self.weight = weight\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/losses.html#illia.losses.jax.elbo.ELBOLoss","title":"<code>ELBOLoss</code>","text":"<p>Compute the Evidence Lower Bound (ELBO) loss for Bayesian networks. Combines a reconstruction loss with a KL divergence term. Monte Carlo sampling can estimate the expected reconstruction loss over stochastic layers.</p> Notes <p>The KL term is weighted by <code>kl_weight</code>. The model is assumed to contain Bayesian layers compatible with <code>KLDivergenceLoss</code>.</p> Source code in <code>illia/losses/jax/elbo.py</code> <pre><code>class ELBOLoss(nnx.Module):\n    \"\"\"\n    Compute the Evidence Lower Bound (ELBO) loss for Bayesian\n    networks. Combines a reconstruction loss with a KL divergence\n    term. Monte Carlo sampling can estimate the expected\n    reconstruction loss over stochastic layers.\n\n    Notes:\n        The KL term is weighted by `kl_weight`. The model is\n        assumed to contain Bayesian layers compatible with\n        `KLDivergenceLoss`.\n    \"\"\"\n\n    def __init__(\n        self,\n        loss_function: Callable[[jax.Array, jax.Array], jax.Array],\n        num_samples: int = 1,\n        kl_weight: float = 1e-3,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the ELBO loss with reconstruction and KL\n        components.\n\n        Args:\n            loss_function: Function to compute reconstruction\n                loss.\n            num_samples: Number of Monte Carlo samples used for\n                estimation.\n            kl_weight: Weight applied to the KL divergence term.\n            **kwargs: Extra arguments passed to the base class.\n\n        Returns:\n            None\n        \"\"\"\n\n        # Call super class constructor\n        super().__init__(**kwargs)\n\n        # Set attributes\n        self.loss_function = loss_function\n        self.num_samples = num_samples\n        self.kl_weight = kl_weight\n        self.kl_loss = KLDivergenceLoss(weight=kl_weight)\n\n    def __call__(\n        self, outputs: jax.Array, targets: jax.Array, model: nnx.Module\n    ) -&gt; jax.Array:\n        \"\"\"\n        Compute the ELBO loss with Monte Carlo sampling and KL\n        regularization.\n\n        Args:\n            outputs: Predictions generated by the model.\n            targets: Ground truth values for training.\n            model: Model containing Bayesian layers.\n\n        Returns:\n            jax.Array: Scalar ELBO loss averaged over samples.\n\n        Notes:\n            The loss is averaged over `num_samples` Monte Carlo\n            draws.\n        \"\"\"\n\n        loss_value: jax.Array = jnp.array(0.0)\n        for _ in range(self.num_samples):\n            loss_value += self.loss_function(outputs, targets) + self.kl_loss(model)\n\n        loss_value /= self.num_samples\n\n        return loss_value\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/losses.html#illia.losses.jax.elbo.ELBOLoss.__call__","title":"<code>__call__(outputs, targets, model)</code>","text":"<p>Compute the ELBO loss with Monte Carlo sampling and KL regularization.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>Array</code> <p>Predictions generated by the model.</p> required <code>targets</code> <code>Array</code> <p>Ground truth values for training.</p> required <code>model</code> <code>Module</code> <p>Model containing Bayesian layers.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>jax.Array: Scalar ELBO loss averaged over samples.</p> Notes <p>The loss is averaged over <code>num_samples</code> Monte Carlo draws.</p> Source code in <code>illia/losses/jax/elbo.py</code> <pre><code>def __call__(\n    self, outputs: jax.Array, targets: jax.Array, model: nnx.Module\n) -&gt; jax.Array:\n    \"\"\"\n    Compute the ELBO loss with Monte Carlo sampling and KL\n    regularization.\n\n    Args:\n        outputs: Predictions generated by the model.\n        targets: Ground truth values for training.\n        model: Model containing Bayesian layers.\n\n    Returns:\n        jax.Array: Scalar ELBO loss averaged over samples.\n\n    Notes:\n        The loss is averaged over `num_samples` Monte Carlo\n        draws.\n    \"\"\"\n\n    loss_value: jax.Array = jnp.array(0.0)\n    for _ in range(self.num_samples):\n        loss_value += self.loss_function(outputs, targets) + self.kl_loss(model)\n\n    loss_value /= self.num_samples\n\n    return loss_value\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/losses.html#illia.losses.jax.elbo.ELBOLoss.__init__","title":"<code>__init__(loss_function, num_samples=1, kl_weight=0.001, **kwargs)</code>","text":"<p>Initialize the ELBO loss with reconstruction and KL components.</p> <p>Parameters:</p> Name Type Description Default <code>loss_function</code> <code>Callable[[Array, Array], Array]</code> <p>Function to compute reconstruction loss.</p> required <code>num_samples</code> <code>int</code> <p>Number of Monte Carlo samples used for estimation.</p> <code>1</code> <code>kl_weight</code> <code>float</code> <p>Weight applied to the KL divergence term.</p> <code>0.001</code> <code>**kwargs</code> <code>Any</code> <p>Extra arguments passed to the base class.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>illia/losses/jax/elbo.py</code> <pre><code>def __init__(\n    self,\n    loss_function: Callable[[jax.Array, jax.Array], jax.Array],\n    num_samples: int = 1,\n    kl_weight: float = 1e-3,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialize the ELBO loss with reconstruction and KL\n    components.\n\n    Args:\n        loss_function: Function to compute reconstruction\n            loss.\n        num_samples: Number of Monte Carlo samples used for\n            estimation.\n        kl_weight: Weight applied to the KL divergence term.\n        **kwargs: Extra arguments passed to the base class.\n\n    Returns:\n        None\n    \"\"\"\n\n    # Call super class constructor\n    super().__init__(**kwargs)\n\n    # Set attributes\n    self.loss_function = loss_function\n    self.num_samples = num_samples\n    self.kl_weight = kl_weight\n    self.kl_loss = KLDivergenceLoss(weight=kl_weight)\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/nn.html","title":"Neural Network Layers","text":""},{"location":"api/Deep%20Learning/Jax/nn.html#illia.nn.jax.base.BayesianModule","title":"<code>BayesianModule</code>","text":"<p>Abstract base for Bayesian-aware modules in JAX. Provides mechanisms to track if a module is Bayesian and control parameter updates through freezing/unfreezing.</p> Notes <p>All derived classes must implement <code>freeze</code> and <code>kl_cost</code> to handle parameter management and compute the KL divergence cost.</p> Source code in <code>illia/nn/jax/base.py</code> <pre><code>class BayesianModule(nnx.Module, ABC):\n    \"\"\"\n    Abstract base for Bayesian-aware modules in JAX.\n    Provides mechanisms to track if a module is Bayesian and control\n    parameter updates through freezing/unfreezing.\n\n    Notes:\n        All derived classes must implement `freeze` and `kl_cost` to\n        handle parameter management and compute the KL divergence cost.\n    \"\"\"\n\n    def __init__(self, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Initialize the Bayesian module with default flags.\n        Sets `frozen` to False and `is_bayesian` to True.\n\n        Args:\n            **kwargs: Extra arguments passed to the base class.\n\n        Returns:\n            None.\n        \"\"\"\n\n        # Call super class constructor\n        super().__init__(**kwargs)\n\n        # Set attributes\n        self.frozen: bool = False\n        self.is_bayesian: bool = True\n\n    @abstractmethod\n    def freeze(self) -&gt; None:\n        \"\"\"\n        Freeze the module's parameters to stop gradient computation.\n        If weights or biases are not sampled yet, they are sampled first.\n        Once frozen, parameters are not resampled or updated.\n\n        Returns:\n            None.\n\n        Notes:\n            Must be implemented by all subclasses.\n        \"\"\"\n\n    def unfreeze(self) -&gt; None:\n        \"\"\"\n        Unfreeze the module by setting its `frozen` flag to False.\n        Allows parameters to be sampled and updated again.\n\n        Returns:\n            None.\n        \"\"\"\n\n        self.frozen = False\n\n    @abstractmethod\n    def kl_cost(self) -&gt; tuple[jax.Array, int]:\n        \"\"\"\n        Compute the KL divergence cost for all Bayesian parameters.\n\n        Returns:\n            tuple[jax.Array, int]: A tuple containing the KL divergence\n                cost and the total number of parameters in the layer.\n\n        Notes:\n            Must be implemented by all subclasses.\n        \"\"\"\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/nn.html#illia.nn.jax.base.BayesianModule.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the Bayesian module with default flags. Sets <code>frozen</code> to False and <code>is_bayesian</code> to True.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Extra arguments passed to the base class.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>illia/nn/jax/base.py</code> <pre><code>def __init__(self, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Initialize the Bayesian module with default flags.\n    Sets `frozen` to False and `is_bayesian` to True.\n\n    Args:\n        **kwargs: Extra arguments passed to the base class.\n\n    Returns:\n        None.\n    \"\"\"\n\n    # Call super class constructor\n    super().__init__(**kwargs)\n\n    # Set attributes\n    self.frozen: bool = False\n    self.is_bayesian: bool = True\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/nn.html#illia.nn.jax.base.BayesianModule.freeze","title":"<code>freeze()</code>  <code>abstractmethod</code>","text":"<p>Freeze the module's parameters to stop gradient computation. If weights or biases are not sampled yet, they are sampled first. Once frozen, parameters are not resampled or updated.</p> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Notes <p>Must be implemented by all subclasses.</p> Source code in <code>illia/nn/jax/base.py</code> <pre><code>@abstractmethod\ndef freeze(self) -&gt; None:\n    \"\"\"\n    Freeze the module's parameters to stop gradient computation.\n    If weights or biases are not sampled yet, they are sampled first.\n    Once frozen, parameters are not resampled or updated.\n\n    Returns:\n        None.\n\n    Notes:\n        Must be implemented by all subclasses.\n    \"\"\"\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/nn.html#illia.nn.jax.base.BayesianModule.kl_cost","title":"<code>kl_cost()</code>  <code>abstractmethod</code>","text":"<p>Compute the KL divergence cost for all Bayesian parameters.</p> <p>Returns:</p> Type Description <code>tuple[Array, int]</code> <p>tuple[jax.Array, int]: A tuple containing the KL divergence cost and the total number of parameters in the layer.</p> Notes <p>Must be implemented by all subclasses.</p> Source code in <code>illia/nn/jax/base.py</code> <pre><code>@abstractmethod\ndef kl_cost(self) -&gt; tuple[jax.Array, int]:\n    \"\"\"\n    Compute the KL divergence cost for all Bayesian parameters.\n\n    Returns:\n        tuple[jax.Array, int]: A tuple containing the KL divergence\n            cost and the total number of parameters in the layer.\n\n    Notes:\n        Must be implemented by all subclasses.\n    \"\"\"\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/nn.html#illia.nn.jax.base.BayesianModule.unfreeze","title":"<code>unfreeze()</code>","text":"<p>Unfreeze the module by setting its <code>frozen</code> flag to False. Allows parameters to be sampled and updated again.</p> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>illia/nn/jax/base.py</code> <pre><code>def unfreeze(self) -&gt; None:\n    \"\"\"\n    Unfreeze the module by setting its `frozen` flag to False.\n    Allows parameters to be sampled and updated again.\n\n    Returns:\n        None.\n    \"\"\"\n\n    self.frozen = False\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/nn.html#illia.nn.jax.conv1d.Conv1d","title":"<code>Conv1d</code>","text":"<p>Bayesian 1D convolutional layer with optional weight and bias priors. Behaves like a standard Conv1d but treats weights and bias as random variables sampled from specified distributions. Parameters become fixed when the layer is frozen.</p> Source code in <code>illia/nn/jax/conv1d.py</code> <pre><code>class Conv1d(BayesianModule):\n    \"\"\"\n    Bayesian 1D convolutional layer with optional weight and bias priors.\n    Behaves like a standard Conv1d but treats weights and bias as random\n    variables sampled from specified distributions. Parameters become fixed\n    when the layer is frozen.\n    \"\"\"\n\n    bias_distribution: Optional[GaussianDistribution] = None\n    bias: Optional[nnx.Param] = None\n\n    def __init__(\n        self,\n        input_channels: int,\n        output_channels: int,\n        kernel_size: int,\n        stride: int = 1,\n        padding: int = 0,\n        dilation: int = 1,\n        groups: int = 1,\n        weights_distribution: Optional[GaussianDistribution] = None,\n        bias_distribution: Optional[GaussianDistribution] = None,\n        use_bias: bool = True,\n        rngs: Rngs = nnx.Rngs(0),\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes a Bayesian 1D convolutional layer.\n\n        Args:\n            input_channels: Number of input feature channels.\n            output_channels: Number of output feature channels.\n            kernel_size: Size of the convolution kernel.\n            stride: Stride of the convolution operation.\n            padding: Amount of zero-padding on both sides.\n            dilation: Spacing between kernel elements.\n            groups: Number of blocked connections between input and output.\n            weights_distribution: Distribution to initialize weights.\n            bias_distribution: Distribution to initialize bias.\n            use_bias: Whether to include a bias term.\n            rngs: Random number generators for reproducibility.\n            **kwargs: Extra arguments passed to the base class.\n\n        Returns:\n            None.\n\n        Notes:\n            Gaussian distributions are used by default if none are\n            provided.\n        \"\"\"\n\n        # Call super class constructor\n        super().__init__(**kwargs)\n\n        # Set attributes\n        self.input_channels = input_channels\n        self.output_channels = output_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n        self.groups = groups\n        self.use_bias = use_bias\n        self.rngs = rngs\n\n        # Set weights prior\n        if weights_distribution is None:\n            self.weights_distribution = GaussianDistribution(\n                shape=(\n                    self.output_channels,\n                    self.input_channels // self.groups,\n                    self.kernel_size,\n                ),\n                rngs=self.rngs,\n            )\n        else:\n            self.weights_distribution = weights_distribution\n\n        # Set bias prior\n        if self.use_bias:\n            if bias_distribution is None:\n                self.bias_distribution = GaussianDistribution(\n                    shape=(self.output_channels,),\n                    rngs=self.rngs,\n                )\n            else:\n                self.bias_distribution = bias_distribution\n        else:\n            self.bias_distribution = None\n\n        # Sample initial weights\n        self.weights = nnx.Param(self.weights_distribution.sample(self.rngs))\n\n        # Sample initial bias only if using bias\n        if self.use_bias and self.bias_distribution is not None:\n            self.bias = nnx.Param(self.bias_distribution.sample(self.rngs))\n        else:\n            self.bias = None\n\n    def freeze(self) -&gt; None:\n        \"\"\"\n        Freeze the module's parameters to stop gradient computation.\n        If weights or biases are not sampled yet, they are sampled first.\n        Once frozen, parameters are not resampled or updated.\n\n        Returns:\n            None.\n        \"\"\"\n\n        # Set indicator\n        self.frozen = True\n\n        # Sample weights if they are undefined\n        if self.weights is None:\n            self.weights = nnx.Param(self.weights_distribution.sample(self.rngs))\n\n        # Sample bias if they are undefined and bias is used\n        if self.use_bias and self.bias is None and self.bias_distribution is not None:\n            self.bias = nnx.Param(self.bias_distribution.sample(self.rngs))\n\n        # Stop gradient computation\n        self.weights = jax.lax.stop_gradient(self.weights)\n        if self.use_bias:\n            self.bias = jax.lax.stop_gradient(self.bias)\n\n    def kl_cost(self) -&gt; tuple[jax.Array, int]:\n        \"\"\"\n        Compute the KL divergence cost for all Bayesian parameters.\n\n        Returns:\n            tuple[jax.Array, int]: A tuple containing the KL divergence\n                cost and the total number of parameters in the layer.\n        \"\"\"\n\n        # Compute log probs for weights\n        log_probs: jax.Array = self.weights_distribution.log_prob(\n            jnp.asarray(self.weights)\n        )\n\n        # Add bias log probs only if using bias\n        if (\n            self.use_bias\n            and self.bias is not None\n            and self.bias_distribution is not None\n        ):\n            log_probs += self.bias_distribution.log_prob(jnp.asarray(self.bias))\n\n        # Compute number of parameters\n        num_params: int = self.weights_distribution.num_params\n        if self.use_bias and self.bias_distribution is not None:\n            num_params += self.bias_distribution.num_params\n\n        return log_probs, num_params\n\n    def __call__(self, inputs: jax.Array) -&gt; jax.Array:\n        \"\"\"\n        Performs a forward pass through the Bayesian Convolution 1D\n        layer. If the layer is not frozen, it samples weights and bias\n        from their respective distributions. If the layer is frozen\n        and the weights or bias are not initialized, it also performs\n        sampling.\n\n        Args:\n            inputs: Input tensor to the layer with shape\n                (batch, channels, length).\n\n        Returns:\n            Output array after convolution with optional bias added.\n\n        Raises:\n            ValueError: If the layer is frozen but weights or bias are\n                undefined.\n        \"\"\"\n\n        # Sample if model not frozen\n        if not self.frozen:\n            # Sample weights\n            self.weights = nnx.Param(self.weights_distribution.sample(self.rngs))\n\n            # Sample bias only if using bias\n            if self.use_bias and self.bias_distribution is not None:\n                self.bias = nnx.Param(self.bias_distribution.sample(self.rngs))\n        elif self.weights is None or (self.use_bias and self.bias is None):\n            raise ValueError(\n                \"Module has been frozen with undefined weights and/or bias.\"\n            )\n\n        # Compute outputs\n        outputs = jax.lax.conv_general_dilated(\n            lhs=inputs,\n            rhs=jnp.asarray(self.weights),\n            window_strides=[self.stride],\n            padding=[(self.padding, self.padding)],\n            lhs_dilation=[1],\n            rhs_dilation=[self.dilation],\n            dimension_numbers=(\n                \"NCH\",  # Input\n                \"OIH\",  # Kernel\n                \"NCH\",  # Output\n            ),\n            feature_group_count=self.groups,\n        )\n\n        # Add bias only if using bias\n        if self.use_bias and self.bias is not None:\n            outputs += jnp.reshape(\n                a=jnp.asarray(self.bias), shape=(1, self.output_channels, 1)\n            )\n\n        return outputs\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/nn.html#illia.nn.jax.conv1d.Conv1d.__call__","title":"<code>__call__(inputs)</code>","text":"<p>Performs a forward pass through the Bayesian Convolution 1D layer. If the layer is not frozen, it samples weights and bias from their respective distributions. If the layer is frozen and the weights or bias are not initialized, it also performs sampling.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Array</code> <p>Input tensor to the layer with shape (batch, channels, length).</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Output array after convolution with optional bias added.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the layer is frozen but weights or bias are undefined.</p> Source code in <code>illia/nn/jax/conv1d.py</code> <pre><code>def __call__(self, inputs: jax.Array) -&gt; jax.Array:\n    \"\"\"\n    Performs a forward pass through the Bayesian Convolution 1D\n    layer. If the layer is not frozen, it samples weights and bias\n    from their respective distributions. If the layer is frozen\n    and the weights or bias are not initialized, it also performs\n    sampling.\n\n    Args:\n        inputs: Input tensor to the layer with shape\n            (batch, channels, length).\n\n    Returns:\n        Output array after convolution with optional bias added.\n\n    Raises:\n        ValueError: If the layer is frozen but weights or bias are\n            undefined.\n    \"\"\"\n\n    # Sample if model not frozen\n    if not self.frozen:\n        # Sample weights\n        self.weights = nnx.Param(self.weights_distribution.sample(self.rngs))\n\n        # Sample bias only if using bias\n        if self.use_bias and self.bias_distribution is not None:\n            self.bias = nnx.Param(self.bias_distribution.sample(self.rngs))\n    elif self.weights is None or (self.use_bias and self.bias is None):\n        raise ValueError(\n            \"Module has been frozen with undefined weights and/or bias.\"\n        )\n\n    # Compute outputs\n    outputs = jax.lax.conv_general_dilated(\n        lhs=inputs,\n        rhs=jnp.asarray(self.weights),\n        window_strides=[self.stride],\n        padding=[(self.padding, self.padding)],\n        lhs_dilation=[1],\n        rhs_dilation=[self.dilation],\n        dimension_numbers=(\n            \"NCH\",  # Input\n            \"OIH\",  # Kernel\n            \"NCH\",  # Output\n        ),\n        feature_group_count=self.groups,\n    )\n\n    # Add bias only if using bias\n    if self.use_bias and self.bias is not None:\n        outputs += jnp.reshape(\n            a=jnp.asarray(self.bias), shape=(1, self.output_channels, 1)\n        )\n\n    return outputs\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/nn.html#illia.nn.jax.conv1d.Conv1d.__init__","title":"<code>__init__(input_channels, output_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, weights_distribution=None, bias_distribution=None, use_bias=True, rngs=nnx.Rngs(0), **kwargs)</code>","text":"<p>Initializes a Bayesian 1D convolutional layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_channels</code> <code>int</code> <p>Number of input feature channels.</p> required <code>output_channels</code> <code>int</code> <p>Number of output feature channels.</p> required <code>kernel_size</code> <code>int</code> <p>Size of the convolution kernel.</p> required <code>stride</code> <code>int</code> <p>Stride of the convolution operation.</p> <code>1</code> <code>padding</code> <code>int</code> <p>Amount of zero-padding on both sides.</p> <code>0</code> <code>dilation</code> <code>int</code> <p>Spacing between kernel elements.</p> <code>1</code> <code>groups</code> <code>int</code> <p>Number of blocked connections between input and output.</p> <code>1</code> <code>weights_distribution</code> <code>Optional[GaussianDistribution]</code> <p>Distribution to initialize weights.</p> <code>None</code> <code>bias_distribution</code> <code>Optional[GaussianDistribution]</code> <p>Distribution to initialize bias.</p> <code>None</code> <code>use_bias</code> <code>bool</code> <p>Whether to include a bias term.</p> <code>True</code> <code>rngs</code> <code>Rngs</code> <p>Random number generators for reproducibility.</p> <code>Rngs(0)</code> <code>**kwargs</code> <code>Any</code> <p>Extra arguments passed to the base class.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Notes <p>Gaussian distributions are used by default if none are provided.</p> Source code in <code>illia/nn/jax/conv1d.py</code> <pre><code>def __init__(\n    self,\n    input_channels: int,\n    output_channels: int,\n    kernel_size: int,\n    stride: int = 1,\n    padding: int = 0,\n    dilation: int = 1,\n    groups: int = 1,\n    weights_distribution: Optional[GaussianDistribution] = None,\n    bias_distribution: Optional[GaussianDistribution] = None,\n    use_bias: bool = True,\n    rngs: Rngs = nnx.Rngs(0),\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes a Bayesian 1D convolutional layer.\n\n    Args:\n        input_channels: Number of input feature channels.\n        output_channels: Number of output feature channels.\n        kernel_size: Size of the convolution kernel.\n        stride: Stride of the convolution operation.\n        padding: Amount of zero-padding on both sides.\n        dilation: Spacing between kernel elements.\n        groups: Number of blocked connections between input and output.\n        weights_distribution: Distribution to initialize weights.\n        bias_distribution: Distribution to initialize bias.\n        use_bias: Whether to include a bias term.\n        rngs: Random number generators for reproducibility.\n        **kwargs: Extra arguments passed to the base class.\n\n    Returns:\n        None.\n\n    Notes:\n        Gaussian distributions are used by default if none are\n        provided.\n    \"\"\"\n\n    # Call super class constructor\n    super().__init__(**kwargs)\n\n    # Set attributes\n    self.input_channels = input_channels\n    self.output_channels = output_channels\n    self.kernel_size = kernel_size\n    self.stride = stride\n    self.padding = padding\n    self.dilation = dilation\n    self.groups = groups\n    self.use_bias = use_bias\n    self.rngs = rngs\n\n    # Set weights prior\n    if weights_distribution is None:\n        self.weights_distribution = GaussianDistribution(\n            shape=(\n                self.output_channels,\n                self.input_channels // self.groups,\n                self.kernel_size,\n            ),\n            rngs=self.rngs,\n        )\n    else:\n        self.weights_distribution = weights_distribution\n\n    # Set bias prior\n    if self.use_bias:\n        if bias_distribution is None:\n            self.bias_distribution = GaussianDistribution(\n                shape=(self.output_channels,),\n                rngs=self.rngs,\n            )\n        else:\n            self.bias_distribution = bias_distribution\n    else:\n        self.bias_distribution = None\n\n    # Sample initial weights\n    self.weights = nnx.Param(self.weights_distribution.sample(self.rngs))\n\n    # Sample initial bias only if using bias\n    if self.use_bias and self.bias_distribution is not None:\n        self.bias = nnx.Param(self.bias_distribution.sample(self.rngs))\n    else:\n        self.bias = None\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/nn.html#illia.nn.jax.conv1d.Conv1d.freeze","title":"<code>freeze()</code>","text":"<p>Freeze the module's parameters to stop gradient computation. If weights or biases are not sampled yet, they are sampled first. Once frozen, parameters are not resampled or updated.</p> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>illia/nn/jax/conv1d.py</code> <pre><code>def freeze(self) -&gt; None:\n    \"\"\"\n    Freeze the module's parameters to stop gradient computation.\n    If weights or biases are not sampled yet, they are sampled first.\n    Once frozen, parameters are not resampled or updated.\n\n    Returns:\n        None.\n    \"\"\"\n\n    # Set indicator\n    self.frozen = True\n\n    # Sample weights if they are undefined\n    if self.weights is None:\n        self.weights = nnx.Param(self.weights_distribution.sample(self.rngs))\n\n    # Sample bias if they are undefined and bias is used\n    if self.use_bias and self.bias is None and self.bias_distribution is not None:\n        self.bias = nnx.Param(self.bias_distribution.sample(self.rngs))\n\n    # Stop gradient computation\n    self.weights = jax.lax.stop_gradient(self.weights)\n    if self.use_bias:\n        self.bias = jax.lax.stop_gradient(self.bias)\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/nn.html#illia.nn.jax.conv1d.Conv1d.kl_cost","title":"<code>kl_cost()</code>","text":"<p>Compute the KL divergence cost for all Bayesian parameters.</p> <p>Returns:</p> Type Description <code>tuple[Array, int]</code> <p>tuple[jax.Array, int]: A tuple containing the KL divergence cost and the total number of parameters in the layer.</p> Source code in <code>illia/nn/jax/conv1d.py</code> <pre><code>def kl_cost(self) -&gt; tuple[jax.Array, int]:\n    \"\"\"\n    Compute the KL divergence cost for all Bayesian parameters.\n\n    Returns:\n        tuple[jax.Array, int]: A tuple containing the KL divergence\n            cost and the total number of parameters in the layer.\n    \"\"\"\n\n    # Compute log probs for weights\n    log_probs: jax.Array = self.weights_distribution.log_prob(\n        jnp.asarray(self.weights)\n    )\n\n    # Add bias log probs only if using bias\n    if (\n        self.use_bias\n        and self.bias is not None\n        and self.bias_distribution is not None\n    ):\n        log_probs += self.bias_distribution.log_prob(jnp.asarray(self.bias))\n\n    # Compute number of parameters\n    num_params: int = self.weights_distribution.num_params\n    if self.use_bias and self.bias_distribution is not None:\n        num_params += self.bias_distribution.num_params\n\n    return log_probs, num_params\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/nn.html#illia.nn.jax.conv2d.Conv2d","title":"<code>Conv2d</code>","text":"<p>Bayesian 2D convolutional layer with optional weight and bias priors. Behaves like a standard Conv2d but treats weights and bias as random variables sampled from specified distributions. Parameters become fixed when the layer is frozen.</p> Source code in <code>illia/nn/jax/conv2d.py</code> <pre><code>class Conv2d(BayesianModule):\n    \"\"\"\n    Bayesian 2D convolutional layer with optional weight and bias priors.\n    Behaves like a standard Conv2d but treats weights and bias as random\n    variables sampled from specified distributions. Parameters become fixed\n    when the layer is frozen.\n    \"\"\"\n\n    bias_distribution: Optional[GaussianDistribution] = None\n    bias: Optional[nnx.Param] = None\n\n    def __init__(\n        self,\n        input_channels: int,\n        output_channels: int,\n        kernel_size: int | tuple[int, int],\n        stride: tuple[int, int] = (1, 1),\n        padding: tuple[int, int] = (0, 0),\n        dilation: tuple[int, int] = (1, 1),\n        groups: int = 1,\n        weights_distribution: Optional[GaussianDistribution] = None,\n        bias_distribution: Optional[GaussianDistribution] = None,\n        use_bias: bool = True,\n        rngs: Rngs = nnx.Rngs(0),\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes a Bayesian 2D convolutional layer.\n\n        Args:\n            input_channels: Number of input feature channels.\n            output_channels: Number of output feature channels.\n            kernel_size: Convolution kernel size. Int is converted to tuple.\n            stride: Stride of the convolution operation.\n            padding: Tuple specifying zero-padding for height and width.\n            dilation: Spacing between kernel elements.\n            groups: Number of blocked connections between input and output.\n            weights_distribution: Distribution to initialize weights.\n            bias_distribution: Distribution to initialize bias.\n            use_bias: Whether to include a bias term.\n            rngs: Random number generators for reproducibility.\n            **kwargs: Extra arguments passed to the base class.\n\n        Returns:\n            None.\n\n        Notes:\n            Gaussian distributions are used by default if none are\n            provided.\n        \"\"\"\n\n        # Call super class constructor\n        super().__init__(**kwargs)\n\n        # Set attributes\n        self.input_channels = input_channels\n        self.output_channels = output_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n        self.groups = groups\n        self.use_bias = use_bias\n        self.rngs = rngs\n\n        # Set weights distribution\n        if weights_distribution is None:\n            # Extend kernel if we only have 1 value\n            if isinstance(self.kernel_size, int):\n                self.kernel_size = (self.kernel_size, self.kernel_size)\n\n            self.weights_distribution: GaussianDistribution = GaussianDistribution(\n                shape=(\n                    self.output_channels,\n                    self.input_channels // self.groups,\n                    *self.kernel_size,\n                ),\n                rngs=self.rngs,\n            )\n        else:\n            self.weights_distribution = weights_distribution\n\n        # Set bias prior\n        if self.use_bias:\n            if bias_distribution is None:\n                # Define weights distribution\n                self.bias_distribution = GaussianDistribution(\n                    shape=(self.output_channels,),\n                    rngs=self.rngs,\n                )\n            else:\n                self.bias_distribution = bias_distribution\n        else:\n            self.bias_distribution = None\n\n        # Sample initial weights\n        self.weights = nnx.Param(self.weights_distribution.sample(self.rngs))\n\n        # Sample initial bias only if using bias\n        if self.use_bias and self.bias_distribution is not None:\n            self.bias = nnx.Param(self.bias_distribution.sample(self.rngs))\n        else:\n            self.bias = None\n\n    def freeze(self) -&gt; None:\n        \"\"\"\n        Freeze the module's parameters to stop gradient computation.\n        If weights or biases are not sampled yet, they are sampled first.\n        Once frozen, parameters are not resampled or updated.\n\n        Returns:\n            None.\n        \"\"\"\n\n        # Set indicator\n        self.frozen = True\n\n        # Sample weights if they are undefined\n        if self.weights is None:\n            self.weights = nnx.Param(self.weights_distribution.sample(self.rngs))\n\n        # Sample bias if they are undefined and bias is used\n        if self.use_bias and self.bias is None and self.bias_distribution is not None:\n            self.bias = nnx.Param(self.bias_distribution.sample(self.rngs))\n\n        # Stop gradient computation\n        self.weights = jax.lax.stop_gradient(self.weights)\n        if self.use_bias:\n            self.bias = jax.lax.stop_gradient(self.bias)\n\n    def kl_cost(self) -&gt; tuple[jax.Array, int]:\n        \"\"\"\n        Compute the KL divergence cost for all Bayesian parameters.\n\n        Returns:\n            tuple[jax.Array, int]: A tuple containing the KL divergence\n                cost and the total number of parameters in the layer.\n        \"\"\"\n\n        # Compute log probs for weights\n        log_probs: jax.Array = self.weights_distribution.log_prob(\n            jnp.asarray(self.weights)\n        )\n\n        # Add bias log probs only if using bias\n        if (\n            self.use_bias\n            and self.bias is not None\n            and self.bias_distribution is not None\n        ):\n            log_probs += self.bias_distribution.log_prob(jnp.asarray(self.bias))\n\n        # Compute number of parameters\n        num_params: int = self.weights_distribution.num_params\n        if self.use_bias and self.bias_distribution is not None:\n            num_params += self.bias_distribution.num_params\n\n        return log_probs, num_params\n\n    def __call__(self, inputs: jax.Array) -&gt; jax.Array:\n        \"\"\"\n        Performs a forward pass through the Bayesian Convolution 2D\n        layer. If the layer is not frozen, it samples weights and bias\n        from their respective distributions. If the layer is frozen\n        and the weights or bias are not initialized, it also performs\n        sampling.\n\n        Args:\n            inputs: Input array with shape (batch, channels, height,\n                width).\n\n        Returns:\n            Output array after convolution with optional bias addition.\n\n        Raises:\n            ValueError: If the layer is frozen but weights or bias are\n                undefined.\n        \"\"\"\n\n        # Sample if model not frozen\n        if not self.frozen:\n            # Sample weights\n            self.weights = nnx.Param(self.weights_distribution.sample(self.rngs))\n\n            # Sample bias only if using bias\n            if self.use_bias and self.bias_distribution is not None:\n                self.bias = nnx.Param(self.bias_distribution.sample(self.rngs))\n        elif self.weights is None or (self.use_bias and self.bias is None):\n            raise ValueError(\n                \"Module has been frozen with undefined weights and/or bias.\"\n            )\n\n        # Compute ouputs\n        outputs = jax.lax.conv_general_dilated(\n            lhs=inputs,\n            rhs=jnp.asarray(self.weights),\n            window_strides=self.stride,\n            padding=[self.padding, self.padding],\n            lhs_dilation=[1, 1],\n            rhs_dilation=self.dilation,\n            dimension_numbers=(\n                \"NCHW\",  # Input\n                \"OIHW\",  # Kernel\n                \"NCHW\",  # Output\n            ),\n            feature_group_count=self.groups,\n        )\n\n        # Add bias only if using bias\n        if self.use_bias and self.bias is not None:\n            outputs += jnp.reshape(\n                a=jnp.asarray(self.bias), shape=(1, self.output_channels, 1, 1)\n            )\n\n        return outputs\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/nn.html#illia.nn.jax.conv2d.Conv2d.__call__","title":"<code>__call__(inputs)</code>","text":"<p>Performs a forward pass through the Bayesian Convolution 2D layer. If the layer is not frozen, it samples weights and bias from their respective distributions. If the layer is frozen and the weights or bias are not initialized, it also performs sampling.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Array</code> <p>Input array with shape (batch, channels, height, width).</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Output array after convolution with optional bias addition.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the layer is frozen but weights or bias are undefined.</p> Source code in <code>illia/nn/jax/conv2d.py</code> <pre><code>def __call__(self, inputs: jax.Array) -&gt; jax.Array:\n    \"\"\"\n    Performs a forward pass through the Bayesian Convolution 2D\n    layer. If the layer is not frozen, it samples weights and bias\n    from their respective distributions. If the layer is frozen\n    and the weights or bias are not initialized, it also performs\n    sampling.\n\n    Args:\n        inputs: Input array with shape (batch, channels, height,\n            width).\n\n    Returns:\n        Output array after convolution with optional bias addition.\n\n    Raises:\n        ValueError: If the layer is frozen but weights or bias are\n            undefined.\n    \"\"\"\n\n    # Sample if model not frozen\n    if not self.frozen:\n        # Sample weights\n        self.weights = nnx.Param(self.weights_distribution.sample(self.rngs))\n\n        # Sample bias only if using bias\n        if self.use_bias and self.bias_distribution is not None:\n            self.bias = nnx.Param(self.bias_distribution.sample(self.rngs))\n    elif self.weights is None or (self.use_bias and self.bias is None):\n        raise ValueError(\n            \"Module has been frozen with undefined weights and/or bias.\"\n        )\n\n    # Compute ouputs\n    outputs = jax.lax.conv_general_dilated(\n        lhs=inputs,\n        rhs=jnp.asarray(self.weights),\n        window_strides=self.stride,\n        padding=[self.padding, self.padding],\n        lhs_dilation=[1, 1],\n        rhs_dilation=self.dilation,\n        dimension_numbers=(\n            \"NCHW\",  # Input\n            \"OIHW\",  # Kernel\n            \"NCHW\",  # Output\n        ),\n        feature_group_count=self.groups,\n    )\n\n    # Add bias only if using bias\n    if self.use_bias and self.bias is not None:\n        outputs += jnp.reshape(\n            a=jnp.asarray(self.bias), shape=(1, self.output_channels, 1, 1)\n        )\n\n    return outputs\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/nn.html#illia.nn.jax.conv2d.Conv2d.__init__","title":"<code>__init__(input_channels, output_channels, kernel_size, stride=(1, 1), padding=(0, 0), dilation=(1, 1), groups=1, weights_distribution=None, bias_distribution=None, use_bias=True, rngs=nnx.Rngs(0), **kwargs)</code>","text":"<p>Initializes a Bayesian 2D convolutional layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_channels</code> <code>int</code> <p>Number of input feature channels.</p> required <code>output_channels</code> <code>int</code> <p>Number of output feature channels.</p> required <code>kernel_size</code> <code>int | tuple[int, int]</code> <p>Convolution kernel size. Int is converted to tuple.</p> required <code>stride</code> <code>tuple[int, int]</code> <p>Stride of the convolution operation.</p> <code>(1, 1)</code> <code>padding</code> <code>tuple[int, int]</code> <p>Tuple specifying zero-padding for height and width.</p> <code>(0, 0)</code> <code>dilation</code> <code>tuple[int, int]</code> <p>Spacing between kernel elements.</p> <code>(1, 1)</code> <code>groups</code> <code>int</code> <p>Number of blocked connections between input and output.</p> <code>1</code> <code>weights_distribution</code> <code>Optional[GaussianDistribution]</code> <p>Distribution to initialize weights.</p> <code>None</code> <code>bias_distribution</code> <code>Optional[GaussianDistribution]</code> <p>Distribution to initialize bias.</p> <code>None</code> <code>use_bias</code> <code>bool</code> <p>Whether to include a bias term.</p> <code>True</code> <code>rngs</code> <code>Rngs</code> <p>Random number generators for reproducibility.</p> <code>Rngs(0)</code> <code>**kwargs</code> <code>Any</code> <p>Extra arguments passed to the base class.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Notes <p>Gaussian distributions are used by default if none are provided.</p> Source code in <code>illia/nn/jax/conv2d.py</code> <pre><code>def __init__(\n    self,\n    input_channels: int,\n    output_channels: int,\n    kernel_size: int | tuple[int, int],\n    stride: tuple[int, int] = (1, 1),\n    padding: tuple[int, int] = (0, 0),\n    dilation: tuple[int, int] = (1, 1),\n    groups: int = 1,\n    weights_distribution: Optional[GaussianDistribution] = None,\n    bias_distribution: Optional[GaussianDistribution] = None,\n    use_bias: bool = True,\n    rngs: Rngs = nnx.Rngs(0),\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes a Bayesian 2D convolutional layer.\n\n    Args:\n        input_channels: Number of input feature channels.\n        output_channels: Number of output feature channels.\n        kernel_size: Convolution kernel size. Int is converted to tuple.\n        stride: Stride of the convolution operation.\n        padding: Tuple specifying zero-padding for height and width.\n        dilation: Spacing between kernel elements.\n        groups: Number of blocked connections between input and output.\n        weights_distribution: Distribution to initialize weights.\n        bias_distribution: Distribution to initialize bias.\n        use_bias: Whether to include a bias term.\n        rngs: Random number generators for reproducibility.\n        **kwargs: Extra arguments passed to the base class.\n\n    Returns:\n        None.\n\n    Notes:\n        Gaussian distributions are used by default if none are\n        provided.\n    \"\"\"\n\n    # Call super class constructor\n    super().__init__(**kwargs)\n\n    # Set attributes\n    self.input_channels = input_channels\n    self.output_channels = output_channels\n    self.kernel_size = kernel_size\n    self.stride = stride\n    self.padding = padding\n    self.dilation = dilation\n    self.groups = groups\n    self.use_bias = use_bias\n    self.rngs = rngs\n\n    # Set weights distribution\n    if weights_distribution is None:\n        # Extend kernel if we only have 1 value\n        if isinstance(self.kernel_size, int):\n            self.kernel_size = (self.kernel_size, self.kernel_size)\n\n        self.weights_distribution: GaussianDistribution = GaussianDistribution(\n            shape=(\n                self.output_channels,\n                self.input_channels // self.groups,\n                *self.kernel_size,\n            ),\n            rngs=self.rngs,\n        )\n    else:\n        self.weights_distribution = weights_distribution\n\n    # Set bias prior\n    if self.use_bias:\n        if bias_distribution is None:\n            # Define weights distribution\n            self.bias_distribution = GaussianDistribution(\n                shape=(self.output_channels,),\n                rngs=self.rngs,\n            )\n        else:\n            self.bias_distribution = bias_distribution\n    else:\n        self.bias_distribution = None\n\n    # Sample initial weights\n    self.weights = nnx.Param(self.weights_distribution.sample(self.rngs))\n\n    # Sample initial bias only if using bias\n    if self.use_bias and self.bias_distribution is not None:\n        self.bias = nnx.Param(self.bias_distribution.sample(self.rngs))\n    else:\n        self.bias = None\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/nn.html#illia.nn.jax.conv2d.Conv2d.freeze","title":"<code>freeze()</code>","text":"<p>Freeze the module's parameters to stop gradient computation. If weights or biases are not sampled yet, they are sampled first. Once frozen, parameters are not resampled or updated.</p> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>illia/nn/jax/conv2d.py</code> <pre><code>def freeze(self) -&gt; None:\n    \"\"\"\n    Freeze the module's parameters to stop gradient computation.\n    If weights or biases are not sampled yet, they are sampled first.\n    Once frozen, parameters are not resampled or updated.\n\n    Returns:\n        None.\n    \"\"\"\n\n    # Set indicator\n    self.frozen = True\n\n    # Sample weights if they are undefined\n    if self.weights is None:\n        self.weights = nnx.Param(self.weights_distribution.sample(self.rngs))\n\n    # Sample bias if they are undefined and bias is used\n    if self.use_bias and self.bias is None and self.bias_distribution is not None:\n        self.bias = nnx.Param(self.bias_distribution.sample(self.rngs))\n\n    # Stop gradient computation\n    self.weights = jax.lax.stop_gradient(self.weights)\n    if self.use_bias:\n        self.bias = jax.lax.stop_gradient(self.bias)\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/nn.html#illia.nn.jax.conv2d.Conv2d.kl_cost","title":"<code>kl_cost()</code>","text":"<p>Compute the KL divergence cost for all Bayesian parameters.</p> <p>Returns:</p> Type Description <code>tuple[Array, int]</code> <p>tuple[jax.Array, int]: A tuple containing the KL divergence cost and the total number of parameters in the layer.</p> Source code in <code>illia/nn/jax/conv2d.py</code> <pre><code>def kl_cost(self) -&gt; tuple[jax.Array, int]:\n    \"\"\"\n    Compute the KL divergence cost for all Bayesian parameters.\n\n    Returns:\n        tuple[jax.Array, int]: A tuple containing the KL divergence\n            cost and the total number of parameters in the layer.\n    \"\"\"\n\n    # Compute log probs for weights\n    log_probs: jax.Array = self.weights_distribution.log_prob(\n        jnp.asarray(self.weights)\n    )\n\n    # Add bias log probs only if using bias\n    if (\n        self.use_bias\n        and self.bias is not None\n        and self.bias_distribution is not None\n    ):\n        log_probs += self.bias_distribution.log_prob(jnp.asarray(self.bias))\n\n    # Compute number of parameters\n    num_params: int = self.weights_distribution.num_params\n    if self.use_bias and self.bias_distribution is not None:\n        num_params += self.bias_distribution.num_params\n\n    return log_probs, num_params\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/nn.html#illia.nn.jax.embedding.Embedding","title":"<code>Embedding</code>","text":"<p>Bayesian embedding layer with optional padding and max-norm constraints. Each embedding vector is sampled from a specified weight distribution. If the layer is frozen, embeddings are fixed and gradients are stopped.</p> Source code in <code>illia/nn/jax/embedding.py</code> <pre><code>class Embedding(BayesianModule):\n    \"\"\"\n    Bayesian embedding layer with optional padding and max-norm constraints.\n    Each embedding vector is sampled from a specified weight distribution.\n    If the layer is frozen, embeddings are fixed and gradients are stopped.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_embeddings: int,\n        embeddings_dim: int,\n        padding_idx: Optional[int] = None,\n        max_norm: Optional[float] = None,\n        norm_type: float = 2.0,\n        scale_grad_by_freq: bool = False,\n        weights_distribution: Optional[GaussianDistribution] = None,\n        rngs: Rngs = nnx.Rngs(0),\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initialize a Bayesian embedding layer with optional constraints.\n        Sets up the embedding weight distribution and samples initial values.\n\n        Args:\n            num_embeddings: Size of the embedding dictionary.\n            embeddings_dim: Dimension of each embedding vector.\n            padding_idx: Index whose embeddings are ignored in gradient.\n            max_norm: Maximum norm for each embedding vector.\n            norm_type: p value for the p-norm in max_norm option.\n            scale_grad_by_freq: Scale gradients by inverse word frequency.\n            weights_distribution: Distribution to initialize embeddings.\n            rngs: Random number generators for reproducibility.\n            **kwargs: Extra arguments passed to the base class.\n\n        Returns:\n            None.\n\n        Notes:\n            Gaussian distributions are used by default if none are\n            provided.\n        \"\"\"\n\n        # Call super class constructor\n        super().__init__(**kwargs)\n\n        # Set attributes\n        self.num_embeddings = num_embeddings\n        self.embeddings_dim = embeddings_dim\n        self.padding_idx = padding_idx\n        self.max_norm = max_norm\n        self.norm_type = norm_type\n        self.scale_grad_by_freq = scale_grad_by_freq\n        self.rngs = rngs\n\n        # Set weights distribution\n        if weights_distribution is None:\n            self.weights_distribution = GaussianDistribution(\n                (self.num_embeddings, self.embeddings_dim)\n            )\n        else:\n            self.weights_distribution = weights_distribution\n\n        # Sample initial weights\n        self.weights = nnx.Param(self.weights_distribution.sample(self.rngs))\n\n    def freeze(self) -&gt; None:\n        \"\"\"\n        Freeze the module's parameters to stop gradient computation.\n        If weights or biases are not sampled yet, they are sampled first.\n        Once frozen, parameters are not resampled or updated.\n\n        Returns:\n            None.\n        \"\"\"\n\n        # Set indicator\n        self.frozen = True\n\n        # Sample weights if they are undefined\n        if self.weights is None:\n            self.weights = nnx.Param(self.weights_distribution.sample(self.rngs))\n\n        # Stop gradient computation\n        self.weights = jax.lax.stop_gradient(self.weights)\n\n    def kl_cost(self) -&gt; tuple[jax.Array, int]:\n        \"\"\"\n        Compute the KL divergence cost for all Bayesian parameters.\n\n        Returns:\n            tuple[jax.Array, int]: A tuple containing the KL divergence\n                cost and the total number of parameters in the layer.\n        \"\"\"\n\n        # Compute log probs for weights\n        log_probs: jax.Array = self.weights_distribution.log_prob(\n            jnp.asarray(self.weights)\n        )\n\n        # get number of parameters\n        num_params: int = self.weights_distribution.num_params\n\n        return log_probs, num_params\n\n    def __call__(self, inputs: jax.Array) -&gt; jax.Array:\n        \"\"\"\n        Perform a forward pass using current embedding weights.\n\n        Args:\n            inputs: Array of indices into the embedding matrix.\n\n        Returns:\n            Array of shape [*, embeddings_dim] containing the embedding\n            vectors corresponding to the input indices.\n\n        Raises:\n            ValueError: If the layer is frozen but weights are\n                undefined.\n\n        Notes:\n            Embeddings at padding_idx are zeroed out, and vectors exceeding\n            max_norm are renormalized if specified.\n        \"\"\"\n\n        # Sample if model not frozen\n        if not self.frozen:\n            # Sample weights\n            self.weights = nnx.Param(self.weights_distribution.sample(self.rngs))\n        elif self.weights is None:\n            raise ValueError(\"Module has been frozen with undefined weights.\")\n\n        # Perform embedding lookup\n        outputs = self.weights.value[inputs]\n\n        # Apply padding_idx\n        if self.padding_idx is not None:\n            # Create mask for padding indices\n            mask = inputs == self.padding_idx\n            # Zero out embeddings for padding indices\n            outputs = jnp.where(mask[..., None], 0.0, outputs)\n\n        # Apply max_norm\n        if self.max_norm is not None:\n            norms = jnp.linalg.norm(outputs, axis=-1, ord=self.norm_type, keepdims=True)\n            # Normalize vectors that exceed max_norm\n            scale = jnp.minimum(1.0, self.max_norm / (norms + 1e-8))\n            outputs = outputs * scale\n\n        return outputs\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/nn.html#illia.nn.jax.embedding.Embedding.__call__","title":"<code>__call__(inputs)</code>","text":"<p>Perform a forward pass using current embedding weights.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Array</code> <p>Array of indices into the embedding matrix.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Array of shape [*, embeddings_dim] containing the embedding</p> <code>Array</code> <p>vectors corresponding to the input indices.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the layer is frozen but weights are undefined.</p> Notes <p>Embeddings at padding_idx are zeroed out, and vectors exceeding max_norm are renormalized if specified.</p> Source code in <code>illia/nn/jax/embedding.py</code> <pre><code>def __call__(self, inputs: jax.Array) -&gt; jax.Array:\n    \"\"\"\n    Perform a forward pass using current embedding weights.\n\n    Args:\n        inputs: Array of indices into the embedding matrix.\n\n    Returns:\n        Array of shape [*, embeddings_dim] containing the embedding\n        vectors corresponding to the input indices.\n\n    Raises:\n        ValueError: If the layer is frozen but weights are\n            undefined.\n\n    Notes:\n        Embeddings at padding_idx are zeroed out, and vectors exceeding\n        max_norm are renormalized if specified.\n    \"\"\"\n\n    # Sample if model not frozen\n    if not self.frozen:\n        # Sample weights\n        self.weights = nnx.Param(self.weights_distribution.sample(self.rngs))\n    elif self.weights is None:\n        raise ValueError(\"Module has been frozen with undefined weights.\")\n\n    # Perform embedding lookup\n    outputs = self.weights.value[inputs]\n\n    # Apply padding_idx\n    if self.padding_idx is not None:\n        # Create mask for padding indices\n        mask = inputs == self.padding_idx\n        # Zero out embeddings for padding indices\n        outputs = jnp.where(mask[..., None], 0.0, outputs)\n\n    # Apply max_norm\n    if self.max_norm is not None:\n        norms = jnp.linalg.norm(outputs, axis=-1, ord=self.norm_type, keepdims=True)\n        # Normalize vectors that exceed max_norm\n        scale = jnp.minimum(1.0, self.max_norm / (norms + 1e-8))\n        outputs = outputs * scale\n\n    return outputs\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/nn.html#illia.nn.jax.embedding.Embedding.__init__","title":"<code>__init__(num_embeddings, embeddings_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, weights_distribution=None, rngs=nnx.Rngs(0), **kwargs)</code>","text":"<p>Initialize a Bayesian embedding layer with optional constraints. Sets up the embedding weight distribution and samples initial values.</p> <p>Parameters:</p> Name Type Description Default <code>num_embeddings</code> <code>int</code> <p>Size of the embedding dictionary.</p> required <code>embeddings_dim</code> <code>int</code> <p>Dimension of each embedding vector.</p> required <code>padding_idx</code> <code>Optional[int]</code> <p>Index whose embeddings are ignored in gradient.</p> <code>None</code> <code>max_norm</code> <code>Optional[float]</code> <p>Maximum norm for each embedding vector.</p> <code>None</code> <code>norm_type</code> <code>float</code> <p>p value for the p-norm in max_norm option.</p> <code>2.0</code> <code>scale_grad_by_freq</code> <code>bool</code> <p>Scale gradients by inverse word frequency.</p> <code>False</code> <code>weights_distribution</code> <code>Optional[GaussianDistribution]</code> <p>Distribution to initialize embeddings.</p> <code>None</code> <code>rngs</code> <code>Rngs</code> <p>Random number generators for reproducibility.</p> <code>Rngs(0)</code> <code>**kwargs</code> <code>Any</code> <p>Extra arguments passed to the base class.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Notes <p>Gaussian distributions are used by default if none are provided.</p> Source code in <code>illia/nn/jax/embedding.py</code> <pre><code>def __init__(\n    self,\n    num_embeddings: int,\n    embeddings_dim: int,\n    padding_idx: Optional[int] = None,\n    max_norm: Optional[float] = None,\n    norm_type: float = 2.0,\n    scale_grad_by_freq: bool = False,\n    weights_distribution: Optional[GaussianDistribution] = None,\n    rngs: Rngs = nnx.Rngs(0),\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialize a Bayesian embedding layer with optional constraints.\n    Sets up the embedding weight distribution and samples initial values.\n\n    Args:\n        num_embeddings: Size of the embedding dictionary.\n        embeddings_dim: Dimension of each embedding vector.\n        padding_idx: Index whose embeddings are ignored in gradient.\n        max_norm: Maximum norm for each embedding vector.\n        norm_type: p value for the p-norm in max_norm option.\n        scale_grad_by_freq: Scale gradients by inverse word frequency.\n        weights_distribution: Distribution to initialize embeddings.\n        rngs: Random number generators for reproducibility.\n        **kwargs: Extra arguments passed to the base class.\n\n    Returns:\n        None.\n\n    Notes:\n        Gaussian distributions are used by default if none are\n        provided.\n    \"\"\"\n\n    # Call super class constructor\n    super().__init__(**kwargs)\n\n    # Set attributes\n    self.num_embeddings = num_embeddings\n    self.embeddings_dim = embeddings_dim\n    self.padding_idx = padding_idx\n    self.max_norm = max_norm\n    self.norm_type = norm_type\n    self.scale_grad_by_freq = scale_grad_by_freq\n    self.rngs = rngs\n\n    # Set weights distribution\n    if weights_distribution is None:\n        self.weights_distribution = GaussianDistribution(\n            (self.num_embeddings, self.embeddings_dim)\n        )\n    else:\n        self.weights_distribution = weights_distribution\n\n    # Sample initial weights\n    self.weights = nnx.Param(self.weights_distribution.sample(self.rngs))\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/nn.html#illia.nn.jax.embedding.Embedding.freeze","title":"<code>freeze()</code>","text":"<p>Freeze the module's parameters to stop gradient computation. If weights or biases are not sampled yet, they are sampled first. Once frozen, parameters are not resampled or updated.</p> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>illia/nn/jax/embedding.py</code> <pre><code>def freeze(self) -&gt; None:\n    \"\"\"\n    Freeze the module's parameters to stop gradient computation.\n    If weights or biases are not sampled yet, they are sampled first.\n    Once frozen, parameters are not resampled or updated.\n\n    Returns:\n        None.\n    \"\"\"\n\n    # Set indicator\n    self.frozen = True\n\n    # Sample weights if they are undefined\n    if self.weights is None:\n        self.weights = nnx.Param(self.weights_distribution.sample(self.rngs))\n\n    # Stop gradient computation\n    self.weights = jax.lax.stop_gradient(self.weights)\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/nn.html#illia.nn.jax.embedding.Embedding.kl_cost","title":"<code>kl_cost()</code>","text":"<p>Compute the KL divergence cost for all Bayesian parameters.</p> <p>Returns:</p> Type Description <code>tuple[Array, int]</code> <p>tuple[jax.Array, int]: A tuple containing the KL divergence cost and the total number of parameters in the layer.</p> Source code in <code>illia/nn/jax/embedding.py</code> <pre><code>def kl_cost(self) -&gt; tuple[jax.Array, int]:\n    \"\"\"\n    Compute the KL divergence cost for all Bayesian parameters.\n\n    Returns:\n        tuple[jax.Array, int]: A tuple containing the KL divergence\n            cost and the total number of parameters in the layer.\n    \"\"\"\n\n    # Compute log probs for weights\n    log_probs: jax.Array = self.weights_distribution.log_prob(\n        jnp.asarray(self.weights)\n    )\n\n    # get number of parameters\n    num_params: int = self.weights_distribution.num_params\n\n    return log_probs, num_params\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/nn.html#illia.nn.jax.linear.Linear","title":"<code>Linear</code>","text":"<p>Bayesian linear (fully connected) layer with optional weight and bias priors. Functions like a standard linear layer but treats weights and bias as probabilistic variables. Freezing the layer fixes parameters and stops gradient computation.</p> Source code in <code>illia/nn/jax/linear.py</code> <pre><code>class Linear(BayesianModule):\n    \"\"\"\n    Bayesian linear (fully connected) layer with optional weight and bias\n    priors. Functions like a standard linear layer but treats weights and\n    bias as probabilistic variables. Freezing the layer fixes parameters\n    and stops gradient computation.\n    \"\"\"\n\n    bias_distribution: Optional[GaussianDistribution] = None\n    bias: Optional[nnx.Param] = None\n\n    def __init__(\n        self,\n        input_size: int,\n        output_size: int,\n        weights_distribution: Optional[GaussianDistribution] = None,\n        bias_distribution: Optional[GaussianDistribution] = None,\n        use_bias: bool = True,\n        precision: PrecisionLike = None,\n        dot_general: DotGeneralT = lax.dot_general,\n        rngs: Rngs = nnx.Rngs(0),\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initialize a Bayesian linear layer with optional priors for weights\n        and bias. Samples initial parameter values from the specified\n        distributions.\n\n        Args:\n            input_size: Number of input features.\n            output_size: Number of output features.\n            weights_distribution: Distribution for weights.\n            bias_distribution: Distribution for bias.\n            use_bias: Whether to include a bias term.\n            precision: Precision for dot product computations.\n            dot_general: Function for generalized dot products.\n            rngs: Random number generators for reproducibility.\n            **kwargs: Extra arguments passed to the base class.\n\n        Returns:\n            None.\n\n        Notes:\n            Gaussian distributions are used by default if none are\n            provided.\n        \"\"\"\n\n        # Call super class constructor\n        super().__init__(**kwargs)\n\n        # Set attributes\n        self.input_size = input_size\n        self.output_size = output_size\n        self.use_bias = use_bias\n        self.precision = precision\n        self.dot_general = dot_general\n        self.rngs = rngs\n\n        # Set weights prior\n        if weights_distribution is None:\n            self.weights_distribution = GaussianDistribution(\n                shape=(self.output_size, self.input_size), rngs=self.rngs\n            )\n        else:\n            self.weights_distribution = weights_distribution\n\n        # Set bias prior\n        if self.use_bias:\n            if bias_distribution is None:\n                self.bias_distribution = GaussianDistribution(\n                    shape=(self.output_size,), rngs=self.rngs\n                )\n            else:\n                self.bias_distribution = bias_distribution\n        else:\n            self.bias_distribution = None\n\n        # Sample initial weights\n        self.weights = nnx.Param(self.weights_distribution.sample(self.rngs))\n\n        # Sample initial bias only if using bias\n        if self.use_bias and self.bias_distribution is not None:\n            self.bias = nnx.Param(self.bias_distribution.sample(self.rngs))\n        else:\n            self.bias = None\n\n    def freeze(self) -&gt; None:\n        \"\"\"\n        Freeze the module's parameters to stop gradient computation.\n        If weights or biases are not sampled yet, they are sampled first.\n        Once frozen, parameters are not resampled or updated.\n\n        Returns:\n            None.\n        \"\"\"\n\n        # Set indicator\n        self.frozen = True\n\n        # Sample weights if they are undefined\n        if self.weights is None:\n            self.weights = nnx.Param(self.weights_distribution.sample(self.rngs))\n\n        # Sample bias if they are undefined and bias is used\n        if self.use_bias and self.bias is None and self.bias_distribution is not None:\n            self.bias = nnx.Param(self.bias_distribution.sample(self.rngs))\n\n        # Stop gradient computation\n        self.weights = jax.lax.stop_gradient(self.weights)\n        if self.use_bias:\n            self.bias = jax.lax.stop_gradient(self.bias)\n\n    def kl_cost(self) -&gt; tuple[jax.Array, int]:\n        \"\"\"\n        Compute the KL divergence cost for all Bayesian parameters.\n\n        Returns:\n            tuple[jax.Array, int]: A tuple containing the KL divergence\n                cost and the total number of parameters in the layer.\n        \"\"\"\n\n        # Compute log probs for weights\n        log_probs: jax.Array = self.weights_distribution.log_prob(\n            jnp.asarray(self.weights)\n        )\n\n        # Add bias log probs only if using bias\n        if (\n            self.use_bias\n            and self.bias is not None\n            and self.bias_distribution is not None\n        ):\n            log_probs += self.bias_distribution.log_prob(jnp.asarray(self.bias))\n\n        # Compute number of parameters\n        num_params: int = self.weights_distribution.num_params\n        if self.use_bias and self.bias_distribution is not None:\n            num_params += self.bias_distribution.num_params\n\n        return log_probs, num_params\n\n    def __call__(self, inputs: jax.Array) -&gt; jax.Array:\n        \"\"\"\n        Perform a forward pass using current weights and bias. Samples new\n        parameters if the layer is not frozen.\n\n        Args:\n            inputs: Input array with shape [*, input_size].\n\n        Returns:\n            Output array with shape [*, output_size].\n\n        Raises:\n            ValueError: If the layer is frozen but weights or bias are\n                undefined.\n        \"\"\"\n\n        # Sample if model not frozen\n        if not self.frozen:\n            # Sample weights\n            self.weights = nnx.Param(self.weights_distribution.sample(self.rngs))\n\n            # Sample bias only if using bias\n            if self.use_bias and self.bias_distribution is not None:\n                self.bias = nnx.Param(self.bias_distribution.sample(self.rngs))\n        elif self.weights is None or (self.use_bias and self.bias is None):\n            raise ValueError(\n                \"Module has been frozen with undefined weights and/or bias.\"\n            )\n\n        # Compute outputs\n        outputs = inputs @ self.weights.T\n\n        # Add bias only if using bias\n        if self.use_bias and self.bias is not None:\n            outputs += jnp.reshape(\n                jnp.asarray(self.bias), (1,) * (outputs.ndim - 1) + (-1,)\n            )\n\n        return outputs\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/nn.html#illia.nn.jax.linear.Linear.__call__","title":"<code>__call__(inputs)</code>","text":"<p>Perform a forward pass using current weights and bias. Samples new parameters if the layer is not frozen.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Array</code> <p>Input array with shape [*, input_size].</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Output array with shape [*, output_size].</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the layer is frozen but weights or bias are undefined.</p> Source code in <code>illia/nn/jax/linear.py</code> <pre><code>def __call__(self, inputs: jax.Array) -&gt; jax.Array:\n    \"\"\"\n    Perform a forward pass using current weights and bias. Samples new\n    parameters if the layer is not frozen.\n\n    Args:\n        inputs: Input array with shape [*, input_size].\n\n    Returns:\n        Output array with shape [*, output_size].\n\n    Raises:\n        ValueError: If the layer is frozen but weights or bias are\n            undefined.\n    \"\"\"\n\n    # Sample if model not frozen\n    if not self.frozen:\n        # Sample weights\n        self.weights = nnx.Param(self.weights_distribution.sample(self.rngs))\n\n        # Sample bias only if using bias\n        if self.use_bias and self.bias_distribution is not None:\n            self.bias = nnx.Param(self.bias_distribution.sample(self.rngs))\n    elif self.weights is None or (self.use_bias and self.bias is None):\n        raise ValueError(\n            \"Module has been frozen with undefined weights and/or bias.\"\n        )\n\n    # Compute outputs\n    outputs = inputs @ self.weights.T\n\n    # Add bias only if using bias\n    if self.use_bias and self.bias is not None:\n        outputs += jnp.reshape(\n            jnp.asarray(self.bias), (1,) * (outputs.ndim - 1) + (-1,)\n        )\n\n    return outputs\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/nn.html#illia.nn.jax.linear.Linear.__init__","title":"<code>__init__(input_size, output_size, weights_distribution=None, bias_distribution=None, use_bias=True, precision=None, dot_general=lax.dot_general, rngs=nnx.Rngs(0), **kwargs)</code>","text":"<p>Initialize a Bayesian linear layer with optional priors for weights and bias. Samples initial parameter values from the specified distributions.</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>int</code> <p>Number of input features.</p> required <code>output_size</code> <code>int</code> <p>Number of output features.</p> required <code>weights_distribution</code> <code>Optional[GaussianDistribution]</code> <p>Distribution for weights.</p> <code>None</code> <code>bias_distribution</code> <code>Optional[GaussianDistribution]</code> <p>Distribution for bias.</p> <code>None</code> <code>use_bias</code> <code>bool</code> <p>Whether to include a bias term.</p> <code>True</code> <code>precision</code> <code>PrecisionLike</code> <p>Precision for dot product computations.</p> <code>None</code> <code>dot_general</code> <code>DotGeneralT</code> <p>Function for generalized dot products.</p> <code>dot_general</code> <code>rngs</code> <code>Rngs</code> <p>Random number generators for reproducibility.</p> <code>Rngs(0)</code> <code>**kwargs</code> <code>Any</code> <p>Extra arguments passed to the base class.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Notes <p>Gaussian distributions are used by default if none are provided.</p> Source code in <code>illia/nn/jax/linear.py</code> <pre><code>def __init__(\n    self,\n    input_size: int,\n    output_size: int,\n    weights_distribution: Optional[GaussianDistribution] = None,\n    bias_distribution: Optional[GaussianDistribution] = None,\n    use_bias: bool = True,\n    precision: PrecisionLike = None,\n    dot_general: DotGeneralT = lax.dot_general,\n    rngs: Rngs = nnx.Rngs(0),\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialize a Bayesian linear layer with optional priors for weights\n    and bias. Samples initial parameter values from the specified\n    distributions.\n\n    Args:\n        input_size: Number of input features.\n        output_size: Number of output features.\n        weights_distribution: Distribution for weights.\n        bias_distribution: Distribution for bias.\n        use_bias: Whether to include a bias term.\n        precision: Precision for dot product computations.\n        dot_general: Function for generalized dot products.\n        rngs: Random number generators for reproducibility.\n        **kwargs: Extra arguments passed to the base class.\n\n    Returns:\n        None.\n\n    Notes:\n        Gaussian distributions are used by default if none are\n        provided.\n    \"\"\"\n\n    # Call super class constructor\n    super().__init__(**kwargs)\n\n    # Set attributes\n    self.input_size = input_size\n    self.output_size = output_size\n    self.use_bias = use_bias\n    self.precision = precision\n    self.dot_general = dot_general\n    self.rngs = rngs\n\n    # Set weights prior\n    if weights_distribution is None:\n        self.weights_distribution = GaussianDistribution(\n            shape=(self.output_size, self.input_size), rngs=self.rngs\n        )\n    else:\n        self.weights_distribution = weights_distribution\n\n    # Set bias prior\n    if self.use_bias:\n        if bias_distribution is None:\n            self.bias_distribution = GaussianDistribution(\n                shape=(self.output_size,), rngs=self.rngs\n            )\n        else:\n            self.bias_distribution = bias_distribution\n    else:\n        self.bias_distribution = None\n\n    # Sample initial weights\n    self.weights = nnx.Param(self.weights_distribution.sample(self.rngs))\n\n    # Sample initial bias only if using bias\n    if self.use_bias and self.bias_distribution is not None:\n        self.bias = nnx.Param(self.bias_distribution.sample(self.rngs))\n    else:\n        self.bias = None\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/nn.html#illia.nn.jax.linear.Linear.freeze","title":"<code>freeze()</code>","text":"<p>Freeze the module's parameters to stop gradient computation. If weights or biases are not sampled yet, they are sampled first. Once frozen, parameters are not resampled or updated.</p> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>illia/nn/jax/linear.py</code> <pre><code>def freeze(self) -&gt; None:\n    \"\"\"\n    Freeze the module's parameters to stop gradient computation.\n    If weights or biases are not sampled yet, they are sampled first.\n    Once frozen, parameters are not resampled or updated.\n\n    Returns:\n        None.\n    \"\"\"\n\n    # Set indicator\n    self.frozen = True\n\n    # Sample weights if they are undefined\n    if self.weights is None:\n        self.weights = nnx.Param(self.weights_distribution.sample(self.rngs))\n\n    # Sample bias if they are undefined and bias is used\n    if self.use_bias and self.bias is None and self.bias_distribution is not None:\n        self.bias = nnx.Param(self.bias_distribution.sample(self.rngs))\n\n    # Stop gradient computation\n    self.weights = jax.lax.stop_gradient(self.weights)\n    if self.use_bias:\n        self.bias = jax.lax.stop_gradient(self.bias)\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/nn.html#illia.nn.jax.linear.Linear.kl_cost","title":"<code>kl_cost()</code>","text":"<p>Compute the KL divergence cost for all Bayesian parameters.</p> <p>Returns:</p> Type Description <code>tuple[Array, int]</code> <p>tuple[jax.Array, int]: A tuple containing the KL divergence cost and the total number of parameters in the layer.</p> Source code in <code>illia/nn/jax/linear.py</code> <pre><code>def kl_cost(self) -&gt; tuple[jax.Array, int]:\n    \"\"\"\n    Compute the KL divergence cost for all Bayesian parameters.\n\n    Returns:\n        tuple[jax.Array, int]: A tuple containing the KL divergence\n            cost and the total number of parameters in the layer.\n    \"\"\"\n\n    # Compute log probs for weights\n    log_probs: jax.Array = self.weights_distribution.log_prob(\n        jnp.asarray(self.weights)\n    )\n\n    # Add bias log probs only if using bias\n    if (\n        self.use_bias\n        and self.bias is not None\n        and self.bias_distribution is not None\n    ):\n        log_probs += self.bias_distribution.log_prob(jnp.asarray(self.bias))\n\n    # Compute number of parameters\n    num_params: int = self.weights_distribution.num_params\n    if self.use_bias and self.bias_distribution is not None:\n        num_params += self.bias_distribution.num_params\n\n    return log_probs, num_params\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/nn.html#illia.nn.jax.lstm.LSTM","title":"<code>LSTM</code>","text":"<p>Bayesian LSTM layer with embedding and probabilistic weights. All weights and biases are treated as random variables sampled from Gaussian distributions. Freezing the layer fixes parameters and stops gradient computation.</p> Source code in <code>illia/nn/jax/lstm.py</code> <pre><code>class LSTM(BayesianModule):\n    \"\"\"\n    Bayesian LSTM layer with embedding and probabilistic weights.\n    All weights and biases are treated as random variables sampled from\n    Gaussian distributions. Freezing the layer fixes parameters and\n    stops gradient computation.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_embeddings: int,\n        embeddings_dim: int,\n        hidden_size: int,\n        output_size: int,\n        padding_idx: Optional[int] = None,\n        max_norm: Optional[float] = None,\n        norm_type: float = 2.0,\n        scale_grad_by_freq: bool = False,\n        rngs: Rngs = nnx.Rngs(0),\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initialize a Bayesian LSTM layer with embedding and probabilistic\n        weights. Sets up all gate distributions and samples initial weights.\n\n        Args:\n            num_embeddings: Vocabulary size.\n            embeddings_dim: Dimension of token embeddings.\n            hidden_size: Number of units in LSTM hidden state.\n            output_size: Size of the output layer.\n            padding_idx: Index in embeddings to ignore (optional).\n            max_norm: Maximum norm for embeddings (optional).\n            norm_type: p-norm for max_norm computation.\n            scale_grad_by_freq: Scale gradients by token frequency.\n            rngs: Random number generators for reproducibility.\n            **kwargs: Extra arguments passed to the base class.\n\n        Returns:\n            None.\n\n        Notes:\n            Gaussian distributions are used by default if none are\n            provided.\n        \"\"\"\n\n        # Call super-class constructor\n        super().__init__(**kwargs)\n\n        # Set attributes\n        self.num_embeddings = num_embeddings\n        self.embeddings_dim = embeddings_dim\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.padding_idx = padding_idx\n        self.max_norm = max_norm\n        self.norm_type = norm_type\n        self.scale_grad_by_freq = scale_grad_by_freq\n        self.rngs = rngs\n\n        # Define the Embedding layer\n        self.embedding = Embedding(\n            num_embeddings=self.num_embeddings,\n            embeddings_dim=self.embeddings_dim,\n            padding_idx=self.padding_idx,\n            max_norm=self.max_norm,\n            norm_type=self.norm_type,\n            scale_grad_by_freq=self.scale_grad_by_freq,\n            rngs=self.rngs,\n        )\n\n        # Initialize weights\n        # Forget gate\n        self.wf_distribution = GaussianDistribution(\n            (self.hidden_size, self.embeddings_dim + self.hidden_size)\n        )\n        self.bf_distribution = GaussianDistribution((self.hidden_size,))\n\n        # Input gate\n        self.wi_distribution = GaussianDistribution(\n            (self.hidden_size, self.embeddings_dim + self.hidden_size)\n        )\n        self.bi_distribution = GaussianDistribution((self.hidden_size,))\n\n        # Candidate gate\n        self.wc_distribution = GaussianDistribution(\n            (self.hidden_size, self.embeddings_dim + self.hidden_size)\n        )\n        self.bc_distribution = GaussianDistribution((self.hidden_size,))\n\n        # Output gate\n        self.wo_distribution = GaussianDistribution(\n            (self.hidden_size, self.embeddings_dim + self.hidden_size)\n        )\n        self.bo_distribution = GaussianDistribution((self.hidden_size,))\n\n        # Final gate\n        self.wv_distribution = GaussianDistribution(\n            (self.output_size, self.hidden_size)\n        )\n        self.bv_distribution = GaussianDistribution((self.output_size,))\n\n        # Sample initial weights and register buffers\n        # Forget gate\n        self.wf = nnx.Param(self.wf_distribution.sample(self.rngs))\n        self.bf = nnx.Param(self.bf_distribution.sample(self.rngs))\n\n        # Input gate\n        self.wi = nnx.Param(self.wi_distribution.sample(self.rngs))\n        self.bi = nnx.Param(self.bi_distribution.sample(self.rngs))\n\n        # Candidate gate\n        self.wc = nnx.Param(self.wc_distribution.sample(self.rngs))\n        self.bc = nnx.Param(self.bc_distribution.sample(self.rngs))\n\n        # Output gate\n        self.wo = nnx.Param(self.wo_distribution.sample(self.rngs))\n        self.bo = nnx.Param(self.bo_distribution.sample(self.rngs))\n\n        # Final output layer\n        self.wv = nnx.Param(self.wv_distribution.sample(self.rngs))\n        self.bv = nnx.Param(self.bv_distribution.sample(self.rngs))\n\n    def freeze(self) -&gt; None:\n        \"\"\"\n        Freeze the module's parameters to stop gradient computation.\n        If weights or biases are not sampled yet, they are sampled first.\n        Once frozen, parameters are not resampled or updated.\n\n        Returns:\n            None.\n        \"\"\"\n\n        # Set indicator\n        self.frozen = True\n\n        # Freeze embedding layer\n        self.embedding.freeze()\n\n        # Forget gate\n        if self.wf is None:\n            self.wf = nnx.Param(self.wf_distribution.sample(self.rngs))\n        if self.bf is None:\n            self.bf = nnx.Param(self.bf_distribution.sample(self.rngs))\n        self.wf = jax.lax.stop_gradient(self.wf)\n        self.bf = jax.lax.stop_gradient(self.bf)\n\n        # Input gate\n        if self.wi is None:\n            self.wi = nnx.Param(self.wi_distribution.sample(self.rngs))\n        if self.bi is None:\n            self.bi = nnx.Param(self.bi_distribution.sample(self.rngs))\n        self.wi = jax.lax.stop_gradient(self.wi)\n        self.bi = jax.lax.stop_gradient(self.bi)\n\n        # Candidate gate\n        if self.wc is None:\n            self.wc = nnx.Param(self.wc_distribution.sample(self.rngs))\n        if self.bc is None:\n            self.bc = nnx.Param(self.bc_distribution.sample(self.rngs))\n        self.wc = jax.lax.stop_gradient(self.wc)\n        self.bc = jax.lax.stop_gradient(self.bc)\n\n        # Output gate\n        if self.wo is None:\n            self.wo = nnx.Param(self.wo_distribution.sample(self.rngs))\n        if self.bo is None:\n            self.bo = nnx.Param(self.bo_distribution.sample(self.rngs))\n        self.wo = jax.lax.stop_gradient(self.wo)\n        self.bo = jax.lax.stop_gradient(self.bo)\n\n        # Final output layer\n        if self.wv is None:\n            self.wv = nnx.Param(self.wv_distribution.sample(self.rngs))\n        if self.bv is None:\n            self.bv = nnx.Param(self.bv_distribution.sample(self.rngs))\n        self.wv = jax.lax.stop_gradient(self.wv)\n        self.bv = jax.lax.stop_gradient(self.bv)\n\n    def kl_cost(self) -&gt; tuple[jax.Array, int]:\n        \"\"\"\n        Compute the KL divergence cost for all Bayesian parameters.\n\n        Returns:\n            tuple[jax.Array, int]: A tuple containing the KL divergence\n                cost and the total number of parameters in the layer.\n        \"\"\"\n\n        # Compute log probs for each pair of weights and bias\n        log_probs_f = self.wf_distribution.log_prob(\n            jnp.asarray(self.wf)\n        ) + self.bf_distribution.log_prob(jnp.asarray(self.bf))\n        log_probs_i = self.wi_distribution.log_prob(\n            jnp.asarray(self.wi)\n        ) + self.bi_distribution.log_prob(jnp.asarray(self.bi))\n        log_probs_c = self.wc_distribution.log_prob(\n            jnp.asarray(self.wc)\n        ) + self.bc_distribution.log_prob(jnp.asarray(self.bc))\n        log_probs_o = self.wo_distribution.log_prob(\n            jnp.asarray(self.wo)\n        ) + self.bo_distribution.log_prob(jnp.asarray(self.bo))\n        log_probs_v = self.wv_distribution.log_prob(\n            jnp.asarray(self.wv)\n        ) + self.bv_distribution.log_prob(jnp.asarray(self.bv))\n\n        # Compute the total loss\n        log_probs = log_probs_f + log_probs_i + log_probs_c + log_probs_o + log_probs_v\n\n        # Compute number of parameters\n        num_params = (\n            self.wf_distribution.num_params\n            + self.bf_distribution.num_params\n            + self.wi_distribution.num_params\n            + self.bi_distribution.num_params\n            + self.wc_distribution.num_params\n            + self.bc_distribution.num_params\n            + self.wo_distribution.num_params\n            + self.bo_distribution.num_params\n            + self.wv_distribution.num_params\n            + self.bv_distribution.num_params\n        )\n\n        return log_probs, num_params\n\n    def __call__(\n        self,\n        inputs: jax.Array,\n        init_states: Optional[tuple[jax.Array, jax.Array]] = None,\n    ) -&gt; tuple[jax.Array, tuple[jax.Array, jax.Array]]:\n        \"\"\"\n        Perform a forward pass through the Bayesian LSTM layer.\n\n        Args:\n            inputs: Token indices with shape [batch, seq_len, 1].\n            init_states: Optional tuple of initial hidden and cell states.\n\n        Returns:\n            Tuple containing:\n            - Output tensor of shape [batch, output_size].\n            - Tuple of (hidden_state, cell_state) after sequence processing.\n\n        Raises:\n            ValueError: If the layer is frozen but weights are\n                undefined.\n        \"\"\"\n\n        # Sample weights if not frozen\n        if not self.frozen:\n            self.wf = nnx.Param(self.wf_distribution.sample(self.rngs))\n            self.bf = nnx.Param(self.bf_distribution.sample(self.rngs))\n            self.wi = nnx.Param(self.wi_distribution.sample(self.rngs))\n            self.bi = nnx.Param(self.bi_distribution.sample(self.rngs))\n            self.wc = nnx.Param(self.wc_distribution.sample(self.rngs))\n            self.bc = nnx.Param(self.bc_distribution.sample(self.rngs))\n            self.wo = nnx.Param(self.wo_distribution.sample(self.rngs))\n            self.bo = nnx.Param(self.bo_distribution.sample(self.rngs))\n            self.wv = nnx.Param(self.wv_distribution.sample(self.rngs))\n            self.bv = nnx.Param(self.bv_distribution.sample(self.rngs))\n        elif any(\n            p is None\n            for p in [\n                self.wf,\n                self.bf,\n                self.wi,\n                self.bi,\n                self.wc,\n                self.bc,\n                self.wo,\n                self.bo,\n                self.wv,\n                self.bv,\n            ]\n        ):\n            raise ValueError(\n                \"Module has been frozen with undefined weights and/or bias.\"\n            )\n\n        # Apply embedding layer to input indices\n        inputs = jnp.squeeze(inputs, axis=-1)\n        inputs = self.embedding(inputs)\n        batch_size = jnp.shape(inputs)[0]\n        seq_len = jnp.shape(inputs)[1]\n\n        # Initialize h_t and c_t if init_states is None\n        if init_states is None:\n            h_t = jnp.zeros([batch_size, self.hidden_size])\n            c_t = jnp.zeros([batch_size, self.hidden_size])\n        else:\n            h_t, c_t = init_states[0], init_states[1]\n\n        # Process sequence\n        for t in range(seq_len):\n            # Shape: (batch_size, embedding_dim)\n            x_t = inputs[:, t, :]\n\n            # Concatenate input and hidden state\n            # Shape: (batch_size, embedding_dim + hidden_size)\n            z_t = jnp.concat([x_t, h_t], axis=1)\n\n            # Forget gate\n            ft = nnx.sigmoid(z_t @ self.wf.T + self.bf)\n\n            # Input gate\n            it = nnx.sigmoid(z_t @ self.wi.T + self.bi)\n\n            # Candidate cell state\n            can = nnx.tanh(z_t @ self.wc.T + self.bc)\n\n            # Output gate\n            ot = nnx.sigmoid(z_t @ self.wo.T + self.bo)\n\n            # Update cell state\n            c_t = c_t * ft + can * it\n\n            # Update hidden state\n            h_t = ot * nnx.tanh(c_t)\n\n        # Compute final output\n        y_t = h_t @ self.wv.T + self.bv\n\n        return y_t, (h_t, c_t)\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/nn.html#illia.nn.jax.lstm.LSTM.__call__","title":"<code>__call__(inputs, init_states=None)</code>","text":"<p>Perform a forward pass through the Bayesian LSTM layer.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Array</code> <p>Token indices with shape [batch, seq_len, 1].</p> required <code>init_states</code> <code>Optional[tuple[Array, Array]]</code> <p>Optional tuple of initial hidden and cell states.</p> <code>None</code> <p>Returns:</p> Type Description <code>Array</code> <p>Tuple containing:</p> <code>tuple[Array, Array]</code> <ul> <li>Output tensor of shape [batch, output_size].</li> </ul> <code>tuple[Array, tuple[Array, Array]]</code> <ul> <li>Tuple of (hidden_state, cell_state) after sequence processing.</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the layer is frozen but weights are undefined.</p> Source code in <code>illia/nn/jax/lstm.py</code> <pre><code>def __call__(\n    self,\n    inputs: jax.Array,\n    init_states: Optional[tuple[jax.Array, jax.Array]] = None,\n) -&gt; tuple[jax.Array, tuple[jax.Array, jax.Array]]:\n    \"\"\"\n    Perform a forward pass through the Bayesian LSTM layer.\n\n    Args:\n        inputs: Token indices with shape [batch, seq_len, 1].\n        init_states: Optional tuple of initial hidden and cell states.\n\n    Returns:\n        Tuple containing:\n        - Output tensor of shape [batch, output_size].\n        - Tuple of (hidden_state, cell_state) after sequence processing.\n\n    Raises:\n        ValueError: If the layer is frozen but weights are\n            undefined.\n    \"\"\"\n\n    # Sample weights if not frozen\n    if not self.frozen:\n        self.wf = nnx.Param(self.wf_distribution.sample(self.rngs))\n        self.bf = nnx.Param(self.bf_distribution.sample(self.rngs))\n        self.wi = nnx.Param(self.wi_distribution.sample(self.rngs))\n        self.bi = nnx.Param(self.bi_distribution.sample(self.rngs))\n        self.wc = nnx.Param(self.wc_distribution.sample(self.rngs))\n        self.bc = nnx.Param(self.bc_distribution.sample(self.rngs))\n        self.wo = nnx.Param(self.wo_distribution.sample(self.rngs))\n        self.bo = nnx.Param(self.bo_distribution.sample(self.rngs))\n        self.wv = nnx.Param(self.wv_distribution.sample(self.rngs))\n        self.bv = nnx.Param(self.bv_distribution.sample(self.rngs))\n    elif any(\n        p is None\n        for p in [\n            self.wf,\n            self.bf,\n            self.wi,\n            self.bi,\n            self.wc,\n            self.bc,\n            self.wo,\n            self.bo,\n            self.wv,\n            self.bv,\n        ]\n    ):\n        raise ValueError(\n            \"Module has been frozen with undefined weights and/or bias.\"\n        )\n\n    # Apply embedding layer to input indices\n    inputs = jnp.squeeze(inputs, axis=-1)\n    inputs = self.embedding(inputs)\n    batch_size = jnp.shape(inputs)[0]\n    seq_len = jnp.shape(inputs)[1]\n\n    # Initialize h_t and c_t if init_states is None\n    if init_states is None:\n        h_t = jnp.zeros([batch_size, self.hidden_size])\n        c_t = jnp.zeros([batch_size, self.hidden_size])\n    else:\n        h_t, c_t = init_states[0], init_states[1]\n\n    # Process sequence\n    for t in range(seq_len):\n        # Shape: (batch_size, embedding_dim)\n        x_t = inputs[:, t, :]\n\n        # Concatenate input and hidden state\n        # Shape: (batch_size, embedding_dim + hidden_size)\n        z_t = jnp.concat([x_t, h_t], axis=1)\n\n        # Forget gate\n        ft = nnx.sigmoid(z_t @ self.wf.T + self.bf)\n\n        # Input gate\n        it = nnx.sigmoid(z_t @ self.wi.T + self.bi)\n\n        # Candidate cell state\n        can = nnx.tanh(z_t @ self.wc.T + self.bc)\n\n        # Output gate\n        ot = nnx.sigmoid(z_t @ self.wo.T + self.bo)\n\n        # Update cell state\n        c_t = c_t * ft + can * it\n\n        # Update hidden state\n        h_t = ot * nnx.tanh(c_t)\n\n    # Compute final output\n    y_t = h_t @ self.wv.T + self.bv\n\n    return y_t, (h_t, c_t)\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/nn.html#illia.nn.jax.lstm.LSTM.__init__","title":"<code>__init__(num_embeddings, embeddings_dim, hidden_size, output_size, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, rngs=nnx.Rngs(0), **kwargs)</code>","text":"<p>Initialize a Bayesian LSTM layer with embedding and probabilistic weights. Sets up all gate distributions and samples initial weights.</p> <p>Parameters:</p> Name Type Description Default <code>num_embeddings</code> <code>int</code> <p>Vocabulary size.</p> required <code>embeddings_dim</code> <code>int</code> <p>Dimension of token embeddings.</p> required <code>hidden_size</code> <code>int</code> <p>Number of units in LSTM hidden state.</p> required <code>output_size</code> <code>int</code> <p>Size of the output layer.</p> required <code>padding_idx</code> <code>Optional[int]</code> <p>Index in embeddings to ignore (optional).</p> <code>None</code> <code>max_norm</code> <code>Optional[float]</code> <p>Maximum norm for embeddings (optional).</p> <code>None</code> <code>norm_type</code> <code>float</code> <p>p-norm for max_norm computation.</p> <code>2.0</code> <code>scale_grad_by_freq</code> <code>bool</code> <p>Scale gradients by token frequency.</p> <code>False</code> <code>rngs</code> <code>Rngs</code> <p>Random number generators for reproducibility.</p> <code>Rngs(0)</code> <code>**kwargs</code> <code>Any</code> <p>Extra arguments passed to the base class.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Notes <p>Gaussian distributions are used by default if none are provided.</p> Source code in <code>illia/nn/jax/lstm.py</code> <pre><code>def __init__(\n    self,\n    num_embeddings: int,\n    embeddings_dim: int,\n    hidden_size: int,\n    output_size: int,\n    padding_idx: Optional[int] = None,\n    max_norm: Optional[float] = None,\n    norm_type: float = 2.0,\n    scale_grad_by_freq: bool = False,\n    rngs: Rngs = nnx.Rngs(0),\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialize a Bayesian LSTM layer with embedding and probabilistic\n    weights. Sets up all gate distributions and samples initial weights.\n\n    Args:\n        num_embeddings: Vocabulary size.\n        embeddings_dim: Dimension of token embeddings.\n        hidden_size: Number of units in LSTM hidden state.\n        output_size: Size of the output layer.\n        padding_idx: Index in embeddings to ignore (optional).\n        max_norm: Maximum norm for embeddings (optional).\n        norm_type: p-norm for max_norm computation.\n        scale_grad_by_freq: Scale gradients by token frequency.\n        rngs: Random number generators for reproducibility.\n        **kwargs: Extra arguments passed to the base class.\n\n    Returns:\n        None.\n\n    Notes:\n        Gaussian distributions are used by default if none are\n        provided.\n    \"\"\"\n\n    # Call super-class constructor\n    super().__init__(**kwargs)\n\n    # Set attributes\n    self.num_embeddings = num_embeddings\n    self.embeddings_dim = embeddings_dim\n    self.hidden_size = hidden_size\n    self.output_size = output_size\n    self.padding_idx = padding_idx\n    self.max_norm = max_norm\n    self.norm_type = norm_type\n    self.scale_grad_by_freq = scale_grad_by_freq\n    self.rngs = rngs\n\n    # Define the Embedding layer\n    self.embedding = Embedding(\n        num_embeddings=self.num_embeddings,\n        embeddings_dim=self.embeddings_dim,\n        padding_idx=self.padding_idx,\n        max_norm=self.max_norm,\n        norm_type=self.norm_type,\n        scale_grad_by_freq=self.scale_grad_by_freq,\n        rngs=self.rngs,\n    )\n\n    # Initialize weights\n    # Forget gate\n    self.wf_distribution = GaussianDistribution(\n        (self.hidden_size, self.embeddings_dim + self.hidden_size)\n    )\n    self.bf_distribution = GaussianDistribution((self.hidden_size,))\n\n    # Input gate\n    self.wi_distribution = GaussianDistribution(\n        (self.hidden_size, self.embeddings_dim + self.hidden_size)\n    )\n    self.bi_distribution = GaussianDistribution((self.hidden_size,))\n\n    # Candidate gate\n    self.wc_distribution = GaussianDistribution(\n        (self.hidden_size, self.embeddings_dim + self.hidden_size)\n    )\n    self.bc_distribution = GaussianDistribution((self.hidden_size,))\n\n    # Output gate\n    self.wo_distribution = GaussianDistribution(\n        (self.hidden_size, self.embeddings_dim + self.hidden_size)\n    )\n    self.bo_distribution = GaussianDistribution((self.hidden_size,))\n\n    # Final gate\n    self.wv_distribution = GaussianDistribution(\n        (self.output_size, self.hidden_size)\n    )\n    self.bv_distribution = GaussianDistribution((self.output_size,))\n\n    # Sample initial weights and register buffers\n    # Forget gate\n    self.wf = nnx.Param(self.wf_distribution.sample(self.rngs))\n    self.bf = nnx.Param(self.bf_distribution.sample(self.rngs))\n\n    # Input gate\n    self.wi = nnx.Param(self.wi_distribution.sample(self.rngs))\n    self.bi = nnx.Param(self.bi_distribution.sample(self.rngs))\n\n    # Candidate gate\n    self.wc = nnx.Param(self.wc_distribution.sample(self.rngs))\n    self.bc = nnx.Param(self.bc_distribution.sample(self.rngs))\n\n    # Output gate\n    self.wo = nnx.Param(self.wo_distribution.sample(self.rngs))\n    self.bo = nnx.Param(self.bo_distribution.sample(self.rngs))\n\n    # Final output layer\n    self.wv = nnx.Param(self.wv_distribution.sample(self.rngs))\n    self.bv = nnx.Param(self.bv_distribution.sample(self.rngs))\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/nn.html#illia.nn.jax.lstm.LSTM.freeze","title":"<code>freeze()</code>","text":"<p>Freeze the module's parameters to stop gradient computation. If weights or biases are not sampled yet, they are sampled first. Once frozen, parameters are not resampled or updated.</p> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>illia/nn/jax/lstm.py</code> <pre><code>def freeze(self) -&gt; None:\n    \"\"\"\n    Freeze the module's parameters to stop gradient computation.\n    If weights or biases are not sampled yet, they are sampled first.\n    Once frozen, parameters are not resampled or updated.\n\n    Returns:\n        None.\n    \"\"\"\n\n    # Set indicator\n    self.frozen = True\n\n    # Freeze embedding layer\n    self.embedding.freeze()\n\n    # Forget gate\n    if self.wf is None:\n        self.wf = nnx.Param(self.wf_distribution.sample(self.rngs))\n    if self.bf is None:\n        self.bf = nnx.Param(self.bf_distribution.sample(self.rngs))\n    self.wf = jax.lax.stop_gradient(self.wf)\n    self.bf = jax.lax.stop_gradient(self.bf)\n\n    # Input gate\n    if self.wi is None:\n        self.wi = nnx.Param(self.wi_distribution.sample(self.rngs))\n    if self.bi is None:\n        self.bi = nnx.Param(self.bi_distribution.sample(self.rngs))\n    self.wi = jax.lax.stop_gradient(self.wi)\n    self.bi = jax.lax.stop_gradient(self.bi)\n\n    # Candidate gate\n    if self.wc is None:\n        self.wc = nnx.Param(self.wc_distribution.sample(self.rngs))\n    if self.bc is None:\n        self.bc = nnx.Param(self.bc_distribution.sample(self.rngs))\n    self.wc = jax.lax.stop_gradient(self.wc)\n    self.bc = jax.lax.stop_gradient(self.bc)\n\n    # Output gate\n    if self.wo is None:\n        self.wo = nnx.Param(self.wo_distribution.sample(self.rngs))\n    if self.bo is None:\n        self.bo = nnx.Param(self.bo_distribution.sample(self.rngs))\n    self.wo = jax.lax.stop_gradient(self.wo)\n    self.bo = jax.lax.stop_gradient(self.bo)\n\n    # Final output layer\n    if self.wv is None:\n        self.wv = nnx.Param(self.wv_distribution.sample(self.rngs))\n    if self.bv is None:\n        self.bv = nnx.Param(self.bv_distribution.sample(self.rngs))\n    self.wv = jax.lax.stop_gradient(self.wv)\n    self.bv = jax.lax.stop_gradient(self.bv)\n</code></pre>"},{"location":"api/Deep%20Learning/Jax/nn.html#illia.nn.jax.lstm.LSTM.kl_cost","title":"<code>kl_cost()</code>","text":"<p>Compute the KL divergence cost for all Bayesian parameters.</p> <p>Returns:</p> Type Description <code>tuple[Array, int]</code> <p>tuple[jax.Array, int]: A tuple containing the KL divergence cost and the total number of parameters in the layer.</p> Source code in <code>illia/nn/jax/lstm.py</code> <pre><code>def kl_cost(self) -&gt; tuple[jax.Array, int]:\n    \"\"\"\n    Compute the KL divergence cost for all Bayesian parameters.\n\n    Returns:\n        tuple[jax.Array, int]: A tuple containing the KL divergence\n            cost and the total number of parameters in the layer.\n    \"\"\"\n\n    # Compute log probs for each pair of weights and bias\n    log_probs_f = self.wf_distribution.log_prob(\n        jnp.asarray(self.wf)\n    ) + self.bf_distribution.log_prob(jnp.asarray(self.bf))\n    log_probs_i = self.wi_distribution.log_prob(\n        jnp.asarray(self.wi)\n    ) + self.bi_distribution.log_prob(jnp.asarray(self.bi))\n    log_probs_c = self.wc_distribution.log_prob(\n        jnp.asarray(self.wc)\n    ) + self.bc_distribution.log_prob(jnp.asarray(self.bc))\n    log_probs_o = self.wo_distribution.log_prob(\n        jnp.asarray(self.wo)\n    ) + self.bo_distribution.log_prob(jnp.asarray(self.bo))\n    log_probs_v = self.wv_distribution.log_prob(\n        jnp.asarray(self.wv)\n    ) + self.bv_distribution.log_prob(jnp.asarray(self.bv))\n\n    # Compute the total loss\n    log_probs = log_probs_f + log_probs_i + log_probs_c + log_probs_o + log_probs_v\n\n    # Compute number of parameters\n    num_params = (\n        self.wf_distribution.num_params\n        + self.bf_distribution.num_params\n        + self.wi_distribution.num_params\n        + self.bi_distribution.num_params\n        + self.wc_distribution.num_params\n        + self.bc_distribution.num_params\n        + self.wo_distribution.num_params\n        + self.bo_distribution.num_params\n        + self.wv_distribution.num_params\n        + self.bv_distribution.num_params\n    )\n\n    return log_probs, num_params\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/distributions.html","title":"Distributions","text":""},{"location":"api/Deep%20Learning/PyTorch/distributions.html#illia.distributions.torch.base.DistributionModule","title":"<code>DistributionModule</code>","text":"<p>Abstract base for probabilistic distribution modules in PyTorch. Defines the required interface for sampling, computing log-probabilities, and counting learnable parameters.</p> Notes <p>This class is abstract and cannot be instantiated directly. All abstract methods must be implemented by subclasses.</p> Source code in <code>illia/distributions/torch/base.py</code> <pre><code>class DistributionModule(torch.nn.Module, ABC):\n    \"\"\"\n    Abstract base for probabilistic distribution modules in PyTorch.\n    Defines the required interface for sampling, computing\n    log-probabilities, and counting learnable parameters.\n\n    Notes:\n        This class is abstract and cannot be instantiated directly.\n        All abstract methods must be implemented by subclasses.\n    \"\"\"\n\n    @abstractmethod\n    def sample(self) -&gt; torch.Tensor:\n        \"\"\"\n        Draw a sample from the distribution.\n\n        Returns:\n            torch.Tensor: A sample drawn from the distribution.\n        \"\"\"\n\n    @abstractmethod\n    def log_prob(self, x: Optional[torch.Tensor] = None) -&gt; torch.Tensor:\n        \"\"\"\n        Compute the log-probability of a provided sample. If no\n        sample is passed, one is drawn internally.\n\n        Args:\n            x: Optional sample to evaluate. If None, a new sample is\n                drawn from the distribution.\n\n        Returns:\n            torch.Tensor: Scalar log-probability value.\n\n        Notes:\n            Works with both user-supplied and internally drawn\n            samples.\n        \"\"\"\n\n    @abstractmethod\n    def num_params(self) -&gt; int:\n        \"\"\"\n        Return the number of learnable parameters in the\n        distribution.\n\n        Returns:\n            int: Total count of learnable parameters.\n        \"\"\"\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/distributions.html#illia.distributions.torch.base.DistributionModule.log_prob","title":"<code>log_prob(x=None)</code>  <code>abstractmethod</code>","text":"<p>Compute the log-probability of a provided sample. If no sample is passed, one is drawn internally.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Optional[Tensor]</code> <p>Optional sample to evaluate. If None, a new sample is drawn from the distribution.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Scalar log-probability value.</p> Notes <p>Works with both user-supplied and internally drawn samples.</p> Source code in <code>illia/distributions/torch/base.py</code> <pre><code>@abstractmethod\ndef log_prob(self, x: Optional[torch.Tensor] = None) -&gt; torch.Tensor:\n    \"\"\"\n    Compute the log-probability of a provided sample. If no\n    sample is passed, one is drawn internally.\n\n    Args:\n        x: Optional sample to evaluate. If None, a new sample is\n            drawn from the distribution.\n\n    Returns:\n        torch.Tensor: Scalar log-probability value.\n\n    Notes:\n        Works with both user-supplied and internally drawn\n        samples.\n    \"\"\"\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/distributions.html#illia.distributions.torch.base.DistributionModule.num_params","title":"<code>num_params()</code>  <code>abstractmethod</code>","text":"<p>Return the number of learnable parameters in the distribution.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Total count of learnable parameters.</p> Source code in <code>illia/distributions/torch/base.py</code> <pre><code>@abstractmethod\ndef num_params(self) -&gt; int:\n    \"\"\"\n    Return the number of learnable parameters in the\n    distribution.\n\n    Returns:\n        int: Total count of learnable parameters.\n    \"\"\"\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/distributions.html#illia.distributions.torch.base.DistributionModule.sample","title":"<code>sample()</code>  <code>abstractmethod</code>","text":"<p>Draw a sample from the distribution.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A sample drawn from the distribution.</p> Source code in <code>illia/distributions/torch/base.py</code> <pre><code>@abstractmethod\ndef sample(self) -&gt; torch.Tensor:\n    \"\"\"\n    Draw a sample from the distribution.\n\n    Returns:\n        torch.Tensor: A sample drawn from the distribution.\n    \"\"\"\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/distributions.html#illia.distributions.torch.gaussian.GaussianDistribution","title":"<code>GaussianDistribution</code>","text":"<p>Learnable Gaussian distribution with diagonal covariance. Represents a Gaussian with trainable mean and standard deviation. The standard deviation is derived from <code>rho</code> using a softplus transformation to ensure positivity.</p> Notes <p>Assumes diagonal covariance. KL divergence can be estimated via log-probability differences from <code>log_prob</code>.</p> Source code in <code>illia/distributions/torch/gaussian.py</code> <pre><code>class GaussianDistribution(DistributionModule):\n    \"\"\"\n    Learnable Gaussian distribution with diagonal covariance.\n    Represents a Gaussian with trainable mean and standard\n    deviation. The standard deviation is derived from `rho`\n    using a softplus transformation to ensure positivity.\n\n    Notes:\n        Assumes diagonal covariance. KL divergence can be\n        estimated via log-probability differences from\n        `log_prob`.\n    \"\"\"\n\n    def __init__(\n        self,\n        shape: tuple[int, ...],\n        mu_prior: float = 0.0,\n        std_prior: float = 0.1,\n        mu_init: float = 0.0,\n        rho_init: float = -7.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initialize a learnable Gaussian distribution module.\n\n        Args:\n            shape: Shape of the learnable parameters.\n            mu_prior: Mean of the Gaussian prior.\n            std_prior: Standard deviation of the prior.\n            mu_init: Initial value for the learnable mean.\n            rho_init: Initial value for the learnable rho.\n            **kwargs: Extra arguments passed to the base class.\n\n        Returns:\n            None\n        \"\"\"\n\n        # Call super-class constructor\n        super().__init__(**kwargs)\n\n        # Set attributes\n        self.shape = shape\n        self.mu_init = mu_init\n        self.rho_init = rho_init\n\n        # Define priors\n        self.register_buffer(\"mu_prior\", torch.tensor([mu_prior]))\n        self.register_buffer(\"std_prior\", torch.tensor([std_prior]))\n\n        # Define initial mu and rho\n        self.mu: torch.Tensor = torch.nn.Parameter(\n            torch.randn(self.shape).normal_(self.mu_init, 0.1)\n        )\n        self.rho: torch.Tensor = torch.nn.Parameter(\n            torch.randn(self.shape).normal_(self.rho_init, 0.1)\n        )\n\n    @torch.jit.export\n    def sample(self) -&gt; torch.Tensor:\n        \"\"\"\n        Draw a sample from the Gaussian distribution.\n\n        Returns:\n            torch.Tensor: A sample drawn from the distribution.\n        \"\"\"\n\n        # Sampling with reparametrization trick\n        eps: torch.Tensor = torch.randn_like(self.rho)\n        sigma: torch.Tensor = torch.log1p(torch.exp(self.rho))\n\n        return self.mu + sigma * eps\n\n    @torch.jit.export\n    def log_prob(self, x: Optional[torch.Tensor] = None) -&gt; torch.Tensor:\n        \"\"\"\n        Compute the log-probability of a given sample. If no\n        sample is provided, one is drawn internally.\n\n        Args:\n            x: Optional input sample to evaluate. If None,\n                a new sample is drawn from the distribution.\n\n        Returns:\n            torch.Tensor: Scalar log-probability value.\n\n        Notes:\n            Supports both user-supplied and internally drawn\n            samples.\n        \"\"\"\n\n        # Sample if x is None\n        if x is None:\n            x = self.sample()\n\n        # Define pi variable\n        pi: torch.Tensor = torch.acos(torch.zeros(1)) * 2\n\n        # Compute log priors\n        log_prior = (\n            -torch.log(torch.sqrt(2 * pi)).to(x.device)\n            - torch.log(self.std_prior)\n            - (((x - self.mu_prior) ** 2) / (2 * self.std_prior**2))\n            - 0.5\n        )\n\n        # Compute sigma\n        sigma: torch.Tensor = torch.log1p(torch.exp(self.rho)).to(x.device)\n\n        # Compute log posteriors\n        log_posteriors = (\n            -torch.log(torch.sqrt(2 * pi)).to(x.device)\n            - torch.log(sigma)\n            - (((x - self.mu) ** 2) / (2 * sigma**2))\n            - 0.5\n        )\n\n        # Compute final log probs\n        log_probs = log_posteriors.sum() - log_prior.sum()\n\n        return log_probs\n\n    @torch.jit.export\n    @torch.no_grad()\n    def num_params(self) -&gt; int:\n        \"\"\"\n        Return the number of learnable parameters in the\n        distribution.\n\n        Returns:\n            int: Total count of learnable parameters.\n        \"\"\"\n\n        return len(self.mu.view(-1))\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/distributions.html#illia.distributions.torch.gaussian.GaussianDistribution.__init__","title":"<code>__init__(shape, mu_prior=0.0, std_prior=0.1, mu_init=0.0, rho_init=-7.0, **kwargs)</code>","text":"<p>Initialize a learnable Gaussian distribution module.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>tuple[int, ...]</code> <p>Shape of the learnable parameters.</p> required <code>mu_prior</code> <code>float</code> <p>Mean of the Gaussian prior.</p> <code>0.0</code> <code>std_prior</code> <code>float</code> <p>Standard deviation of the prior.</p> <code>0.1</code> <code>mu_init</code> <code>float</code> <p>Initial value for the learnable mean.</p> <code>0.0</code> <code>rho_init</code> <code>float</code> <p>Initial value for the learnable rho.</p> <code>-7.0</code> <code>**kwargs</code> <code>Any</code> <p>Extra arguments passed to the base class.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>illia/distributions/torch/gaussian.py</code> <pre><code>def __init__(\n    self,\n    shape: tuple[int, ...],\n    mu_prior: float = 0.0,\n    std_prior: float = 0.1,\n    mu_init: float = 0.0,\n    rho_init: float = -7.0,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialize a learnable Gaussian distribution module.\n\n    Args:\n        shape: Shape of the learnable parameters.\n        mu_prior: Mean of the Gaussian prior.\n        std_prior: Standard deviation of the prior.\n        mu_init: Initial value for the learnable mean.\n        rho_init: Initial value for the learnable rho.\n        **kwargs: Extra arguments passed to the base class.\n\n    Returns:\n        None\n    \"\"\"\n\n    # Call super-class constructor\n    super().__init__(**kwargs)\n\n    # Set attributes\n    self.shape = shape\n    self.mu_init = mu_init\n    self.rho_init = rho_init\n\n    # Define priors\n    self.register_buffer(\"mu_prior\", torch.tensor([mu_prior]))\n    self.register_buffer(\"std_prior\", torch.tensor([std_prior]))\n\n    # Define initial mu and rho\n    self.mu: torch.Tensor = torch.nn.Parameter(\n        torch.randn(self.shape).normal_(self.mu_init, 0.1)\n    )\n    self.rho: torch.Tensor = torch.nn.Parameter(\n        torch.randn(self.shape).normal_(self.rho_init, 0.1)\n    )\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/distributions.html#illia.distributions.torch.gaussian.GaussianDistribution.log_prob","title":"<code>log_prob(x=None)</code>","text":"<p>Compute the log-probability of a given sample. If no sample is provided, one is drawn internally.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Optional[Tensor]</code> <p>Optional input sample to evaluate. If None, a new sample is drawn from the distribution.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Scalar log-probability value.</p> Notes <p>Supports both user-supplied and internally drawn samples.</p> Source code in <code>illia/distributions/torch/gaussian.py</code> <pre><code>@torch.jit.export\ndef log_prob(self, x: Optional[torch.Tensor] = None) -&gt; torch.Tensor:\n    \"\"\"\n    Compute the log-probability of a given sample. If no\n    sample is provided, one is drawn internally.\n\n    Args:\n        x: Optional input sample to evaluate. If None,\n            a new sample is drawn from the distribution.\n\n    Returns:\n        torch.Tensor: Scalar log-probability value.\n\n    Notes:\n        Supports both user-supplied and internally drawn\n        samples.\n    \"\"\"\n\n    # Sample if x is None\n    if x is None:\n        x = self.sample()\n\n    # Define pi variable\n    pi: torch.Tensor = torch.acos(torch.zeros(1)) * 2\n\n    # Compute log priors\n    log_prior = (\n        -torch.log(torch.sqrt(2 * pi)).to(x.device)\n        - torch.log(self.std_prior)\n        - (((x - self.mu_prior) ** 2) / (2 * self.std_prior**2))\n        - 0.5\n    )\n\n    # Compute sigma\n    sigma: torch.Tensor = torch.log1p(torch.exp(self.rho)).to(x.device)\n\n    # Compute log posteriors\n    log_posteriors = (\n        -torch.log(torch.sqrt(2 * pi)).to(x.device)\n        - torch.log(sigma)\n        - (((x - self.mu) ** 2) / (2 * sigma**2))\n        - 0.5\n    )\n\n    # Compute final log probs\n    log_probs = log_posteriors.sum() - log_prior.sum()\n\n    return log_probs\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/distributions.html#illia.distributions.torch.gaussian.GaussianDistribution.num_params","title":"<code>num_params()</code>","text":"<p>Return the number of learnable parameters in the distribution.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Total count of learnable parameters.</p> Source code in <code>illia/distributions/torch/gaussian.py</code> <pre><code>@torch.jit.export\n@torch.no_grad()\ndef num_params(self) -&gt; int:\n    \"\"\"\n    Return the number of learnable parameters in the\n    distribution.\n\n    Returns:\n        int: Total count of learnable parameters.\n    \"\"\"\n\n    return len(self.mu.view(-1))\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/distributions.html#illia.distributions.torch.gaussian.GaussianDistribution.sample","title":"<code>sample()</code>","text":"<p>Draw a sample from the Gaussian distribution.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A sample drawn from the distribution.</p> Source code in <code>illia/distributions/torch/gaussian.py</code> <pre><code>@torch.jit.export\ndef sample(self) -&gt; torch.Tensor:\n    \"\"\"\n    Draw a sample from the Gaussian distribution.\n\n    Returns:\n        torch.Tensor: A sample drawn from the distribution.\n    \"\"\"\n\n    # Sampling with reparametrization trick\n    eps: torch.Tensor = torch.randn_like(self.rho)\n    sigma: torch.Tensor = torch.log1p(torch.exp(self.rho))\n\n    return self.mu + sigma * eps\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/losses.html","title":"Losses","text":""},{"location":"api/Deep%20Learning/PyTorch/losses.html#illia.losses.torch.kl.KLDivergenceLoss","title":"<code>KLDivergenceLoss</code>","text":"<p>Compute Kullback-Leibler divergence across Bayesian modules. This loss sums the KL divergence from all Bayesian layers in the model. It can be reduced by averaging and scaled by a weight factor.</p> Notes <p>Assumes the model contains submodules derived from <code>BayesianModule</code>.</p> Source code in <code>illia/losses/torch/kl.py</code> <pre><code>class KLDivergenceLoss(torch.nn.Module):\n    \"\"\"\n    Compute Kullback-Leibler divergence across Bayesian modules.\n    This loss sums the KL divergence from all Bayesian layers in\n    the model. It can be reduced by averaging and scaled by a\n    weight factor.\n\n    Notes:\n        Assumes the model contains submodules derived from\n        `BayesianModule`.\n    \"\"\"\n\n    def __init__(\n        self,\n        reduction: Literal[\"mean\"] = \"mean\",\n        weight: float = 1.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the KL divergence loss.\n\n        Args:\n            reduction: Method used to reduce the KL loss.\n            weight: Scaling factor for the KL divergence.\n            **kwargs: Extra arguments passed to the base class.\n\n        Returns:\n            None\n        \"\"\"\n\n        # call super class constructor\n        super().__init__(**kwargs)\n\n        # Set attributes\n        self.reduction = reduction\n        self.weight = weight\n\n    def forward(self, model: torch.nn.Module) -&gt; torch.Tensor:\n        \"\"\"\n        Compute KL divergence for all Bayesian modules in a model.\n\n        Args:\n            model: Model containing Bayesian submodules.\n\n        Returns:\n            torch.Tensor: Weighted KL divergence loss.\n\n        Notes:\n            The KL loss is averaged over the number of parameters\n            and scaled by the `weight` attribute.\n        \"\"\"\n\n        # Get device and dtype\n        parameter: torch.nn.Parameter = next(model.parameters())\n        device: torch.device = parameter.device\n        dtype = parameter.dtype\n\n        # Init kl cost and params\n        kl_global_cost: torch.Tensor = torch.tensor(0, device=device, dtype=dtype)\n        num_params_global: int = 0\n\n        # Iter over modules\n        for module in model.modules():\n            if isinstance(module, BayesianModule):\n                kl_cost, num_params = module.kl_cost()\n                kl_global_cost += kl_cost\n                num_params_global += num_params\n\n        # Average by the number of parameters\n        kl_global_cost /= num_params\n        kl_global_cost *= self.weight\n\n        return kl_global_cost\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/losses.html#illia.losses.torch.kl.KLDivergenceLoss.__init__","title":"<code>__init__(reduction='mean', weight=1.0, **kwargs)</code>","text":"<p>Initialize the KL divergence loss.</p> <p>Parameters:</p> Name Type Description Default <code>reduction</code> <code>Literal['mean']</code> <p>Method used to reduce the KL loss.</p> <code>'mean'</code> <code>weight</code> <code>float</code> <p>Scaling factor for the KL divergence.</p> <code>1.0</code> <code>**kwargs</code> <code>Any</code> <p>Extra arguments passed to the base class.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>illia/losses/torch/kl.py</code> <pre><code>def __init__(\n    self,\n    reduction: Literal[\"mean\"] = \"mean\",\n    weight: float = 1.0,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialize the KL divergence loss.\n\n    Args:\n        reduction: Method used to reduce the KL loss.\n        weight: Scaling factor for the KL divergence.\n        **kwargs: Extra arguments passed to the base class.\n\n    Returns:\n        None\n    \"\"\"\n\n    # call super class constructor\n    super().__init__(**kwargs)\n\n    # Set attributes\n    self.reduction = reduction\n    self.weight = weight\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/losses.html#illia.losses.torch.kl.KLDivergenceLoss.forward","title":"<code>forward(model)</code>","text":"<p>Compute KL divergence for all Bayesian modules in a model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Model containing Bayesian submodules.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Weighted KL divergence loss.</p> Notes <p>The KL loss is averaged over the number of parameters and scaled by the <code>weight</code> attribute.</p> Source code in <code>illia/losses/torch/kl.py</code> <pre><code>def forward(self, model: torch.nn.Module) -&gt; torch.Tensor:\n    \"\"\"\n    Compute KL divergence for all Bayesian modules in a model.\n\n    Args:\n        model: Model containing Bayesian submodules.\n\n    Returns:\n        torch.Tensor: Weighted KL divergence loss.\n\n    Notes:\n        The KL loss is averaged over the number of parameters\n        and scaled by the `weight` attribute.\n    \"\"\"\n\n    # Get device and dtype\n    parameter: torch.nn.Parameter = next(model.parameters())\n    device: torch.device = parameter.device\n    dtype = parameter.dtype\n\n    # Init kl cost and params\n    kl_global_cost: torch.Tensor = torch.tensor(0, device=device, dtype=dtype)\n    num_params_global: int = 0\n\n    # Iter over modules\n    for module in model.modules():\n        if isinstance(module, BayesianModule):\n            kl_cost, num_params = module.kl_cost()\n            kl_global_cost += kl_cost\n            num_params_global += num_params\n\n    # Average by the number of parameters\n    kl_global_cost /= num_params\n    kl_global_cost *= self.weight\n\n    return kl_global_cost\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/losses.html#illia.losses.torch.elbo.ELBOLoss","title":"<code>ELBOLoss</code>","text":"<p>Compute the Evidence Lower Bound (ELBO) loss for Bayesian networks. Combines a reconstruction loss with a KL divergence term. Monte Carlo sampling can estimate the expected reconstruction loss over stochastic layers.</p> Notes <p>The KL term is weighted by <code>kl_weight</code>. The model is assumed to contain Bayesian layers compatible with <code>KLDivergenceLoss</code>.</p> Source code in <code>illia/losses/torch/elbo.py</code> <pre><code>class ELBOLoss(torch.nn.Module):\n    \"\"\"\n    Compute the Evidence Lower Bound (ELBO) loss for Bayesian\n    networks. Combines a reconstruction loss with a KL divergence\n    term. Monte Carlo sampling can estimate the expected\n    reconstruction loss over stochastic layers.\n\n    Notes:\n        The KL term is weighted by `kl_weight`. The model is\n        assumed to contain Bayesian layers compatible with\n        `KLDivergenceLoss`.\n    \"\"\"\n\n    def __init__(\n        self,\n        loss_function: torch.nn.Module,\n        num_samples: int = 1,\n        kl_weight: float = 1e-3,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the ELBO loss with reconstruction and KL\n        components.\n\n        Args:\n            loss_function: Function or module used to compute\n                reconstruction loss.\n            num_samples: Number of Monte Carlo samples used for\n                estimation.\n            kl_weight: Weight applied to the KL divergence term.\n            **kwargs: Extra arguments passed to the base class.\n\n        Returns:\n            None\n        \"\"\"\n\n        # Call super class constructor\n        super().__init__(**kwargs)\n\n        # Set attributes\n        self.loss_function = loss_function\n        self.num_samples = num_samples\n        self.kl_weight = kl_weight\n        self.kl_loss = KLDivergenceLoss(weight=kl_weight)\n\n    def forward(\n        self, outputs: torch.Tensor, targets: torch.Tensor, model: torch.nn.Module\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Compute the ELBO loss with Monte Carlo sampling and KL\n        regularization.\n\n        Args:\n            outputs: Predictions generated by the model.\n            targets: Ground truth values for training.\n            model: Model containing Bayesian layers.\n\n        Returns:\n            torch.Tensor: Scalar ELBO loss averaged over samples.\n\n        Notes:\n            The loss is averaged over `num_samples` Monte Carlo\n            draws.\n        \"\"\"\n\n        loss_value = torch.tensor(\n            0, device=next(model.parameters()).device, dtype=torch.float32\n        )\n\n        for _ in range(self.num_samples):\n            loss_value += self.loss_function(outputs, targets) + self.kl_loss(model)\n\n        loss_value /= self.num_samples\n\n        return loss_value\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/losses.html#illia.losses.torch.elbo.ELBOLoss.__init__","title":"<code>__init__(loss_function, num_samples=1, kl_weight=0.001, **kwargs)</code>","text":"<p>Initialize the ELBO loss with reconstruction and KL components.</p> <p>Parameters:</p> Name Type Description Default <code>loss_function</code> <code>Module</code> <p>Function or module used to compute reconstruction loss.</p> required <code>num_samples</code> <code>int</code> <p>Number of Monte Carlo samples used for estimation.</p> <code>1</code> <code>kl_weight</code> <code>float</code> <p>Weight applied to the KL divergence term.</p> <code>0.001</code> <code>**kwargs</code> <code>Any</code> <p>Extra arguments passed to the base class.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>illia/losses/torch/elbo.py</code> <pre><code>def __init__(\n    self,\n    loss_function: torch.nn.Module,\n    num_samples: int = 1,\n    kl_weight: float = 1e-3,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialize the ELBO loss with reconstruction and KL\n    components.\n\n    Args:\n        loss_function: Function or module used to compute\n            reconstruction loss.\n        num_samples: Number of Monte Carlo samples used for\n            estimation.\n        kl_weight: Weight applied to the KL divergence term.\n        **kwargs: Extra arguments passed to the base class.\n\n    Returns:\n        None\n    \"\"\"\n\n    # Call super class constructor\n    super().__init__(**kwargs)\n\n    # Set attributes\n    self.loss_function = loss_function\n    self.num_samples = num_samples\n    self.kl_weight = kl_weight\n    self.kl_loss = KLDivergenceLoss(weight=kl_weight)\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/losses.html#illia.losses.torch.elbo.ELBOLoss.forward","title":"<code>forward(outputs, targets, model)</code>","text":"<p>Compute the ELBO loss with Monte Carlo sampling and KL regularization.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>Tensor</code> <p>Predictions generated by the model.</p> required <code>targets</code> <code>Tensor</code> <p>Ground truth values for training.</p> required <code>model</code> <code>Module</code> <p>Model containing Bayesian layers.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Scalar ELBO loss averaged over samples.</p> Notes <p>The loss is averaged over <code>num_samples</code> Monte Carlo draws.</p> Source code in <code>illia/losses/torch/elbo.py</code> <pre><code>def forward(\n    self, outputs: torch.Tensor, targets: torch.Tensor, model: torch.nn.Module\n) -&gt; torch.Tensor:\n    \"\"\"\n    Compute the ELBO loss with Monte Carlo sampling and KL\n    regularization.\n\n    Args:\n        outputs: Predictions generated by the model.\n        targets: Ground truth values for training.\n        model: Model containing Bayesian layers.\n\n    Returns:\n        torch.Tensor: Scalar ELBO loss averaged over samples.\n\n    Notes:\n        The loss is averaged over `num_samples` Monte Carlo\n        draws.\n    \"\"\"\n\n    loss_value = torch.tensor(\n        0, device=next(model.parameters()).device, dtype=torch.float32\n    )\n\n    for _ in range(self.num_samples):\n        loss_value += self.loss_function(outputs, targets) + self.kl_loss(model)\n\n    loss_value /= self.num_samples\n\n    return loss_value\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/nn.html","title":"Neural Network Layers","text":""},{"location":"api/Deep%20Learning/PyTorch/nn.html#illia.nn.torch.base.BayesianModule","title":"<code>BayesianModule</code>","text":"<p>Abstract base for Bayesian-aware modules in PyTorch. Provides mechanisms to track if a module is Bayesian and control parameter updates through freezing/unfreezing.</p> Notes <p>All derived classes must implement <code>freeze</code> and <code>kl_cost</code> to handle parameter management and compute the KL divergence cost.</p> Source code in <code>illia/nn/torch/base.py</code> <pre><code>class BayesianModule(torch.nn.Module, ABC):\n    \"\"\"\n    Abstract base for Bayesian-aware modules in PyTorch.\n    Provides mechanisms to track if a module is Bayesian and control\n    parameter updates through freezing/unfreezing.\n\n    Notes:\n        All derived classes must implement `freeze` and `kl_cost` to\n        handle parameter management and compute the KL divergence cost.\n    \"\"\"\n\n    def __init__(self, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Initialize the Bayesian module with default flags.\n        Sets `frozen` to False and `is_bayesian` to True.\n\n        Args:\n            **kwargs: Extra arguments passed to the base class.\n\n        Returns:\n            None.\n        \"\"\"\n\n        # Call super class constructor\n        super().__init__(**kwargs)\n\n        # Set attributes\n        self.frozen: bool = False\n        self.is_bayesian: bool = True\n\n    @torch.jit.export\n    def freeze(self) -&gt; None:\n        \"\"\"\n        Freeze the module's parameters to stop gradient computation.\n        If weights or biases are not sampled yet, they are sampled first.\n        Once frozen, parameters are not resampled or updated.\n\n        Returns:\n            None.\n\n        Notes:\n            Must be implemented by all subclasses.\n        \"\"\"\n\n        # Set frozen indicator to true for current layer\n        self.frozen = True\n\n    @torch.jit.export\n    def unfreeze(self) -&gt; None:\n        \"\"\"\n        Unfreeze the module by setting its `frozen` flag to False.\n        Allows parameters to be sampled and updated again.\n\n        Returns:\n            None.\n        \"\"\"\n\n        # Set frozen indicator to false for current layer\n        self.frozen = False\n\n    @torch.jit.export\n    def kl_cost(self) -&gt; tuple[torch.Tensor, int]:\n        \"\"\"\n        Compute the KL divergence cost for all Bayesian parameters.\n\n        Returns:\n            tuple[torch.Tensor, int]: A tuple containing the KL\n                divergence cost and the total number of parameters in\n                the layer.\n\n        Notes:\n            Must be implemented by all subclasses.\n        \"\"\"\n\n        return torch.tensor(0.0), 0\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/nn.html#illia.nn.torch.base.BayesianModule.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the Bayesian module with default flags. Sets <code>frozen</code> to False and <code>is_bayesian</code> to True.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Extra arguments passed to the base class.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>illia/nn/torch/base.py</code> <pre><code>def __init__(self, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Initialize the Bayesian module with default flags.\n    Sets `frozen` to False and `is_bayesian` to True.\n\n    Args:\n        **kwargs: Extra arguments passed to the base class.\n\n    Returns:\n        None.\n    \"\"\"\n\n    # Call super class constructor\n    super().__init__(**kwargs)\n\n    # Set attributes\n    self.frozen: bool = False\n    self.is_bayesian: bool = True\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/nn.html#illia.nn.torch.base.BayesianModule.freeze","title":"<code>freeze()</code>","text":"<p>Freeze the module's parameters to stop gradient computation. If weights or biases are not sampled yet, they are sampled first. Once frozen, parameters are not resampled or updated.</p> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Notes <p>Must be implemented by all subclasses.</p> Source code in <code>illia/nn/torch/base.py</code> <pre><code>@torch.jit.export\ndef freeze(self) -&gt; None:\n    \"\"\"\n    Freeze the module's parameters to stop gradient computation.\n    If weights or biases are not sampled yet, they are sampled first.\n    Once frozen, parameters are not resampled or updated.\n\n    Returns:\n        None.\n\n    Notes:\n        Must be implemented by all subclasses.\n    \"\"\"\n\n    # Set frozen indicator to true for current layer\n    self.frozen = True\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/nn.html#illia.nn.torch.base.BayesianModule.kl_cost","title":"<code>kl_cost()</code>","text":"<p>Compute the KL divergence cost for all Bayesian parameters.</p> <p>Returns:</p> Type Description <code>tuple[Tensor, int]</code> <p>tuple[torch.Tensor, int]: A tuple containing the KL divergence cost and the total number of parameters in the layer.</p> Notes <p>Must be implemented by all subclasses.</p> Source code in <code>illia/nn/torch/base.py</code> <pre><code>@torch.jit.export\ndef kl_cost(self) -&gt; tuple[torch.Tensor, int]:\n    \"\"\"\n    Compute the KL divergence cost for all Bayesian parameters.\n\n    Returns:\n        tuple[torch.Tensor, int]: A tuple containing the KL\n            divergence cost and the total number of parameters in\n            the layer.\n\n    Notes:\n        Must be implemented by all subclasses.\n    \"\"\"\n\n    return torch.tensor(0.0), 0\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/nn.html#illia.nn.torch.base.BayesianModule.unfreeze","title":"<code>unfreeze()</code>","text":"<p>Unfreeze the module by setting its <code>frozen</code> flag to False. Allows parameters to be sampled and updated again.</p> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>illia/nn/torch/base.py</code> <pre><code>@torch.jit.export\ndef unfreeze(self) -&gt; None:\n    \"\"\"\n    Unfreeze the module by setting its `frozen` flag to False.\n    Allows parameters to be sampled and updated again.\n\n    Returns:\n        None.\n    \"\"\"\n\n    # Set frozen indicator to false for current layer\n    self.frozen = False\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/nn.html#illia.nn.torch.conv1d.Conv1d","title":"<code>Conv1d</code>","text":"<p>Bayesian 1D convolutional layer with optional weight and bias priors. Behaves like a standard Conv1d but treats weights and bias as random variables sampled from specified distributions. Parameters become fixed when the layer is frozen.</p> Source code in <code>illia/nn/torch/conv1d.py</code> <pre><code>class Conv1d(BayesianModule):\n    \"\"\"\n    Bayesian 1D convolutional layer with optional weight and bias priors.\n    Behaves like a standard Conv1d but treats weights and bias as random\n    variables sampled from specified distributions. Parameters become fixed\n    when the layer is frozen.\n    \"\"\"\n\n    weights: torch.Tensor\n    bias: torch.Tensor\n\n    def __init__(\n        self,\n        input_channels: int,\n        output_channels: int,\n        kernel_size: int,\n        stride: int = 1,\n        padding: int = 0,\n        dilation: int = 1,\n        groups: int = 1,\n        weights_distribution: Optional[GaussianDistribution] = None,\n        bias_distribution: Optional[GaussianDistribution] = None,\n        use_bias: bool = True,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes a Bayesian 1D convolutional layer.\n\n        Args:\n            input_channels: Number of input channels.\n            output_channels: Number of output channels.\n            kernel_size: Size of the convolution kernel.\n            stride: Stride of the convolution.\n            padding: Padding added to both sides of the input.\n            dilation: Spacing between kernel elements.\n            groups: Number of blocked connections.\n            weights_distribution: Distribution for the weights.\n            bias_distribution: Distribution for the bias.\n            use_bias: Whether to include a bias term.\n            **kwargs: Extra arguments passed to the base class.\n\n        Returns:\n            None.\n\n        Notes:\n            Gaussian distributions are used by default if none are\n            provided.\n        \"\"\"\n\n        # Call super class constructor\n        super().__init__(**kwargs)\n\n        # Set attributes\n        self.input_channels = input_channels\n        self.output_channels = output_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n        self.groups = groups\n        self.use_bias = use_bias\n\n        # Set weights distribution\n        if weights_distribution is None:\n            # Define weights distribution\n            self.weights_distribution = GaussianDistribution(\n                (\n                    self.output_channels,\n                    self.input_channels // self.groups,\n                    self.kernel_size,\n                )\n            )\n        else:\n            self.weights_distribution = weights_distribution\n\n        # Set bias distribution\n        if self.use_bias:\n            if bias_distribution is None:\n                self.bias_distribution = GaussianDistribution((self.output_channels,))\n            else:\n                self.bias_distribution = bias_distribution\n        else:\n            self.bias_distribution = None  # type: ignore\n\n        # Sample initial weights\n        weights = self.weights_distribution.sample()\n\n        # Register buffers\n        self.register_buffer(\"weights\", weights)\n\n        if self.use_bias and self.bias_distribution is not None:\n            bias = self.bias_distribution.sample()\n            self.register_buffer(\"bias\", bias)\n        else:\n            self.bias = None  # type: ignore\n\n    @torch.jit.export\n    def freeze(self) -&gt; None:\n        \"\"\"\n        Freeze the module's parameters to stop gradient computation.\n        If weights or biases are not sampled yet, they are sampled first.\n        Once frozen, parameters are not resampled or updated.\n\n        Returns:\n            None.\n        \"\"\"\n\n        # Set indicator\n        self.frozen = True\n\n        # Sample weights if they are undefined\n        if self.weights is None:\n            self.weights = self.weights_distribution.sample()\n\n        # Sample bias if they are undefined and bias is used\n        if self.use_bias and self.bias_distribution is not None:\n            if not hasattr(self, \"bias\") or self.bias is None:\n                self.bias = self.bias_distribution.sample()\n            self.bias = self.bias.detach()\n\n        # Detach weights and bias\n        self.weights = self.weights.detach()\n\n    @torch.jit.export\n    def kl_cost(self) -&gt; tuple[torch.Tensor, int]:\n        \"\"\"\n        Compute the KL divergence cost for all Bayesian parameters.\n\n        Returns:\n            tuple[torch.Tensor, int]: A tuple containing the KL\n                divergence cost and the total number of parameters in\n                the layer.\n        \"\"\"\n\n        # Compute log probs\n        log_probs: torch.Tensor = self.weights_distribution.log_prob(self.weights)\n\n        # Add bias log probs if bias is used\n        if self.use_bias and self.bias_distribution is not None:\n            log_probs += self.bias_distribution.log_prob(self.bias)\n\n        # Compute number of parameters\n        num_params: int = self.weights_distribution.num_params()\n        if self.use_bias and self.bias_distribution is not None:\n            num_params += self.bias_distribution.num_params()\n\n        return log_probs, num_params\n\n    def forward(self, inputs: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a forward pass through the Bayesian Convolution 1D\n        layer. If the layer is not frozen, it samples weights and bias\n        from their respective distributions. If the layer is frozen\n        and the weights or bias are not initialized, it also performs\n        sampling.\n\n        Args:\n            inputs: Input tensor to the layer with shape (batch,\n                input channels, input width, input height).\n\n        Returns:\n            Output tensor after passing through the layer with shape\n                (batch, output channels, output width, output height).\n\n        Raises:\n            ValueError: If the layer is frozen but weights or bias are\n                undefined.\n        \"\"\"\n\n        # Check if layer is frozen\n        if not self.frozen:\n            self.weights = self.weights_distribution.sample()\n\n            # Sample bias only if using bias\n            if self.use_bias and self.bias_distribution is not None:\n                self.bias = self.bias_distribution.sample()\n        elif self.weights is None or (self.use_bias and self.bias is None):\n            raise ValueError(\n                \"Module has been frozen with undefined weights and/or bias.\"\n            )\n\n        # Compute outputs\n        # pylint: disable=E1102\n        outputs: torch.Tensor = F.conv1d(\n            input=inputs,\n            weight=self.weights,\n            stride=self.stride,\n            padding=self.padding,\n            dilation=self.dilation,\n            groups=self.groups,\n        )\n\n        # Add bias only if using bias\n        if self.use_bias and self.bias is not None:\n            outputs += torch.reshape(\n                input=self.bias, shape=(1, self.output_channels, 1)\n            )\n\n        return outputs\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/nn.html#illia.nn.torch.conv1d.Conv1d.__init__","title":"<code>__init__(input_channels, output_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, weights_distribution=None, bias_distribution=None, use_bias=True, **kwargs)</code>","text":"<p>Initializes a Bayesian 1D convolutional layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>output_channels</code> <code>int</code> <p>Number of output channels.</p> required <code>kernel_size</code> <code>int</code> <p>Size of the convolution kernel.</p> required <code>stride</code> <code>int</code> <p>Stride of the convolution.</p> <code>1</code> <code>padding</code> <code>int</code> <p>Padding added to both sides of the input.</p> <code>0</code> <code>dilation</code> <code>int</code> <p>Spacing between kernel elements.</p> <code>1</code> <code>groups</code> <code>int</code> <p>Number of blocked connections.</p> <code>1</code> <code>weights_distribution</code> <code>Optional[GaussianDistribution]</code> <p>Distribution for the weights.</p> <code>None</code> <code>bias_distribution</code> <code>Optional[GaussianDistribution]</code> <p>Distribution for the bias.</p> <code>None</code> <code>use_bias</code> <code>bool</code> <p>Whether to include a bias term.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Extra arguments passed to the base class.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Notes <p>Gaussian distributions are used by default if none are provided.</p> Source code in <code>illia/nn/torch/conv1d.py</code> <pre><code>def __init__(\n    self,\n    input_channels: int,\n    output_channels: int,\n    kernel_size: int,\n    stride: int = 1,\n    padding: int = 0,\n    dilation: int = 1,\n    groups: int = 1,\n    weights_distribution: Optional[GaussianDistribution] = None,\n    bias_distribution: Optional[GaussianDistribution] = None,\n    use_bias: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes a Bayesian 1D convolutional layer.\n\n    Args:\n        input_channels: Number of input channels.\n        output_channels: Number of output channels.\n        kernel_size: Size of the convolution kernel.\n        stride: Stride of the convolution.\n        padding: Padding added to both sides of the input.\n        dilation: Spacing between kernel elements.\n        groups: Number of blocked connections.\n        weights_distribution: Distribution for the weights.\n        bias_distribution: Distribution for the bias.\n        use_bias: Whether to include a bias term.\n        **kwargs: Extra arguments passed to the base class.\n\n    Returns:\n        None.\n\n    Notes:\n        Gaussian distributions are used by default if none are\n        provided.\n    \"\"\"\n\n    # Call super class constructor\n    super().__init__(**kwargs)\n\n    # Set attributes\n    self.input_channels = input_channels\n    self.output_channels = output_channels\n    self.kernel_size = kernel_size\n    self.stride = stride\n    self.padding = padding\n    self.dilation = dilation\n    self.groups = groups\n    self.use_bias = use_bias\n\n    # Set weights distribution\n    if weights_distribution is None:\n        # Define weights distribution\n        self.weights_distribution = GaussianDistribution(\n            (\n                self.output_channels,\n                self.input_channels // self.groups,\n                self.kernel_size,\n            )\n        )\n    else:\n        self.weights_distribution = weights_distribution\n\n    # Set bias distribution\n    if self.use_bias:\n        if bias_distribution is None:\n            self.bias_distribution = GaussianDistribution((self.output_channels,))\n        else:\n            self.bias_distribution = bias_distribution\n    else:\n        self.bias_distribution = None  # type: ignore\n\n    # Sample initial weights\n    weights = self.weights_distribution.sample()\n\n    # Register buffers\n    self.register_buffer(\"weights\", weights)\n\n    if self.use_bias and self.bias_distribution is not None:\n        bias = self.bias_distribution.sample()\n        self.register_buffer(\"bias\", bias)\n    else:\n        self.bias = None  # type: ignore\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/nn.html#illia.nn.torch.conv1d.Conv1d.forward","title":"<code>forward(inputs)</code>","text":"<p>Performs a forward pass through the Bayesian Convolution 1D layer. If the layer is not frozen, it samples weights and bias from their respective distributions. If the layer is frozen and the weights or bias are not initialized, it also performs sampling.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Tensor</code> <p>Input tensor to the layer with shape (batch, input channels, input width, input height).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor after passing through the layer with shape (batch, output channels, output width, output height).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the layer is frozen but weights or bias are undefined.</p> Source code in <code>illia/nn/torch/conv1d.py</code> <pre><code>def forward(self, inputs: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a forward pass through the Bayesian Convolution 1D\n    layer. If the layer is not frozen, it samples weights and bias\n    from their respective distributions. If the layer is frozen\n    and the weights or bias are not initialized, it also performs\n    sampling.\n\n    Args:\n        inputs: Input tensor to the layer with shape (batch,\n            input channels, input width, input height).\n\n    Returns:\n        Output tensor after passing through the layer with shape\n            (batch, output channels, output width, output height).\n\n    Raises:\n        ValueError: If the layer is frozen but weights or bias are\n            undefined.\n    \"\"\"\n\n    # Check if layer is frozen\n    if not self.frozen:\n        self.weights = self.weights_distribution.sample()\n\n        # Sample bias only if using bias\n        if self.use_bias and self.bias_distribution is not None:\n            self.bias = self.bias_distribution.sample()\n    elif self.weights is None or (self.use_bias and self.bias is None):\n        raise ValueError(\n            \"Module has been frozen with undefined weights and/or bias.\"\n        )\n\n    # Compute outputs\n    # pylint: disable=E1102\n    outputs: torch.Tensor = F.conv1d(\n        input=inputs,\n        weight=self.weights,\n        stride=self.stride,\n        padding=self.padding,\n        dilation=self.dilation,\n        groups=self.groups,\n    )\n\n    # Add bias only if using bias\n    if self.use_bias and self.bias is not None:\n        outputs += torch.reshape(\n            input=self.bias, shape=(1, self.output_channels, 1)\n        )\n\n    return outputs\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/nn.html#illia.nn.torch.conv1d.Conv1d.freeze","title":"<code>freeze()</code>","text":"<p>Freeze the module's parameters to stop gradient computation. If weights or biases are not sampled yet, they are sampled first. Once frozen, parameters are not resampled or updated.</p> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>illia/nn/torch/conv1d.py</code> <pre><code>@torch.jit.export\ndef freeze(self) -&gt; None:\n    \"\"\"\n    Freeze the module's parameters to stop gradient computation.\n    If weights or biases are not sampled yet, they are sampled first.\n    Once frozen, parameters are not resampled or updated.\n\n    Returns:\n        None.\n    \"\"\"\n\n    # Set indicator\n    self.frozen = True\n\n    # Sample weights if they are undefined\n    if self.weights is None:\n        self.weights = self.weights_distribution.sample()\n\n    # Sample bias if they are undefined and bias is used\n    if self.use_bias and self.bias_distribution is not None:\n        if not hasattr(self, \"bias\") or self.bias is None:\n            self.bias = self.bias_distribution.sample()\n        self.bias = self.bias.detach()\n\n    # Detach weights and bias\n    self.weights = self.weights.detach()\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/nn.html#illia.nn.torch.conv1d.Conv1d.kl_cost","title":"<code>kl_cost()</code>","text":"<p>Compute the KL divergence cost for all Bayesian parameters.</p> <p>Returns:</p> Type Description <code>tuple[Tensor, int]</code> <p>tuple[torch.Tensor, int]: A tuple containing the KL divergence cost and the total number of parameters in the layer.</p> Source code in <code>illia/nn/torch/conv1d.py</code> <pre><code>@torch.jit.export\ndef kl_cost(self) -&gt; tuple[torch.Tensor, int]:\n    \"\"\"\n    Compute the KL divergence cost for all Bayesian parameters.\n\n    Returns:\n        tuple[torch.Tensor, int]: A tuple containing the KL\n            divergence cost and the total number of parameters in\n            the layer.\n    \"\"\"\n\n    # Compute log probs\n    log_probs: torch.Tensor = self.weights_distribution.log_prob(self.weights)\n\n    # Add bias log probs if bias is used\n    if self.use_bias and self.bias_distribution is not None:\n        log_probs += self.bias_distribution.log_prob(self.bias)\n\n    # Compute number of parameters\n    num_params: int = self.weights_distribution.num_params()\n    if self.use_bias and self.bias_distribution is not None:\n        num_params += self.bias_distribution.num_params()\n\n    return log_probs, num_params\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/nn.html#illia.nn.torch.conv2d.Conv2d","title":"<code>Conv2d</code>","text":"<p>Bayesian 2D convolutional layer with optional weight and bias priors. Behaves like a standard Conv2d but treats weights and bias as random variables sampled from specified distributions. Parameters become fixed when the layer is frozen.</p> Source code in <code>illia/nn/torch/conv2d.py</code> <pre><code>class Conv2d(BayesianModule):\n    \"\"\"\n    Bayesian 2D convolutional layer with optional weight and bias priors.\n    Behaves like a standard Conv2d but treats weights and bias as random\n    variables sampled from specified distributions. Parameters become fixed\n    when the layer is frozen.\n    \"\"\"\n\n    weights: torch.Tensor\n    bias: torch.Tensor\n\n    def __init__(\n        self,\n        input_channels: int,\n        output_channels: int,\n        kernel_size: int | tuple[int, int],\n        stride: int | tuple[int, int] = 1,\n        padding: int | tuple[int, int] = 0,\n        dilation: int | tuple[int, int] = 1,\n        groups: int = 1,\n        weights_distribution: Optional[GaussianDistribution] = None,\n        bias_distribution: Optional[GaussianDistribution] = None,\n        use_bias: bool = True,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes a Bayesian 2D convolutional layer.\n\n        Args:\n            kernel_size: Size of the convolving kernel.\n            stride: Stride of the convolution. Deafults to 1.\n            padding: Padding added to all four sides of the input.\n            dilation: Spacing between kernel elements.\n            groups: Number of blocked connections from input channels\n                to output channels.\n            weights_distribution: The distribution for the weights.\n            bias_distribution: The distribution for the bias.\n            use_bias: Whether to include a bias term.\n            **kwargs: Extra arguments passed to the base class.\n\n        Returns:\n            None.\n\n        Notes:\n            Gaussian distributions are used by default if none are\n            provided.\n        \"\"\"\n\n        # Call super class constructor\n        super().__init__(**kwargs)\n\n        # Set attributes\n        self.input_channels = input_channels\n        self.output_channels = output_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n        self.groups = groups\n        self.use_bias = use_bias\n\n        # Set weights distribution\n        if weights_distribution is None:\n            # Extend kernel if we only have 1 value\n            if isinstance(self.kernel_size, int):\n                self.kernel_size = (self.kernel_size, self.kernel_size)\n\n            # Define weights distribution\n            self.weights_distribution = GaussianDistribution(\n                (\n                    self.output_channels,\n                    self.input_channels // self.groups,\n                    *self.kernel_size,\n                )\n            )\n        else:\n            self.weights_distribution = weights_distribution\n\n        # Set bias distribution\n        if self.use_bias:\n            if bias_distribution is None:\n                self.bias_distribution = GaussianDistribution((self.output_channels,))\n            else:\n                self.bias_distribution = bias_distribution\n        else:\n            self.bias_distribution = None  # type: ignore\n\n        # Sample initial weights\n        weights = self.weights_distribution.sample()\n\n        # Register buffers\n        self.register_buffer(\"weights\", weights)\n\n        if self.use_bias and self.bias_distribution is not None:\n            bias = self.bias_distribution.sample()\n            self.register_buffer(\"bias\", bias)\n        else:\n            self.bias = None  # type: ignore\n\n    @torch.jit.export\n    def freeze(self) -&gt; None:\n        \"\"\"\n        Freeze the module's parameters to stop gradient computation.\n        If weights or biases are not sampled yet, they are sampled first.\n        Once frozen, parameters are not resampled or updated.\n\n        Returns:\n            None.\n        \"\"\"\n\n        # Set indicator\n        self.frozen = True\n\n        # Sample weights if they are undefined\n        if self.weights is None:\n            self.weights = self.weights_distribution.sample()\n\n        # Sample bias if they are undefined and bias is used\n        if self.use_bias and self.bias_distribution is not None:\n            if not hasattr(self, \"bias\") or self.bias is None:\n                self.bias = self.bias_distribution.sample()\n            self.bias = self.bias.detach()\n\n        # Detach weights and bias\n        self.weights = self.weights.detach()\n\n    @torch.jit.export\n    def kl_cost(self) -&gt; tuple[torch.Tensor, int]:\n        \"\"\"\n        Compute the KL divergence cost for all Bayesian parameters.\n\n        Returns:\n            tuple[torch.Tensor, int]: A tuple containing the KL\n                divergence cost and the total number of parameters in\n                the layer.\n        \"\"\"\n\n        # Compute log probs\n        log_probs: torch.Tensor = self.weights_distribution.log_prob(self.weights)\n\n        # Add bias log probs if bias is used\n        if self.use_bias and self.bias_distribution is not None:\n            log_probs += self.bias_distribution.log_prob(self.bias)\n\n        # Compute number of parameters\n        num_params: int = self.weights_distribution.num_params()\n        if self.use_bias and self.bias_distribution is not None:\n            num_params += self.bias_distribution.num_params()\n\n        return log_probs, num_params\n\n    def forward(self, inputs: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a forward pass through the Bayesian Convolution 2D\n        layer. If the layer is not frozen, it samples weights and bias\n        from their respective distributions. If the layer is frozen\n        and the weights or bias are not initialized, it also performs\n        sampling.\n\n        Args:\n            inputs: Input tensor to the layer with shape (batch,\n                input channels, input width, input height).\n\n        Returns:\n            Output tensor after passing through the layer with shape\n                (batch, output channels, output width, output height).\n\n        Raises:\n            ValueError: If the layer is frozen but weights or bias are\n                undefined.\n        \"\"\"\n\n        # Check if layer is frozen\n        if not self.frozen:\n            self.weights = self.weights_distribution.sample()\n\n            # Sample bias only if using bias\n            if self.use_bias and self.bias_distribution is not None:\n                self.bias = self.bias_distribution.sample()\n        elif self.weights is None or (self.use_bias and self.bias is None):\n            raise ValueError(\n                \"Module has been frozen with undefined weights and/or bias.\"\n            )\n\n        # Compute outputs\n        # pylint: disable=E1102\n        outputs: torch.Tensor = F.conv2d(\n            input=inputs,\n            weight=self.weights,\n            bias=self.bias,\n            padding=self.padding,\n            dilation=self.dilation,\n            groups=self.groups,\n        )\n\n        # Add bias only if using bias\n        if self.use_bias and self.bias is not None:\n            outputs += torch.reshape(\n                input=self.bias, shape=(1, self.output_channels, 1, 1)\n            )\n\n        return outputs\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/nn.html#illia.nn.torch.conv2d.Conv2d.__init__","title":"<code>__init__(input_channels, output_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, weights_distribution=None, bias_distribution=None, use_bias=True, **kwargs)</code>","text":"<p>Initializes a Bayesian 2D convolutional layer.</p> <p>Parameters:</p> Name Type Description Default <code>kernel_size</code> <code>int | tuple[int, int]</code> <p>Size of the convolving kernel.</p> required <code>stride</code> <code>int | tuple[int, int]</code> <p>Stride of the convolution. Deafults to 1.</p> <code>1</code> <code>padding</code> <code>int | tuple[int, int]</code> <p>Padding added to all four sides of the input.</p> <code>0</code> <code>dilation</code> <code>int | tuple[int, int]</code> <p>Spacing between kernel elements.</p> <code>1</code> <code>groups</code> <code>int</code> <p>Number of blocked connections from input channels to output channels.</p> <code>1</code> <code>weights_distribution</code> <code>Optional[GaussianDistribution]</code> <p>The distribution for the weights.</p> <code>None</code> <code>bias_distribution</code> <code>Optional[GaussianDistribution]</code> <p>The distribution for the bias.</p> <code>None</code> <code>use_bias</code> <code>bool</code> <p>Whether to include a bias term.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Extra arguments passed to the base class.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Notes <p>Gaussian distributions are used by default if none are provided.</p> Source code in <code>illia/nn/torch/conv2d.py</code> <pre><code>def __init__(\n    self,\n    input_channels: int,\n    output_channels: int,\n    kernel_size: int | tuple[int, int],\n    stride: int | tuple[int, int] = 1,\n    padding: int | tuple[int, int] = 0,\n    dilation: int | tuple[int, int] = 1,\n    groups: int = 1,\n    weights_distribution: Optional[GaussianDistribution] = None,\n    bias_distribution: Optional[GaussianDistribution] = None,\n    use_bias: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes a Bayesian 2D convolutional layer.\n\n    Args:\n        kernel_size: Size of the convolving kernel.\n        stride: Stride of the convolution. Deafults to 1.\n        padding: Padding added to all four sides of the input.\n        dilation: Spacing between kernel elements.\n        groups: Number of blocked connections from input channels\n            to output channels.\n        weights_distribution: The distribution for the weights.\n        bias_distribution: The distribution for the bias.\n        use_bias: Whether to include a bias term.\n        **kwargs: Extra arguments passed to the base class.\n\n    Returns:\n        None.\n\n    Notes:\n        Gaussian distributions are used by default if none are\n        provided.\n    \"\"\"\n\n    # Call super class constructor\n    super().__init__(**kwargs)\n\n    # Set attributes\n    self.input_channels = input_channels\n    self.output_channels = output_channels\n    self.kernel_size = kernel_size\n    self.stride = stride\n    self.padding = padding\n    self.dilation = dilation\n    self.groups = groups\n    self.use_bias = use_bias\n\n    # Set weights distribution\n    if weights_distribution is None:\n        # Extend kernel if we only have 1 value\n        if isinstance(self.kernel_size, int):\n            self.kernel_size = (self.kernel_size, self.kernel_size)\n\n        # Define weights distribution\n        self.weights_distribution = GaussianDistribution(\n            (\n                self.output_channels,\n                self.input_channels // self.groups,\n                *self.kernel_size,\n            )\n        )\n    else:\n        self.weights_distribution = weights_distribution\n\n    # Set bias distribution\n    if self.use_bias:\n        if bias_distribution is None:\n            self.bias_distribution = GaussianDistribution((self.output_channels,))\n        else:\n            self.bias_distribution = bias_distribution\n    else:\n        self.bias_distribution = None  # type: ignore\n\n    # Sample initial weights\n    weights = self.weights_distribution.sample()\n\n    # Register buffers\n    self.register_buffer(\"weights\", weights)\n\n    if self.use_bias and self.bias_distribution is not None:\n        bias = self.bias_distribution.sample()\n        self.register_buffer(\"bias\", bias)\n    else:\n        self.bias = None  # type: ignore\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/nn.html#illia.nn.torch.conv2d.Conv2d.forward","title":"<code>forward(inputs)</code>","text":"<p>Performs a forward pass through the Bayesian Convolution 2D layer. If the layer is not frozen, it samples weights and bias from their respective distributions. If the layer is frozen and the weights or bias are not initialized, it also performs sampling.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Tensor</code> <p>Input tensor to the layer with shape (batch, input channels, input width, input height).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor after passing through the layer with shape (batch, output channels, output width, output height).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the layer is frozen but weights or bias are undefined.</p> Source code in <code>illia/nn/torch/conv2d.py</code> <pre><code>def forward(self, inputs: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a forward pass through the Bayesian Convolution 2D\n    layer. If the layer is not frozen, it samples weights and bias\n    from their respective distributions. If the layer is frozen\n    and the weights or bias are not initialized, it also performs\n    sampling.\n\n    Args:\n        inputs: Input tensor to the layer with shape (batch,\n            input channels, input width, input height).\n\n    Returns:\n        Output tensor after passing through the layer with shape\n            (batch, output channels, output width, output height).\n\n    Raises:\n        ValueError: If the layer is frozen but weights or bias are\n            undefined.\n    \"\"\"\n\n    # Check if layer is frozen\n    if not self.frozen:\n        self.weights = self.weights_distribution.sample()\n\n        # Sample bias only if using bias\n        if self.use_bias and self.bias_distribution is not None:\n            self.bias = self.bias_distribution.sample()\n    elif self.weights is None or (self.use_bias and self.bias is None):\n        raise ValueError(\n            \"Module has been frozen with undefined weights and/or bias.\"\n        )\n\n    # Compute outputs\n    # pylint: disable=E1102\n    outputs: torch.Tensor = F.conv2d(\n        input=inputs,\n        weight=self.weights,\n        bias=self.bias,\n        padding=self.padding,\n        dilation=self.dilation,\n        groups=self.groups,\n    )\n\n    # Add bias only if using bias\n    if self.use_bias and self.bias is not None:\n        outputs += torch.reshape(\n            input=self.bias, shape=(1, self.output_channels, 1, 1)\n        )\n\n    return outputs\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/nn.html#illia.nn.torch.conv2d.Conv2d.freeze","title":"<code>freeze()</code>","text":"<p>Freeze the module's parameters to stop gradient computation. If weights or biases are not sampled yet, they are sampled first. Once frozen, parameters are not resampled or updated.</p> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>illia/nn/torch/conv2d.py</code> <pre><code>@torch.jit.export\ndef freeze(self) -&gt; None:\n    \"\"\"\n    Freeze the module's parameters to stop gradient computation.\n    If weights or biases are not sampled yet, they are sampled first.\n    Once frozen, parameters are not resampled or updated.\n\n    Returns:\n        None.\n    \"\"\"\n\n    # Set indicator\n    self.frozen = True\n\n    # Sample weights if they are undefined\n    if self.weights is None:\n        self.weights = self.weights_distribution.sample()\n\n    # Sample bias if they are undefined and bias is used\n    if self.use_bias and self.bias_distribution is not None:\n        if not hasattr(self, \"bias\") or self.bias is None:\n            self.bias = self.bias_distribution.sample()\n        self.bias = self.bias.detach()\n\n    # Detach weights and bias\n    self.weights = self.weights.detach()\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/nn.html#illia.nn.torch.conv2d.Conv2d.kl_cost","title":"<code>kl_cost()</code>","text":"<p>Compute the KL divergence cost for all Bayesian parameters.</p> <p>Returns:</p> Type Description <code>tuple[Tensor, int]</code> <p>tuple[torch.Tensor, int]: A tuple containing the KL divergence cost and the total number of parameters in the layer.</p> Source code in <code>illia/nn/torch/conv2d.py</code> <pre><code>@torch.jit.export\ndef kl_cost(self) -&gt; tuple[torch.Tensor, int]:\n    \"\"\"\n    Compute the KL divergence cost for all Bayesian parameters.\n\n    Returns:\n        tuple[torch.Tensor, int]: A tuple containing the KL\n            divergence cost and the total number of parameters in\n            the layer.\n    \"\"\"\n\n    # Compute log probs\n    log_probs: torch.Tensor = self.weights_distribution.log_prob(self.weights)\n\n    # Add bias log probs if bias is used\n    if self.use_bias and self.bias_distribution is not None:\n        log_probs += self.bias_distribution.log_prob(self.bias)\n\n    # Compute number of parameters\n    num_params: int = self.weights_distribution.num_params()\n    if self.use_bias and self.bias_distribution is not None:\n        num_params += self.bias_distribution.num_params()\n\n    return log_probs, num_params\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/nn.html#illia.nn.torch.embedding.Embedding","title":"<code>Embedding</code>","text":"<p>This class is the bayesian implementation of the Embedding class.</p> Source code in <code>illia/nn/torch/embedding.py</code> <pre><code>class Embedding(BayesianModule):\n    \"\"\"\n    This class is the bayesian implementation of the Embedding class.\n    \"\"\"\n\n    weights: torch.Tensor\n\n    def __init__(\n        self,\n        num_embeddings: int,\n        embeddings_dim: int,\n        padding_idx: Optional[int] = None,\n        max_norm: Optional[float] = None,\n        norm_type: float = 2.0,\n        scale_grad_by_freq: bool = False,\n        sparse: bool = False,\n        weights_distribution: Optional[GaussianDistribution] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes a Embedding layer.\n\n        Args:\n            num_embeddings: size of the dictionary of embeddings.\n            embeddings_dim: the size of each embedding vector.\n            padding_idx: If specified, the entries at padding_idx do\n                not contribute to the gradient.\n            max_norm: If given, each embedding vector with norm larger\n                than max_norm is renormalized to have norm max_norm.\n            norm_type: The p of the p-norm to compute for the max_norm\n                option.\n            scale_grad_by_freq: If given, this will scale gradients by\n                the inverse of frequency of the words in the\n                mini-batch.\n            sparse: If True, gradient w.r.t. weight matrix will be a\n                sparse tensor.\n            weights_distribution: distribution for the weights of the\n                layer.\n            **kwargs: Extra arguments passed to the base class.\n\n        Returns:\n            None.\n\n        Notes:\n            Gaussian distributions are used by default if none are\n            provided.\n        \"\"\"\n\n        # Call super class constructor\n        super().__init__(**kwargs)\n\n        # Set embeddings atributtes\n        self.num_embeddings = num_embeddings\n        self.embeddings_dim = embeddings_dim\n        self.padding_idx = padding_idx\n        self.max_norm = max_norm\n        self.norm_type = norm_type\n        self.scale_grad_by_freq = scale_grad_by_freq\n        self.sparse = sparse\n\n        # Set weights distribution\n        if weights_distribution is None:\n            self.weights_distribution = GaussianDistribution(\n                (self.num_embeddings, self.embeddings_dim)\n            )\n        else:\n            self.weights_distribution = weights_distribution\n\n        # Sample initial weights\n        weights = self.weights_distribution.sample()\n\n        # Register buffers\n        self.register_buffer(\"weights\", weights)\n\n    @torch.jit.export\n    def freeze(self) -&gt; None:\n        \"\"\"\n        Freeze the module's parameters to stop gradient computation.\n        If weights or biases are not sampled yet, they are sampled first.\n        Once frozen, parameters are not resampled or updated.\n\n        Returns:\n            None.\n        \"\"\"\n\n        # set indicator\n        self.frozen = True\n\n        # sample weights if they are undefined\n        if self.weights is None:\n            self.weights = self.weights_distribution.sample()\n\n        # detach weights\n        self.weights = self.weights.detach()\n\n    @torch.jit.export\n    def kl_cost(self) -&gt; tuple[torch.Tensor, int]:\n        \"\"\"\n        Compute the KL divergence cost for all Bayesian parameters.\n\n        Returns:\n            tuple[torch.Tensor, int]: A tuple containing the KL\n                divergence cost and the total number of parameters in\n                the layer.\n        \"\"\"\n\n        # get log posterior and log prior\n        log_probs: torch.Tensor = self.weights_distribution.log_prob(self.weights)\n\n        # get number of parameters\n        num_params: int = self.weights_distribution.num_params()\n\n        return log_probs, num_params\n\n    def forward(self, inputs: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        This method is the forward pass of the layer.\n\n        Args:\n            inputs: input tensor. Dimensions: [*].\n\n        Returns:\n            outputs tensor. Dimension: [*, embedding dim].\n\n        Raises:\n            ValueError: If the layer is frozen but weights are\n                undefined.\n        \"\"\"\n\n        # Forward depeding of frozen state\n        if not self.frozen:\n            self.weights = self.weights_distribution.sample()\n        elif self.weights is None:\n            raise ValueError(\"Module has been frozen with undefined weights\")\n\n        # Run torch forward\n        return F.embedding(\n            input=inputs,\n            weight=self.weights,\n            padding_idx=self.padding_idx,\n            max_norm=self.max_norm,\n            norm_type=self.norm_type,\n            scale_grad_by_freq=self.scale_grad_by_freq,\n            sparse=self.sparse,\n        )\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/nn.html#illia.nn.torch.embedding.Embedding.__init__","title":"<code>__init__(num_embeddings, embeddings_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, weights_distribution=None, **kwargs)</code>","text":"<p>Initializes a Embedding layer.</p> <p>Parameters:</p> Name Type Description Default <code>num_embeddings</code> <code>int</code> <p>size of the dictionary of embeddings.</p> required <code>embeddings_dim</code> <code>int</code> <p>the size of each embedding vector.</p> required <code>padding_idx</code> <code>Optional[int]</code> <p>If specified, the entries at padding_idx do not contribute to the gradient.</p> <code>None</code> <code>max_norm</code> <code>Optional[float]</code> <p>If given, each embedding vector with norm larger than max_norm is renormalized to have norm max_norm.</p> <code>None</code> <code>norm_type</code> <code>float</code> <p>The p of the p-norm to compute for the max_norm option.</p> <code>2.0</code> <code>scale_grad_by_freq</code> <code>bool</code> <p>If given, this will scale gradients by the inverse of frequency of the words in the mini-batch.</p> <code>False</code> <code>sparse</code> <code>bool</code> <p>If True, gradient w.r.t. weight matrix will be a sparse tensor.</p> <code>False</code> <code>weights_distribution</code> <code>Optional[GaussianDistribution]</code> <p>distribution for the weights of the layer.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Extra arguments passed to the base class.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Notes <p>Gaussian distributions are used by default if none are provided.</p> Source code in <code>illia/nn/torch/embedding.py</code> <pre><code>def __init__(\n    self,\n    num_embeddings: int,\n    embeddings_dim: int,\n    padding_idx: Optional[int] = None,\n    max_norm: Optional[float] = None,\n    norm_type: float = 2.0,\n    scale_grad_by_freq: bool = False,\n    sparse: bool = False,\n    weights_distribution: Optional[GaussianDistribution] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes a Embedding layer.\n\n    Args:\n        num_embeddings: size of the dictionary of embeddings.\n        embeddings_dim: the size of each embedding vector.\n        padding_idx: If specified, the entries at padding_idx do\n            not contribute to the gradient.\n        max_norm: If given, each embedding vector with norm larger\n            than max_norm is renormalized to have norm max_norm.\n        norm_type: The p of the p-norm to compute for the max_norm\n            option.\n        scale_grad_by_freq: If given, this will scale gradients by\n            the inverse of frequency of the words in the\n            mini-batch.\n        sparse: If True, gradient w.r.t. weight matrix will be a\n            sparse tensor.\n        weights_distribution: distribution for the weights of the\n            layer.\n        **kwargs: Extra arguments passed to the base class.\n\n    Returns:\n        None.\n\n    Notes:\n        Gaussian distributions are used by default if none are\n        provided.\n    \"\"\"\n\n    # Call super class constructor\n    super().__init__(**kwargs)\n\n    # Set embeddings atributtes\n    self.num_embeddings = num_embeddings\n    self.embeddings_dim = embeddings_dim\n    self.padding_idx = padding_idx\n    self.max_norm = max_norm\n    self.norm_type = norm_type\n    self.scale_grad_by_freq = scale_grad_by_freq\n    self.sparse = sparse\n\n    # Set weights distribution\n    if weights_distribution is None:\n        self.weights_distribution = GaussianDistribution(\n            (self.num_embeddings, self.embeddings_dim)\n        )\n    else:\n        self.weights_distribution = weights_distribution\n\n    # Sample initial weights\n    weights = self.weights_distribution.sample()\n\n    # Register buffers\n    self.register_buffer(\"weights\", weights)\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/nn.html#illia.nn.torch.embedding.Embedding.forward","title":"<code>forward(inputs)</code>","text":"<p>This method is the forward pass of the layer.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Tensor</code> <p>input tensor. Dimensions: [*].</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>outputs tensor. Dimension: [*, embedding dim].</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the layer is frozen but weights are undefined.</p> Source code in <code>illia/nn/torch/embedding.py</code> <pre><code>def forward(self, inputs: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    This method is the forward pass of the layer.\n\n    Args:\n        inputs: input tensor. Dimensions: [*].\n\n    Returns:\n        outputs tensor. Dimension: [*, embedding dim].\n\n    Raises:\n        ValueError: If the layer is frozen but weights are\n            undefined.\n    \"\"\"\n\n    # Forward depeding of frozen state\n    if not self.frozen:\n        self.weights = self.weights_distribution.sample()\n    elif self.weights is None:\n        raise ValueError(\"Module has been frozen with undefined weights\")\n\n    # Run torch forward\n    return F.embedding(\n        input=inputs,\n        weight=self.weights,\n        padding_idx=self.padding_idx,\n        max_norm=self.max_norm,\n        norm_type=self.norm_type,\n        scale_grad_by_freq=self.scale_grad_by_freq,\n        sparse=self.sparse,\n    )\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/nn.html#illia.nn.torch.embedding.Embedding.freeze","title":"<code>freeze()</code>","text":"<p>Freeze the module's parameters to stop gradient computation. If weights or biases are not sampled yet, they are sampled first. Once frozen, parameters are not resampled or updated.</p> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>illia/nn/torch/embedding.py</code> <pre><code>@torch.jit.export\ndef freeze(self) -&gt; None:\n    \"\"\"\n    Freeze the module's parameters to stop gradient computation.\n    If weights or biases are not sampled yet, they are sampled first.\n    Once frozen, parameters are not resampled or updated.\n\n    Returns:\n        None.\n    \"\"\"\n\n    # set indicator\n    self.frozen = True\n\n    # sample weights if they are undefined\n    if self.weights is None:\n        self.weights = self.weights_distribution.sample()\n\n    # detach weights\n    self.weights = self.weights.detach()\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/nn.html#illia.nn.torch.embedding.Embedding.kl_cost","title":"<code>kl_cost()</code>","text":"<p>Compute the KL divergence cost for all Bayesian parameters.</p> <p>Returns:</p> Type Description <code>tuple[Tensor, int]</code> <p>tuple[torch.Tensor, int]: A tuple containing the KL divergence cost and the total number of parameters in the layer.</p> Source code in <code>illia/nn/torch/embedding.py</code> <pre><code>@torch.jit.export\ndef kl_cost(self) -&gt; tuple[torch.Tensor, int]:\n    \"\"\"\n    Compute the KL divergence cost for all Bayesian parameters.\n\n    Returns:\n        tuple[torch.Tensor, int]: A tuple containing the KL\n            divergence cost and the total number of parameters in\n            the layer.\n    \"\"\"\n\n    # get log posterior and log prior\n    log_probs: torch.Tensor = self.weights_distribution.log_prob(self.weights)\n\n    # get number of parameters\n    num_params: int = self.weights_distribution.num_params()\n\n    return log_probs, num_params\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/nn.html#illia.nn.torch.linear.Linear","title":"<code>Linear</code>","text":"<p>This class is the bayesian implementation of the torch Linear layer.</p> Source code in <code>illia/nn/torch/linear.py</code> <pre><code>class Linear(BayesianModule):\n    \"\"\"\n    This class is the bayesian implementation of the torch Linear layer.\n    \"\"\"\n\n    weights: torch.Tensor\n    bias: torch.Tensor\n\n    def __init__(\n        self,\n        input_size: int,\n        output_size: int,\n        weights_distribution: Optional[GaussianDistribution] = None,\n        bias_distribution: Optional[GaussianDistribution] = None,\n        use_bias: bool = True,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes a Linear layer.\n\n        Args:\n            input_size: Input size of the linear layer.\n            output_size: Output size of the linear layer.\n            weights_distribution: GaussianDistribution for the weights of the\n                layer. Defaults to None.\n            bias_distribution: GaussianDistribution for the bias of the layer.\n                Defaults to None.\n            use_bias: Whether to include a bias term.\n            **kwargs: Extra arguments passed to the base class.\n\n        Returns:\n            None.\n\n        Notes:\n            Gaussian distributions are used by default if none are\n            provided.\n        \"\"\"\n\n        # Call super-class constructor\n        super().__init__(**kwargs)\n\n        # Set attributes\n        self.input_size = input_size\n        self.output_size = output_size\n        self.use_bias = use_bias\n\n        # Set weights distribution\n        if weights_distribution is None:\n            self.weights_distribution = GaussianDistribution(\n                (self.output_size, self.input_size)\n            )\n        else:\n            self.weights_distribution = weights_distribution\n\n        # Set bias distribution\n        if self.use_bias:\n            if bias_distribution is None:\n                self.bias_distribution = GaussianDistribution((self.output_size,))\n            else:\n                self.bias_distribution = bias_distribution\n        else:\n            self.bias_distribution = None  # type: ignore\n\n        # Sample initial weights\n        weights = self.weights_distribution.sample()\n\n        # Register buffers\n        self.register_buffer(\"weights\", weights)\n\n        if self.use_bias and self.bias_distribution is not None:\n            bias = self.bias_distribution.sample()\n            self.register_buffer(\"bias\", bias)\n        else:\n            self.bias = None  # type: ignore\n\n    @torch.jit.export\n    def freeze(self) -&gt; None:\n        \"\"\"\n        Freeze the module's parameters to stop gradient computation.\n        If weights or biases are not sampled yet, they are sampled first.\n        Once frozen, parameters are not resampled or updated.\n\n        Returns:\n            None.\n        \"\"\"\n\n        # Set indicator\n        self.frozen = True\n\n        # Sample weights if they are undefined\n        if self.weights is None:\n            self.weights = self.weights_distribution.sample()\n\n        # Sample bias if they are undefined and bias is used\n        if self.use_bias and self.bias_distribution is not None:\n            if not hasattr(self, \"bias\") or self.bias is None:\n                self.bias = self.bias_distribution.sample()\n            self.bias = self.bias.detach()\n\n        # Detach weights and bias\n        self.weights = self.weights.detach()\n\n    @torch.jit.export\n    def kl_cost(self) -&gt; tuple[torch.Tensor, int]:\n        \"\"\"\n        Compute the KL divergence cost for all Bayesian parameters.\n\n        Returns:\n            tuple[torch.Tensor, int]: A tuple containing the KL\n                divergence cost and the total number of parameters in\n                the layer.\n        \"\"\"\n\n        # Compute log probs\n        log_probs: torch.Tensor = self.weights_distribution.log_prob(self.weights)\n\n        # Add bias log probs if bias is used\n        if self.use_bias and self.bias_distribution is not None:\n            log_probs += self.bias_distribution.log_prob(self.bias)\n\n        # Compute number of parameters\n        num_params: int = self.weights_distribution.num_params()\n        if self.use_bias and self.bias_distribution is not None:\n            num_params += self.bias_distribution.num_params()\n\n        return log_probs, num_params\n\n    def forward(self, inputs: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        This method is the forward pass of the layer.\n\n        Args:\n            inputs: input tensor. Dimensions: [batch, *].\n\n        Returns:\n            outputs tensor. Dimensions: [batch, *].\n\n        Raises:\n            ValueError: If the layer is frozen but weights or bias are\n                undefined.\n        \"\"\"\n\n        # Check if layer is frozen\n        if not self.frozen:\n            self.weights = self.weights_distribution.sample()\n\n            # Sample bias only if using bias\n            if self.use_bias and self.bias_distribution is not None:\n                self.bias = self.bias_distribution.sample()\n        elif self.weights is None or (self.use_bias and self.bias is None):\n            raise ValueError(\n                \"Module has been frozen with undefined weights and/or bias.\"\n            )\n\n        # Compute outputs\n        # pylint: disable=E1102\n        outputs: torch.Tensor = F.linear(input=inputs, weight=self.weights)\n\n        # Add bias only if using bias\n        if self.use_bias and self.bias is not None:\n            outputs += torch.reshape(input=self.bias, shape=(1, self.output_size))\n\n        return outputs\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/nn.html#illia.nn.torch.linear.Linear.__init__","title":"<code>__init__(input_size, output_size, weights_distribution=None, bias_distribution=None, use_bias=True, **kwargs)</code>","text":"<p>Initializes a Linear layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>int</code> <p>Input size of the linear layer.</p> required <code>output_size</code> <code>int</code> <p>Output size of the linear layer.</p> required <code>weights_distribution</code> <code>Optional[GaussianDistribution]</code> <p>GaussianDistribution for the weights of the layer. Defaults to None.</p> <code>None</code> <code>bias_distribution</code> <code>Optional[GaussianDistribution]</code> <p>GaussianDistribution for the bias of the layer. Defaults to None.</p> <code>None</code> <code>use_bias</code> <code>bool</code> <p>Whether to include a bias term.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Extra arguments passed to the base class.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Notes <p>Gaussian distributions are used by default if none are provided.</p> Source code in <code>illia/nn/torch/linear.py</code> <pre><code>def __init__(\n    self,\n    input_size: int,\n    output_size: int,\n    weights_distribution: Optional[GaussianDistribution] = None,\n    bias_distribution: Optional[GaussianDistribution] = None,\n    use_bias: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes a Linear layer.\n\n    Args:\n        input_size: Input size of the linear layer.\n        output_size: Output size of the linear layer.\n        weights_distribution: GaussianDistribution for the weights of the\n            layer. Defaults to None.\n        bias_distribution: GaussianDistribution for the bias of the layer.\n            Defaults to None.\n        use_bias: Whether to include a bias term.\n        **kwargs: Extra arguments passed to the base class.\n\n    Returns:\n        None.\n\n    Notes:\n        Gaussian distributions are used by default if none are\n        provided.\n    \"\"\"\n\n    # Call super-class constructor\n    super().__init__(**kwargs)\n\n    # Set attributes\n    self.input_size = input_size\n    self.output_size = output_size\n    self.use_bias = use_bias\n\n    # Set weights distribution\n    if weights_distribution is None:\n        self.weights_distribution = GaussianDistribution(\n            (self.output_size, self.input_size)\n        )\n    else:\n        self.weights_distribution = weights_distribution\n\n    # Set bias distribution\n    if self.use_bias:\n        if bias_distribution is None:\n            self.bias_distribution = GaussianDistribution((self.output_size,))\n        else:\n            self.bias_distribution = bias_distribution\n    else:\n        self.bias_distribution = None  # type: ignore\n\n    # Sample initial weights\n    weights = self.weights_distribution.sample()\n\n    # Register buffers\n    self.register_buffer(\"weights\", weights)\n\n    if self.use_bias and self.bias_distribution is not None:\n        bias = self.bias_distribution.sample()\n        self.register_buffer(\"bias\", bias)\n    else:\n        self.bias = None  # type: ignore\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/nn.html#illia.nn.torch.linear.Linear.forward","title":"<code>forward(inputs)</code>","text":"<p>This method is the forward pass of the layer.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Tensor</code> <p>input tensor. Dimensions: [batch, *].</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>outputs tensor. Dimensions: [batch, *].</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the layer is frozen but weights or bias are undefined.</p> Source code in <code>illia/nn/torch/linear.py</code> <pre><code>def forward(self, inputs: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    This method is the forward pass of the layer.\n\n    Args:\n        inputs: input tensor. Dimensions: [batch, *].\n\n    Returns:\n        outputs tensor. Dimensions: [batch, *].\n\n    Raises:\n        ValueError: If the layer is frozen but weights or bias are\n            undefined.\n    \"\"\"\n\n    # Check if layer is frozen\n    if not self.frozen:\n        self.weights = self.weights_distribution.sample()\n\n        # Sample bias only if using bias\n        if self.use_bias and self.bias_distribution is not None:\n            self.bias = self.bias_distribution.sample()\n    elif self.weights is None or (self.use_bias and self.bias is None):\n        raise ValueError(\n            \"Module has been frozen with undefined weights and/or bias.\"\n        )\n\n    # Compute outputs\n    # pylint: disable=E1102\n    outputs: torch.Tensor = F.linear(input=inputs, weight=self.weights)\n\n    # Add bias only if using bias\n    if self.use_bias and self.bias is not None:\n        outputs += torch.reshape(input=self.bias, shape=(1, self.output_size))\n\n    return outputs\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/nn.html#illia.nn.torch.linear.Linear.freeze","title":"<code>freeze()</code>","text":"<p>Freeze the module's parameters to stop gradient computation. If weights or biases are not sampled yet, they are sampled first. Once frozen, parameters are not resampled or updated.</p> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>illia/nn/torch/linear.py</code> <pre><code>@torch.jit.export\ndef freeze(self) -&gt; None:\n    \"\"\"\n    Freeze the module's parameters to stop gradient computation.\n    If weights or biases are not sampled yet, they are sampled first.\n    Once frozen, parameters are not resampled or updated.\n\n    Returns:\n        None.\n    \"\"\"\n\n    # Set indicator\n    self.frozen = True\n\n    # Sample weights if they are undefined\n    if self.weights is None:\n        self.weights = self.weights_distribution.sample()\n\n    # Sample bias if they are undefined and bias is used\n    if self.use_bias and self.bias_distribution is not None:\n        if not hasattr(self, \"bias\") or self.bias is None:\n            self.bias = self.bias_distribution.sample()\n        self.bias = self.bias.detach()\n\n    # Detach weights and bias\n    self.weights = self.weights.detach()\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/nn.html#illia.nn.torch.linear.Linear.kl_cost","title":"<code>kl_cost()</code>","text":"<p>Compute the KL divergence cost for all Bayesian parameters.</p> <p>Returns:</p> Type Description <code>tuple[Tensor, int]</code> <p>tuple[torch.Tensor, int]: A tuple containing the KL divergence cost and the total number of parameters in the layer.</p> Source code in <code>illia/nn/torch/linear.py</code> <pre><code>@torch.jit.export\ndef kl_cost(self) -&gt; tuple[torch.Tensor, int]:\n    \"\"\"\n    Compute the KL divergence cost for all Bayesian parameters.\n\n    Returns:\n        tuple[torch.Tensor, int]: A tuple containing the KL\n            divergence cost and the total number of parameters in\n            the layer.\n    \"\"\"\n\n    # Compute log probs\n    log_probs: torch.Tensor = self.weights_distribution.log_prob(self.weights)\n\n    # Add bias log probs if bias is used\n    if self.use_bias and self.bias_distribution is not None:\n        log_probs += self.bias_distribution.log_prob(self.bias)\n\n    # Compute number of parameters\n    num_params: int = self.weights_distribution.num_params()\n    if self.use_bias and self.bias_distribution is not None:\n        num_params += self.bias_distribution.num_params()\n\n    return log_probs, num_params\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/nn.html#illia.nn.torch.lstm.LSTM","title":"<code>LSTM</code>","text":"<p>Bayesian LSTM layer with embedding and probabilistic weights. All weights and biases are sampled from Gaussian distributions. Freezing the layer fixes parameters and stops gradient computation.</p> Source code in <code>illia/nn/torch/lstm.py</code> <pre><code>class LSTM(BayesianModule):\n    \"\"\"\n    Bayesian LSTM layer with embedding and probabilistic weights.\n    All weights and biases are sampled from Gaussian distributions.\n    Freezing the layer fixes parameters and stops gradient computation.\n    \"\"\"\n\n    # Forget gate\n    wf: torch.Tensor\n    bf: torch.Tensor\n\n    # Input gate\n    wi: torch.Tensor\n    bi: torch.Tensor\n\n    # Candidate gate\n    wc: torch.Tensor\n    bc: torch.Tensor\n\n    # Output gate\n    wo: torch.Tensor\n    bo: torch.Tensor\n\n    # Final output layer\n    wv: torch.Tensor\n    bv: torch.Tensor\n\n    def __init__(\n        self,\n        num_embeddings: int,\n        embeddings_dim: int,\n        hidden_size: int,\n        output_size: int,\n        padding_idx: Optional[int] = None,\n        max_norm: Optional[float] = None,\n        norm_type: float = 2.0,\n        scale_grad_by_freq: bool = False,\n        sparse: bool = False,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the Bayesian LSTM layer.\n\n        Args:\n            num_embeddings: Size of the embedding dictionary.\n            embeddings_dim: Dimensionality of each embedding vector.\n            hidden_size: Number of hidden units in the LSTM.\n            output_size: Size of the final output.\n            padding_idx: Index to ignore in embeddings.\n            max_norm: Maximum norm for embedding vectors.\n            norm_type: Norm type used for max_norm.\n            scale_grad_by_freq: Scale gradient by inverse frequency.\n            sparse: Use sparse embedding updates.\n            **kwargs: Extra arguments passed to the base class.\n\n        Returns:\n            None.\n\n        Notes:\n            Gaussian distributions are used by default if none are\n            provided.\n        \"\"\"\n\n        # Call super-class constructor\n        super().__init__(**kwargs)\n\n        # Set attributes\n        self.num_embeddings = num_embeddings\n        self.embeddings_dim = embeddings_dim\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.padding_idx = padding_idx\n        self.max_norm = max_norm\n        self.norm_type = norm_type\n        self.scale_grad_by_freq = scale_grad_by_freq\n        self.sparse = sparse\n\n        # Define the Embedding layer\n        self.embedding = Embedding(\n            num_embeddings=self.num_embeddings,\n            embeddings_dim=self.embeddings_dim,\n            padding_idx=self.padding_idx,\n            max_norm=self.max_norm,\n            norm_type=self.norm_type,\n            scale_grad_by_freq=self.scale_grad_by_freq,\n            sparse=self.sparse,\n        )\n\n        # Initialize weights\n        # Forget gate\n        self.wf_distribution = GaussianDistribution(\n            (self.hidden_size, self.embeddings_dim + self.hidden_size)\n        )\n        self.bf_distribution = GaussianDistribution((self.hidden_size,))\n\n        # Input gate\n        self.wi_distribution = GaussianDistribution(\n            (self.hidden_size, self.embeddings_dim + self.hidden_size)\n        )\n        self.bi_distribution = GaussianDistribution((self.hidden_size,))\n\n        # Candidate gate\n        self.wc_distribution = GaussianDistribution(\n            (self.hidden_size, self.embeddings_dim + self.hidden_size)\n        )\n        self.bc_distribution = GaussianDistribution((self.hidden_size,))\n\n        # Output gate\n        self.wo_distribution = GaussianDistribution(\n            (self.hidden_size, self.embeddings_dim + self.hidden_size)\n        )\n        self.bo_distribution = GaussianDistribution((self.hidden_size,))\n\n        # Final gate\n        self.wv_distribution = GaussianDistribution(\n            (self.output_size, self.hidden_size)\n        )\n        self.bv_distribution = GaussianDistribution((self.output_size,))\n\n        # Sample initial weights and register buffers\n        # Forget gate\n        wf = self.wf_distribution.sample()\n        bf = self.bf_distribution.sample()\n        self.register_buffer(\"wf\", wf)\n        self.register_buffer(\"bf\", bf)\n\n        # Input gate\n        wi = self.wi_distribution.sample()\n        bi = self.bi_distribution.sample()\n        self.register_buffer(\"wi\", wi)\n        self.register_buffer(\"bi\", bi)\n\n        # Candidate gate\n        wc = self.wc_distribution.sample()\n        bc = self.bc_distribution.sample()\n        self.register_buffer(\"wc\", wc)\n        self.register_buffer(\"bc\", bc)\n\n        # Output gate\n        wo = self.wo_distribution.sample()\n        bo = self.bo_distribution.sample()\n        self.register_buffer(\"wo\", wo)\n        self.register_buffer(\"bo\", bo)\n\n        # Final output layer\n        wv = self.wv_distribution.sample()\n        bv = self.bv_distribution.sample()\n        self.register_buffer(\"wv\", wv)\n        self.register_buffer(\"bv\", bv)\n\n    @torch.jit.export\n    def freeze(self) -&gt; None:\n        \"\"\"\n        Freeze the module's parameters to stop gradient computation.\n        If weights or biases are not sampled yet, they are sampled first.\n        Once frozen, parameters are not resampled or updated.\n\n        Returns:\n            None.\n        \"\"\"\n\n        # Set indicator\n        self.frozen = True\n\n        # Freeze embedding layer\n        self.embedding.freeze()\n\n        # Forget gate\n        if self.wf is None:\n            self.wf = self.wf_distribution.sample()\n        if self.bf is None:\n            self.bf = self.bf_distribution.sample()\n        self.wf = self.wf.detach()\n        self.bf = self.bf.detach()\n\n        # Input gate\n        if self.wi is None:\n            self.wi = self.wi_distribution.sample()\n        if self.bi is None:\n            self.bi = self.bi_distribution.sample()\n        self.wi = self.wi.detach()\n        self.bi = self.bi.detach()\n\n        # Candidate gate\n        if self.wc is None:\n            self.wc = self.wc_distribution.sample()\n        if self.bc is None:\n            self.bc = self.bc_distribution.sample()\n        self.wc = self.wc.detach()\n        self.bc = self.bc.detach()\n\n        # Output gate\n        if self.wo is None:\n            self.wo = self.wo_distribution.sample()\n        if self.bo is None:\n            self.bo = self.bo_distribution.sample()\n        self.wo = self.wo.detach()\n        self.bo = self.bo.detach()\n\n        # Final output layer\n        if self.wv is None:\n            self.wv = self.wv_distribution.sample()\n        if self.bv is None:\n            self.bv = self.bv_distribution.sample()\n        self.wv = self.wv.detach()\n        self.bv = self.bv.detach()\n\n    @torch.jit.export\n    def kl_cost(self) -&gt; tuple[torch.Tensor, int]:\n        \"\"\"\n        Compute the KL divergence cost for all Bayesian parameters.\n\n        Returns:\n            tuple[torch.Tensor, int]: A tuple containing the KL\n                divergence cost and the total number of parameters in\n                the layer.\n        \"\"\"\n\n        # Compute log probs for each pair of weights and bias\n        # Forget gate\n        log_probs_f: torch.Tensor = self.wf_distribution.log_prob(\n            self.wf\n        ) + self.bf_distribution.log_prob(self.bf)\n        # Input gate\n        log_probs_i: torch.Tensor = self.wi_distribution.log_prob(\n            self.wi\n        ) + self.bi_distribution.log_prob(self.bi)\n        # Candidate gate\n        log_probs_c: torch.Tensor = self.wc_distribution.log_prob(\n            self.wc\n        ) + self.bc_distribution.log_prob(self.bc)\n        # Output gate\n        log_probs_o: torch.Tensor = self.wo_distribution.log_prob(\n            self.wo\n        ) + self.bo_distribution.log_prob(self.bo)\n        # Final output layer\n        log_probs_v: torch.Tensor = self.wv_distribution.log_prob(\n            self.wv\n        ) + self.bv_distribution.log_prob(self.bv)\n\n        # Compute the total loss\n        log_probs = log_probs_f + log_probs_i + log_probs_c + log_probs_o + log_probs_v\n\n        # Compute number of parameters\n        num_params: int = (\n            self.wf_distribution.num_params()\n            + self.bf_distribution.num_params()\n            + self.wi_distribution.num_params()\n            + self.bi_distribution.num_params()\n            + self.wc_distribution.num_params()\n            + self.bc_distribution.num_params()\n            + self.wo_distribution.num_params()\n            + self.bo_distribution.num_params()\n            + self.wv_distribution.num_params()\n            + self.bv_distribution.num_params()\n        )\n\n        return log_probs, num_params\n\n    def forward(\n        self,\n        inputs: torch.Tensor,\n        init_states: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n    ) -&gt; tuple[torch.Tensor, tuple[torch.Tensor, torch.Tensor]]:\n        \"\"\"\n        Performs a forward pass through the Bayesian LSTM layer.\n        If the layer is not frozen, it samples weights and bias\n        from their respective distributions. If the layer is frozen\n        and the weights or bias are not initialized, it also performs\n        sampling.\n\n        Args:\n            inputs: Input tensor to the layer with shape [batch,\n                input channels, input width, input height].\n\n        Returns:\n            Output tensor after passing through the layer with shape\n                [batch, output channels, output width, output height].\n\n        Raises:\n            ValueError: If the layer is frozen but weights are\n                undefined.\n        \"\"\"\n\n        # Sample weights if not frozen\n        if not self.frozen:\n            self.wf = self.wf_distribution.sample()\n            self.bf = self.bf_distribution.sample()\n            self.wi = self.wi_distribution.sample()\n            self.bi = self.bi_distribution.sample()\n            self.wc = self.wc_distribution.sample()\n            self.bc = self.bc_distribution.sample()\n            self.wo = self.wo_distribution.sample()\n            self.bo = self.bo_distribution.sample()\n            self.wv = self.wv_distribution.sample()\n            self.bv = self.bv_distribution.sample()\n        elif any(\n            p is None\n            for p in [\n                self.wf,\n                self.bf,\n                self.wi,\n                self.bi,\n                self.wc,\n                self.bc,\n                self.wo,\n                self.bo,\n                self.wv,\n                self.bv,\n            ]\n        ):\n            raise ValueError(\n                \"Module has been frozen with undefined weights and/or bias.\"\n            )\n\n        # Apply embedding layer to input indices\n        inputs = inputs.squeeze(dim=-1)\n        inputs = self.embedding(inputs)\n        batch_size, seq_len, _ = inputs.size()\n\n        # Initialize h_t and c_t if init_states is None\n        if init_states is None:\n            device = inputs.device\n            h_t = torch.zeros(batch_size, self.hidden_size, device=device)\n            c_t = torch.zeros(batch_size, self.hidden_size, device=device)\n        else:\n            h_t, c_t = init_states[0], init_states[1]\n\n        for t in range(seq_len):\n            # Shape: (batch_size, embedding_dim)\n            x_t = inputs[:, t, :]\n\n            # Concatenate input and hidden state\n            # Shape: (batch_size, embedding_dim + hidden_size)\n            z_t = torch.cat([x_t, h_t], dim=1)\n\n            # Forget gate\n            ft = torch.sigmoid(z_t @ self.wf.t() + self.bf)\n\n            # Input gate\n            it = torch.sigmoid(z_t @ self.wi.t() + self.bi)\n\n            # Candidate cell state\n            can = torch.tanh(z_t @ self.wc.t() + self.bc)\n\n            # Output gate\n            ot = torch.sigmoid(z_t @ self.wo.t() + self.bo)\n\n            # Update cell state\n            c_t = c_t * ft + can * it\n\n            # Update hidden state\n            h_t = ot * torch.tanh(c_t)\n\n        # Compute final output\n        y_t = h_t @ self.wv.t() + self.bv\n\n        return y_t, (h_t, c_t)\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/nn.html#illia.nn.torch.lstm.LSTM.__init__","title":"<code>__init__(num_embeddings, embeddings_dim, hidden_size, output_size, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, **kwargs)</code>","text":"<p>Initializes the Bayesian LSTM layer.</p> <p>Parameters:</p> Name Type Description Default <code>num_embeddings</code> <code>int</code> <p>Size of the embedding dictionary.</p> required <code>embeddings_dim</code> <code>int</code> <p>Dimensionality of each embedding vector.</p> required <code>hidden_size</code> <code>int</code> <p>Number of hidden units in the LSTM.</p> required <code>output_size</code> <code>int</code> <p>Size of the final output.</p> required <code>padding_idx</code> <code>Optional[int]</code> <p>Index to ignore in embeddings.</p> <code>None</code> <code>max_norm</code> <code>Optional[float]</code> <p>Maximum norm for embedding vectors.</p> <code>None</code> <code>norm_type</code> <code>float</code> <p>Norm type used for max_norm.</p> <code>2.0</code> <code>scale_grad_by_freq</code> <code>bool</code> <p>Scale gradient by inverse frequency.</p> <code>False</code> <code>sparse</code> <code>bool</code> <p>Use sparse embedding updates.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Extra arguments passed to the base class.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Notes <p>Gaussian distributions are used by default if none are provided.</p> Source code in <code>illia/nn/torch/lstm.py</code> <pre><code>def __init__(\n    self,\n    num_embeddings: int,\n    embeddings_dim: int,\n    hidden_size: int,\n    output_size: int,\n    padding_idx: Optional[int] = None,\n    max_norm: Optional[float] = None,\n    norm_type: float = 2.0,\n    scale_grad_by_freq: bool = False,\n    sparse: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the Bayesian LSTM layer.\n\n    Args:\n        num_embeddings: Size of the embedding dictionary.\n        embeddings_dim: Dimensionality of each embedding vector.\n        hidden_size: Number of hidden units in the LSTM.\n        output_size: Size of the final output.\n        padding_idx: Index to ignore in embeddings.\n        max_norm: Maximum norm for embedding vectors.\n        norm_type: Norm type used for max_norm.\n        scale_grad_by_freq: Scale gradient by inverse frequency.\n        sparse: Use sparse embedding updates.\n        **kwargs: Extra arguments passed to the base class.\n\n    Returns:\n        None.\n\n    Notes:\n        Gaussian distributions are used by default if none are\n        provided.\n    \"\"\"\n\n    # Call super-class constructor\n    super().__init__(**kwargs)\n\n    # Set attributes\n    self.num_embeddings = num_embeddings\n    self.embeddings_dim = embeddings_dim\n    self.hidden_size = hidden_size\n    self.output_size = output_size\n    self.padding_idx = padding_idx\n    self.max_norm = max_norm\n    self.norm_type = norm_type\n    self.scale_grad_by_freq = scale_grad_by_freq\n    self.sparse = sparse\n\n    # Define the Embedding layer\n    self.embedding = Embedding(\n        num_embeddings=self.num_embeddings,\n        embeddings_dim=self.embeddings_dim,\n        padding_idx=self.padding_idx,\n        max_norm=self.max_norm,\n        norm_type=self.norm_type,\n        scale_grad_by_freq=self.scale_grad_by_freq,\n        sparse=self.sparse,\n    )\n\n    # Initialize weights\n    # Forget gate\n    self.wf_distribution = GaussianDistribution(\n        (self.hidden_size, self.embeddings_dim + self.hidden_size)\n    )\n    self.bf_distribution = GaussianDistribution((self.hidden_size,))\n\n    # Input gate\n    self.wi_distribution = GaussianDistribution(\n        (self.hidden_size, self.embeddings_dim + self.hidden_size)\n    )\n    self.bi_distribution = GaussianDistribution((self.hidden_size,))\n\n    # Candidate gate\n    self.wc_distribution = GaussianDistribution(\n        (self.hidden_size, self.embeddings_dim + self.hidden_size)\n    )\n    self.bc_distribution = GaussianDistribution((self.hidden_size,))\n\n    # Output gate\n    self.wo_distribution = GaussianDistribution(\n        (self.hidden_size, self.embeddings_dim + self.hidden_size)\n    )\n    self.bo_distribution = GaussianDistribution((self.hidden_size,))\n\n    # Final gate\n    self.wv_distribution = GaussianDistribution(\n        (self.output_size, self.hidden_size)\n    )\n    self.bv_distribution = GaussianDistribution((self.output_size,))\n\n    # Sample initial weights and register buffers\n    # Forget gate\n    wf = self.wf_distribution.sample()\n    bf = self.bf_distribution.sample()\n    self.register_buffer(\"wf\", wf)\n    self.register_buffer(\"bf\", bf)\n\n    # Input gate\n    wi = self.wi_distribution.sample()\n    bi = self.bi_distribution.sample()\n    self.register_buffer(\"wi\", wi)\n    self.register_buffer(\"bi\", bi)\n\n    # Candidate gate\n    wc = self.wc_distribution.sample()\n    bc = self.bc_distribution.sample()\n    self.register_buffer(\"wc\", wc)\n    self.register_buffer(\"bc\", bc)\n\n    # Output gate\n    wo = self.wo_distribution.sample()\n    bo = self.bo_distribution.sample()\n    self.register_buffer(\"wo\", wo)\n    self.register_buffer(\"bo\", bo)\n\n    # Final output layer\n    wv = self.wv_distribution.sample()\n    bv = self.bv_distribution.sample()\n    self.register_buffer(\"wv\", wv)\n    self.register_buffer(\"bv\", bv)\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/nn.html#illia.nn.torch.lstm.LSTM.forward","title":"<code>forward(inputs, init_states=None)</code>","text":"<p>Performs a forward pass through the Bayesian LSTM layer. If the layer is not frozen, it samples weights and bias from their respective distributions. If the layer is frozen and the weights or bias are not initialized, it also performs sampling.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Tensor</code> <p>Input tensor to the layer with shape [batch, input channels, input width, input height].</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, tuple[Tensor, Tensor]]</code> <p>Output tensor after passing through the layer with shape [batch, output channels, output width, output height].</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the layer is frozen but weights are undefined.</p> Source code in <code>illia/nn/torch/lstm.py</code> <pre><code>def forward(\n    self,\n    inputs: torch.Tensor,\n    init_states: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n) -&gt; tuple[torch.Tensor, tuple[torch.Tensor, torch.Tensor]]:\n    \"\"\"\n    Performs a forward pass through the Bayesian LSTM layer.\n    If the layer is not frozen, it samples weights and bias\n    from their respective distributions. If the layer is frozen\n    and the weights or bias are not initialized, it also performs\n    sampling.\n\n    Args:\n        inputs: Input tensor to the layer with shape [batch,\n            input channels, input width, input height].\n\n    Returns:\n        Output tensor after passing through the layer with shape\n            [batch, output channels, output width, output height].\n\n    Raises:\n        ValueError: If the layer is frozen but weights are\n            undefined.\n    \"\"\"\n\n    # Sample weights if not frozen\n    if not self.frozen:\n        self.wf = self.wf_distribution.sample()\n        self.bf = self.bf_distribution.sample()\n        self.wi = self.wi_distribution.sample()\n        self.bi = self.bi_distribution.sample()\n        self.wc = self.wc_distribution.sample()\n        self.bc = self.bc_distribution.sample()\n        self.wo = self.wo_distribution.sample()\n        self.bo = self.bo_distribution.sample()\n        self.wv = self.wv_distribution.sample()\n        self.bv = self.bv_distribution.sample()\n    elif any(\n        p is None\n        for p in [\n            self.wf,\n            self.bf,\n            self.wi,\n            self.bi,\n            self.wc,\n            self.bc,\n            self.wo,\n            self.bo,\n            self.wv,\n            self.bv,\n        ]\n    ):\n        raise ValueError(\n            \"Module has been frozen with undefined weights and/or bias.\"\n        )\n\n    # Apply embedding layer to input indices\n    inputs = inputs.squeeze(dim=-1)\n    inputs = self.embedding(inputs)\n    batch_size, seq_len, _ = inputs.size()\n\n    # Initialize h_t and c_t if init_states is None\n    if init_states is None:\n        device = inputs.device\n        h_t = torch.zeros(batch_size, self.hidden_size, device=device)\n        c_t = torch.zeros(batch_size, self.hidden_size, device=device)\n    else:\n        h_t, c_t = init_states[0], init_states[1]\n\n    for t in range(seq_len):\n        # Shape: (batch_size, embedding_dim)\n        x_t = inputs[:, t, :]\n\n        # Concatenate input and hidden state\n        # Shape: (batch_size, embedding_dim + hidden_size)\n        z_t = torch.cat([x_t, h_t], dim=1)\n\n        # Forget gate\n        ft = torch.sigmoid(z_t @ self.wf.t() + self.bf)\n\n        # Input gate\n        it = torch.sigmoid(z_t @ self.wi.t() + self.bi)\n\n        # Candidate cell state\n        can = torch.tanh(z_t @ self.wc.t() + self.bc)\n\n        # Output gate\n        ot = torch.sigmoid(z_t @ self.wo.t() + self.bo)\n\n        # Update cell state\n        c_t = c_t * ft + can * it\n\n        # Update hidden state\n        h_t = ot * torch.tanh(c_t)\n\n    # Compute final output\n    y_t = h_t @ self.wv.t() + self.bv\n\n    return y_t, (h_t, c_t)\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/nn.html#illia.nn.torch.lstm.LSTM.freeze","title":"<code>freeze()</code>","text":"<p>Freeze the module's parameters to stop gradient computation. If weights or biases are not sampled yet, they are sampled first. Once frozen, parameters are not resampled or updated.</p> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>illia/nn/torch/lstm.py</code> <pre><code>@torch.jit.export\ndef freeze(self) -&gt; None:\n    \"\"\"\n    Freeze the module's parameters to stop gradient computation.\n    If weights or biases are not sampled yet, they are sampled first.\n    Once frozen, parameters are not resampled or updated.\n\n    Returns:\n        None.\n    \"\"\"\n\n    # Set indicator\n    self.frozen = True\n\n    # Freeze embedding layer\n    self.embedding.freeze()\n\n    # Forget gate\n    if self.wf is None:\n        self.wf = self.wf_distribution.sample()\n    if self.bf is None:\n        self.bf = self.bf_distribution.sample()\n    self.wf = self.wf.detach()\n    self.bf = self.bf.detach()\n\n    # Input gate\n    if self.wi is None:\n        self.wi = self.wi_distribution.sample()\n    if self.bi is None:\n        self.bi = self.bi_distribution.sample()\n    self.wi = self.wi.detach()\n    self.bi = self.bi.detach()\n\n    # Candidate gate\n    if self.wc is None:\n        self.wc = self.wc_distribution.sample()\n    if self.bc is None:\n        self.bc = self.bc_distribution.sample()\n    self.wc = self.wc.detach()\n    self.bc = self.bc.detach()\n\n    # Output gate\n    if self.wo is None:\n        self.wo = self.wo_distribution.sample()\n    if self.bo is None:\n        self.bo = self.bo_distribution.sample()\n    self.wo = self.wo.detach()\n    self.bo = self.bo.detach()\n\n    # Final output layer\n    if self.wv is None:\n        self.wv = self.wv_distribution.sample()\n    if self.bv is None:\n        self.bv = self.bv_distribution.sample()\n    self.wv = self.wv.detach()\n    self.bv = self.bv.detach()\n</code></pre>"},{"location":"api/Deep%20Learning/PyTorch/nn.html#illia.nn.torch.lstm.LSTM.kl_cost","title":"<code>kl_cost()</code>","text":"<p>Compute the KL divergence cost for all Bayesian parameters.</p> <p>Returns:</p> Type Description <code>tuple[Tensor, int]</code> <p>tuple[torch.Tensor, int]: A tuple containing the KL divergence cost and the total number of parameters in the layer.</p> Source code in <code>illia/nn/torch/lstm.py</code> <pre><code>@torch.jit.export\ndef kl_cost(self) -&gt; tuple[torch.Tensor, int]:\n    \"\"\"\n    Compute the KL divergence cost for all Bayesian parameters.\n\n    Returns:\n        tuple[torch.Tensor, int]: A tuple containing the KL\n            divergence cost and the total number of parameters in\n            the layer.\n    \"\"\"\n\n    # Compute log probs for each pair of weights and bias\n    # Forget gate\n    log_probs_f: torch.Tensor = self.wf_distribution.log_prob(\n        self.wf\n    ) + self.bf_distribution.log_prob(self.bf)\n    # Input gate\n    log_probs_i: torch.Tensor = self.wi_distribution.log_prob(\n        self.wi\n    ) + self.bi_distribution.log_prob(self.bi)\n    # Candidate gate\n    log_probs_c: torch.Tensor = self.wc_distribution.log_prob(\n        self.wc\n    ) + self.bc_distribution.log_prob(self.bc)\n    # Output gate\n    log_probs_o: torch.Tensor = self.wo_distribution.log_prob(\n        self.wo\n    ) + self.bo_distribution.log_prob(self.bo)\n    # Final output layer\n    log_probs_v: torch.Tensor = self.wv_distribution.log_prob(\n        self.wv\n    ) + self.bv_distribution.log_prob(self.bv)\n\n    # Compute the total loss\n    log_probs = log_probs_f + log_probs_i + log_probs_c + log_probs_o + log_probs_v\n\n    # Compute number of parameters\n    num_params: int = (\n        self.wf_distribution.num_params()\n        + self.bf_distribution.num_params()\n        + self.wi_distribution.num_params()\n        + self.bi_distribution.num_params()\n        + self.wc_distribution.num_params()\n        + self.bc_distribution.num_params()\n        + self.wo_distribution.num_params()\n        + self.bo_distribution.num_params()\n        + self.wv_distribution.num_params()\n        + self.bv_distribution.num_params()\n    )\n\n    return log_probs, num_params\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/distributions.html","title":"Distributions","text":""},{"location":"api/Deep%20Learning/Tensorflow/distributions.html#illia.distributions.tf.base.DistributionModule","title":"<code>DistributionModule</code>","text":"<p>Abstract base for probabilistic distribution modules in Tensorflow. Defines the required interface for sampling, computing log-probabilities, and counting learnable parameters.</p> Notes <p>This class is abstract and cannot be instantiated directly. All abstract methods must be implemented by subclasses.</p> Source code in <code>illia/distributions/tf/base.py</code> <pre><code>@saving.register_keras_serializable(package=\"illia\", name=\"DistributionModule\")\nclass DistributionModule(layers.Layer, ABC):\n    \"\"\"\n    Abstract base for probabilistic distribution modules in Tensorflow.\n    Defines the required interface for sampling, computing\n    log-probabilities, and counting learnable parameters.\n\n    Notes:\n        This class is abstract and cannot be instantiated directly.\n        All abstract methods must be implemented by subclasses.\n    \"\"\"\n\n    @abstractmethod\n    def sample(self) -&gt; tf.Tensor:\n        \"\"\"\n        Draw a sample from the distribution.\n\n        Returns:\n            tf.Tensor: A sample drawn from the distribution.\n        \"\"\"\n\n    @abstractmethod\n    def log_prob(self, x: Optional[tf.Tensor] = None) -&gt; tf.Tensor:\n        \"\"\"\n        Compute the log-probability of a provided sample. If no\n        sample is passed, one is drawn internally.\n\n        Args:\n            x: Optional sample to evaluate. If None, a new sample is\n                drawn from the distribution.\n\n        Returns:\n            tf.Tensor: Scalar log-probability value.\n\n        Notes:\n            Works with both user-supplied and internally drawn\n            samples.\n        \"\"\"\n\n    @property\n    @abstractmethod\n    def num_params(self) -&gt; int:\n        \"\"\"\n        Return the number of learnable parameters in the\n        distribution.\n\n        Returns:\n            int: Total count of learnable parameters.\n        \"\"\"\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/distributions.html#illia.distributions.tf.base.DistributionModule.num_params","title":"<code>num_params</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Return the number of learnable parameters in the distribution.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Total count of learnable parameters.</p>"},{"location":"api/Deep%20Learning/Tensorflow/distributions.html#illia.distributions.tf.base.DistributionModule.log_prob","title":"<code>log_prob(x=None)</code>  <code>abstractmethod</code>","text":"<p>Compute the log-probability of a provided sample. If no sample is passed, one is drawn internally.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Optional[Tensor]</code> <p>Optional sample to evaluate. If None, a new sample is drawn from the distribution.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>tf.Tensor: Scalar log-probability value.</p> Notes <p>Works with both user-supplied and internally drawn samples.</p> Source code in <code>illia/distributions/tf/base.py</code> <pre><code>@abstractmethod\ndef log_prob(self, x: Optional[tf.Tensor] = None) -&gt; tf.Tensor:\n    \"\"\"\n    Compute the log-probability of a provided sample. If no\n    sample is passed, one is drawn internally.\n\n    Args:\n        x: Optional sample to evaluate. If None, a new sample is\n            drawn from the distribution.\n\n    Returns:\n        tf.Tensor: Scalar log-probability value.\n\n    Notes:\n        Works with both user-supplied and internally drawn\n        samples.\n    \"\"\"\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/distributions.html#illia.distributions.tf.base.DistributionModule.sample","title":"<code>sample()</code>  <code>abstractmethod</code>","text":"<p>Draw a sample from the distribution.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>tf.Tensor: A sample drawn from the distribution.</p> Source code in <code>illia/distributions/tf/base.py</code> <pre><code>@abstractmethod\ndef sample(self) -&gt; tf.Tensor:\n    \"\"\"\n    Draw a sample from the distribution.\n\n    Returns:\n        tf.Tensor: A sample drawn from the distribution.\n    \"\"\"\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/distributions.html#illia.distributions.tf.gaussian.GaussianDistribution","title":"<code>GaussianDistribution</code>","text":"<p>Learnable Gaussian distribution with diagonal covariance. Represents a Gaussian with trainable mean and standard deviation. The standard deviation is derived from <code>rho</code> using a softplus transformation to ensure positivity.</p> Notes <p>Assumes diagonal covariance. KL divergence can be estimated via log-probability differences from <code>log_prob</code>.</p> Source code in <code>illia/distributions/tf/gaussian.py</code> <pre><code>@saving.register_keras_serializable(package=\"illia\", name=\"GaussianDistribution\")\nclass GaussianDistribution(DistributionModule):\n    \"\"\"\n    Learnable Gaussian distribution with diagonal covariance.\n    Represents a Gaussian with trainable mean and standard\n    deviation. The standard deviation is derived from `rho`\n    using a softplus transformation to ensure positivity.\n\n    Notes:\n        Assumes diagonal covariance. KL divergence can be\n        estimated via log-probability differences from\n        `log_prob`.\n    \"\"\"\n\n    def __init__(\n        self,\n        shape: tuple[int, ...],\n        mu_prior: float = 0.0,\n        std_prior: float = 0.1,\n        mu_init: float = 0.0,\n        rho_init: float = -7.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initialize a learnable Gaussian distribution layer.\n\n        Args:\n            shape: Shape of the learnable parameters.\n            mu_prior: Mean of the Gaussian prior.\n            std_prior: Standard deviation of the prior.\n            mu_init: Initial value for the learnable mean.\n            rho_init: Initial value for the learnable rho.\n            **kwargs: Extra arguments passed to the base class.\n\n        Returns:\n            None\n        \"\"\"\n\n        # Call super class constructor\n        super().__init__(**kwargs)\n\n        # Set attributes\n        self.shape = shape\n        self.mu_prior_value = mu_prior\n        self.std_prior_value = std_prior\n        self.mu_init = mu_init\n        self.rho_init = rho_init\n\n        # Call build method\n        self.build(self.shape)\n\n    def build(self, input_shape: tf.TensorShape) -&gt; None:\n        \"\"\"\n        Build trainable and non-trainable parameters.\n\n        Args:\n            input_shape: Input shape used to trigger layer build.\n\n        Returns:\n            None\n        \"\"\"\n\n        # Define non-trainable priors variables\n        self.mu_prior = self.add_weight(\n            shape=(),\n            initializer=tf.constant_initializer(self.mu_prior_value),\n            trainable=False,\n            name=\"mu_prior\",\n        )\n\n        self.std_prior = self.add_weight(\n            shape=(),\n            initializer=tf.constant_initializer(self.std_prior_value),\n            trainable=False,\n            name=\"std_prior\",\n        )\n\n        # Define trainable parameters\n        self.mu = self.add_weight(\n            shape=self.shape,\n            initializer=tf.random_normal_initializer(mean=self.mu_init, stddev=0.1),\n            trainable=True,\n            name=\"mu\",\n        )\n\n        self.rho = self.add_weight(\n            shape=self.shape,\n            initializer=tf.random_normal_initializer(mean=self.rho_init, stddev=0.1),\n            trainable=True,\n            name=\"rho\",\n        )\n\n        # Call super-class build method\n        super().build(input_shape)\n\n    def get_config(self) -&gt; dict:\n        \"\"\"\n        Return the configuration dictionary for serialization.\n\n        Returns:\n            dict: Dictionary with the layer configuration.\n        \"\"\"\n\n        base_config = super().get_config()\n\n        config = {\n            \"shape\": self.shape,\n            \"mu_prior\": self.mu_prior_value,\n            \"std_prior\": self.std_prior_value,\n            \"mu_init\": self.mu_init,\n            \"rho_init\": self.rho_init,\n        }\n\n        return {**base_config, **config}\n\n    def sample(self) -&gt; tf.Tensor:\n        \"\"\"\n        Draw a sample from the Gaussian distribution.\n\n        Returns:\n            tf.Tensor: A sample drawn from the distribution.\n        \"\"\"\n\n        # Sampling with reparametrization trick\n        eps: tf.Tensor = tf.random.normal(shape=self.rho.shape)\n        sigma: tf.Tensor = tf.math.log1p(tf.math.exp(self.rho))\n\n        return self.mu + sigma * eps\n\n    def log_prob(self, x: Optional[tf.Tensor] = None) -&gt; tf.Tensor:\n        \"\"\"\n        Compute the log-probability of a given sample. If no\n        sample is provided, one is drawn internally.\n\n        Args:\n            x: Optional input sample to evaluate. If None,\n                a new sample is drawn from the distribution.\n\n        Returns:\n            tf.Tensor: Scalar log-probability value.\n\n        Notes:\n            Supports both user-supplied and internally drawn\n            samples.\n        \"\"\"\n\n        # Sample if x is None\n        if x is None:\n            x = self.sample()\n\n        # Define pi variable\n        pi: tf.Tensor = tf.convert_to_tensor(math.pi)\n\n        # Compute log priors\n        log_prior = (\n            -tf.math.log(tf.math.sqrt(2 * pi))\n            - tf.math.log(self.std_prior)\n            - (((x - self.mu_prior) ** 2) / (2 * self.std_prior**2))\n            - 0.5\n        )\n\n        # Compute sigma\n        sigma: tf.Tensor = tf.math.log1p(tf.math.exp(self.rho))\n\n        # Compute log posteriors\n        log_posteriors: tf.Tensor = (\n            -tf.math.log(tf.math.sqrt(2 * pi))\n            - tf.math.log(sigma)\n            - (((x - self.mu) ** 2) / (2 * sigma**2))\n            - 0.5\n        )\n\n        # Compute final log probs\n        log_probs = tf.math.reduce_sum(log_posteriors) - tf.math.reduce_sum(log_prior)\n\n        return log_probs\n\n    @property\n    def num_params(self) -&gt; int:\n        \"\"\"\n        Return the number of learnable parameters in the\n        distribution.\n\n        Returns:\n            int: Total count of learnable parameters.\n        \"\"\"\n\n        return int(tf.size(self.mu))\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/distributions.html#illia.distributions.tf.gaussian.GaussianDistribution.num_params","title":"<code>num_params</code>  <code>property</code>","text":"<p>Return the number of learnable parameters in the distribution.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Total count of learnable parameters.</p>"},{"location":"api/Deep%20Learning/Tensorflow/distributions.html#illia.distributions.tf.gaussian.GaussianDistribution.__init__","title":"<code>__init__(shape, mu_prior=0.0, std_prior=0.1, mu_init=0.0, rho_init=-7.0, **kwargs)</code>","text":"<p>Initialize a learnable Gaussian distribution layer.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>tuple[int, ...]</code> <p>Shape of the learnable parameters.</p> required <code>mu_prior</code> <code>float</code> <p>Mean of the Gaussian prior.</p> <code>0.0</code> <code>std_prior</code> <code>float</code> <p>Standard deviation of the prior.</p> <code>0.1</code> <code>mu_init</code> <code>float</code> <p>Initial value for the learnable mean.</p> <code>0.0</code> <code>rho_init</code> <code>float</code> <p>Initial value for the learnable rho.</p> <code>-7.0</code> <code>**kwargs</code> <code>Any</code> <p>Extra arguments passed to the base class.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>illia/distributions/tf/gaussian.py</code> <pre><code>def __init__(\n    self,\n    shape: tuple[int, ...],\n    mu_prior: float = 0.0,\n    std_prior: float = 0.1,\n    mu_init: float = 0.0,\n    rho_init: float = -7.0,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialize a learnable Gaussian distribution layer.\n\n    Args:\n        shape: Shape of the learnable parameters.\n        mu_prior: Mean of the Gaussian prior.\n        std_prior: Standard deviation of the prior.\n        mu_init: Initial value for the learnable mean.\n        rho_init: Initial value for the learnable rho.\n        **kwargs: Extra arguments passed to the base class.\n\n    Returns:\n        None\n    \"\"\"\n\n    # Call super class constructor\n    super().__init__(**kwargs)\n\n    # Set attributes\n    self.shape = shape\n    self.mu_prior_value = mu_prior\n    self.std_prior_value = std_prior\n    self.mu_init = mu_init\n    self.rho_init = rho_init\n\n    # Call build method\n    self.build(self.shape)\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/distributions.html#illia.distributions.tf.gaussian.GaussianDistribution.log_prob","title":"<code>log_prob(x=None)</code>","text":"<p>Compute the log-probability of a given sample. If no sample is provided, one is drawn internally.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Optional[Tensor]</code> <p>Optional input sample to evaluate. If None, a new sample is drawn from the distribution.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>tf.Tensor: Scalar log-probability value.</p> Notes <p>Supports both user-supplied and internally drawn samples.</p> Source code in <code>illia/distributions/tf/gaussian.py</code> <pre><code>def log_prob(self, x: Optional[tf.Tensor] = None) -&gt; tf.Tensor:\n    \"\"\"\n    Compute the log-probability of a given sample. If no\n    sample is provided, one is drawn internally.\n\n    Args:\n        x: Optional input sample to evaluate. If None,\n            a new sample is drawn from the distribution.\n\n    Returns:\n        tf.Tensor: Scalar log-probability value.\n\n    Notes:\n        Supports both user-supplied and internally drawn\n        samples.\n    \"\"\"\n\n    # Sample if x is None\n    if x is None:\n        x = self.sample()\n\n    # Define pi variable\n    pi: tf.Tensor = tf.convert_to_tensor(math.pi)\n\n    # Compute log priors\n    log_prior = (\n        -tf.math.log(tf.math.sqrt(2 * pi))\n        - tf.math.log(self.std_prior)\n        - (((x - self.mu_prior) ** 2) / (2 * self.std_prior**2))\n        - 0.5\n    )\n\n    # Compute sigma\n    sigma: tf.Tensor = tf.math.log1p(tf.math.exp(self.rho))\n\n    # Compute log posteriors\n    log_posteriors: tf.Tensor = (\n        -tf.math.log(tf.math.sqrt(2 * pi))\n        - tf.math.log(sigma)\n        - (((x - self.mu) ** 2) / (2 * sigma**2))\n        - 0.5\n    )\n\n    # Compute final log probs\n    log_probs = tf.math.reduce_sum(log_posteriors) - tf.math.reduce_sum(log_prior)\n\n    return log_probs\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/distributions.html#illia.distributions.tf.gaussian.GaussianDistribution.sample","title":"<code>sample()</code>","text":"<p>Draw a sample from the Gaussian distribution.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>tf.Tensor: A sample drawn from the distribution.</p> Source code in <code>illia/distributions/tf/gaussian.py</code> <pre><code>def sample(self) -&gt; tf.Tensor:\n    \"\"\"\n    Draw a sample from the Gaussian distribution.\n\n    Returns:\n        tf.Tensor: A sample drawn from the distribution.\n    \"\"\"\n\n    # Sampling with reparametrization trick\n    eps: tf.Tensor = tf.random.normal(shape=self.rho.shape)\n    sigma: tf.Tensor = tf.math.log1p(tf.math.exp(self.rho))\n\n    return self.mu + sigma * eps\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/losses.html","title":"Losses","text":""},{"location":"api/Deep%20Learning/Tensorflow/losses.html#illia.losses.tf.kl.KLDivergenceLoss","title":"<code>KLDivergenceLoss</code>","text":"<p>Compute Kullback-Leibler divergence across Bayesian modules. This loss sums the KL divergence from all Bayesian layers in the model. It can be reduced by averaging and scaled by a weight factor.</p> Notes <p>Assumes the model contains submodules derived from <code>BayesianModule</code>.</p> Source code in <code>illia/losses/tf/kl.py</code> <pre><code>@saving.register_keras_serializable(package=\"illia\", name=\"KLDivergenceLoss\")\nclass KLDivergenceLoss(losses.Loss):\n    \"\"\"\n    Compute Kullback-Leibler divergence across Bayesian modules.\n    This loss sums the KL divergence from all Bayesian layers in\n    the model. It can be reduced by averaging and scaled by a\n    weight factor.\n\n    Notes:\n        Assumes the model contains submodules derived from\n        `BayesianModule`.\n    \"\"\"\n\n    def __init__(\n        self,\n        reduction: Literal[\"mean\"] = \"mean\",\n        weight: float = 1.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the KL divergence loss.\n\n        Args:\n            reduction: Method used to reduce the KL loss.\n            weight: Scaling factor for the KL divergence.\n            **kwargs: Extra arguments passed to the base class.\n\n        Returns:\n            None\n        \"\"\"\n\n        # Call super class constructor\n        super().__init__(**kwargs)\n\n        # Set attributes\n        self.reduction = reduction\n        self.weight = weight\n\n    def get_config(self) -&gt; dict:\n        \"\"\"\n        Return the configuration dictionary for serialization.\n\n        Returns:\n            dict: Dictionary containing the KL loss configuration.\n        \"\"\"\n\n        # Get the base configuration\n        base_config = super().get_config()\n\n        # Add custom configurations\n        custom_config = {\"reduction\": self.reduction, \"weight\": self.weight}\n\n        # Combine both configurations\n        return {**base_config, **custom_config}\n\n    def __call__(self, *args: Any, **kwargs: Any) -&gt; tf.Tensor:\n        \"\"\"\n        Compute KL divergence for all Bayesian modules in a model.\n\n        Args:\n            *args: Unused positional arguments.\n            **kwargs: Must include 'model' containing Bayesian\n                layers.\n\n        Returns:\n            tf.Tensor: Weighted KL divergence loss.\n\n        Notes:\n            The KL loss is averaged over the number of parameters\n            and scaled by the `weight` attribute.\n        \"\"\"\n\n        model = kwargs.get(\"model\")\n        if model is None:\n            raise ValueError(\"Model must be provided as a keyword argument\")\n\n        kl_global_cost: tf.Tensor = tf.constant(0.0, dtype=tf.float32)\n        num_params_global: int = 0\n\n        # Iterate through the model's layers\n        for layer in model.layers:\n            if isinstance(layer, BayesianModule):\n                kl_cost, num_params = layer.kl_cost()\n                kl_global_cost += kl_cost\n                num_params_global += num_params\n\n        # Compute mean KL cost and scale by weight\n        kl_global_cost = tf.divide(\n            kl_global_cost, tf.cast(num_params_global, tf.float32)\n        )\n        kl_global_cost = tf.multiply(kl_global_cost, self.weight)\n\n        return kl_global_cost\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/losses.html#illia.losses.tf.kl.KLDivergenceLoss.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Compute KL divergence for all Bayesian modules in a model.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Unused positional arguments.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Must include 'model' containing Bayesian layers.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>tf.Tensor: Weighted KL divergence loss.</p> Notes <p>The KL loss is averaged over the number of parameters and scaled by the <code>weight</code> attribute.</p> Source code in <code>illia/losses/tf/kl.py</code> <pre><code>def __call__(self, *args: Any, **kwargs: Any) -&gt; tf.Tensor:\n    \"\"\"\n    Compute KL divergence for all Bayesian modules in a model.\n\n    Args:\n        *args: Unused positional arguments.\n        **kwargs: Must include 'model' containing Bayesian\n            layers.\n\n    Returns:\n        tf.Tensor: Weighted KL divergence loss.\n\n    Notes:\n        The KL loss is averaged over the number of parameters\n        and scaled by the `weight` attribute.\n    \"\"\"\n\n    model = kwargs.get(\"model\")\n    if model is None:\n        raise ValueError(\"Model must be provided as a keyword argument\")\n\n    kl_global_cost: tf.Tensor = tf.constant(0.0, dtype=tf.float32)\n    num_params_global: int = 0\n\n    # Iterate through the model's layers\n    for layer in model.layers:\n        if isinstance(layer, BayesianModule):\n            kl_cost, num_params = layer.kl_cost()\n            kl_global_cost += kl_cost\n            num_params_global += num_params\n\n    # Compute mean KL cost and scale by weight\n    kl_global_cost = tf.divide(\n        kl_global_cost, tf.cast(num_params_global, tf.float32)\n    )\n    kl_global_cost = tf.multiply(kl_global_cost, self.weight)\n\n    return kl_global_cost\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/losses.html#illia.losses.tf.kl.KLDivergenceLoss.__init__","title":"<code>__init__(reduction='mean', weight=1.0, **kwargs)</code>","text":"<p>Initialize the KL divergence loss.</p> <p>Parameters:</p> Name Type Description Default <code>reduction</code> <code>Literal['mean']</code> <p>Method used to reduce the KL loss.</p> <code>'mean'</code> <code>weight</code> <code>float</code> <p>Scaling factor for the KL divergence.</p> <code>1.0</code> <code>**kwargs</code> <code>Any</code> <p>Extra arguments passed to the base class.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>illia/losses/tf/kl.py</code> <pre><code>def __init__(\n    self,\n    reduction: Literal[\"mean\"] = \"mean\",\n    weight: float = 1.0,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialize the KL divergence loss.\n\n    Args:\n        reduction: Method used to reduce the KL loss.\n        weight: Scaling factor for the KL divergence.\n        **kwargs: Extra arguments passed to the base class.\n\n    Returns:\n        None\n    \"\"\"\n\n    # Call super class constructor\n    super().__init__(**kwargs)\n\n    # Set attributes\n    self.reduction = reduction\n    self.weight = weight\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/losses.html#illia.losses.tf.elbo.ELBOLoss","title":"<code>ELBOLoss</code>","text":"<p>Compute the Evidence Lower Bound (ELBO) loss for Bayesian networks. Combines a reconstruction loss with a KL divergence term. Monte Carlo sampling can estimate the expected reconstruction loss over stochastic layers.</p> Notes <p>The KL term is weighted by <code>kl_weight</code>. The model is assumed to contain Bayesian layers compatible with <code>KLDivergenceLoss</code>.</p> Source code in <code>illia/losses/tf/elbo.py</code> <pre><code>@saving.register_keras_serializable(package=\"illia\", name=\"ELBOLoss\")\nclass ELBOLoss(losses.Loss):\n    \"\"\"\n    Compute the Evidence Lower Bound (ELBO) loss for Bayesian\n    networks. Combines a reconstruction loss with a KL divergence\n    term. Monte Carlo sampling can estimate the expected\n    reconstruction loss over stochastic layers.\n\n    Notes:\n        The KL term is weighted by `kl_weight`. The model is\n        assumed to contain Bayesian layers compatible with\n        `KLDivergenceLoss`.\n    \"\"\"\n\n    def __init__(\n        self,\n        loss_function: Callable[[tf.Tensor, tf.Tensor], tf.Tensor],\n        num_samples: int = 1,\n        kl_weight: float = 1e-3,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the ELBO loss with reconstruction and KL\n        components.\n\n        Args:\n            loss_function: Function used to compute reconstruction\n                loss.\n            num_samples: Number of Monte Carlo samples used for\n                estimation.\n            kl_weight: Weight applied to the KL divergence term.\n            **kwargs: Extra arguments passed to the base class.\n\n        Returns:\n            None\n        \"\"\"\n\n        # Call super class constructor\n        super().__init__(**kwargs)\n\n        # Set attributes\n        self.loss_function = loss_function\n        self.num_samples = num_samples\n        self.kl_weight = kl_weight\n        self.kl_loss = KLDivergenceLoss(weight=kl_weight)\n\n    def get_config(self) -&gt; dict:\n        \"\"\"\n        Return the configuration dictionary for serialization.\n\n        Returns:\n            dict: Dictionary containing the layer configuration.\n        \"\"\"\n\n        # Get the base configuration\n        base_config = super().get_config()\n\n        # Add custom configurations\n        custom_config = {\n            \"loss_function\": self.loss_function,\n            \"num_samples\": self.num_samples,\n            \"kl_weight\": self.kl_weight,\n        }\n\n        # Combine both configurations\n        return {**base_config, **custom_config}\n\n    def __call__(\n        self, y_true: tf.Tensor, y_pred: tf.Tensor, *args: Any, **kwargs: Any\n    ) -&gt; tf.Tensor:\n        \"\"\"\n        Compute the ELBO loss with Monte Carlo sampling and KL\n        regularization.\n\n        Args:\n            y_true: Ground truth targets.\n            y_pred: Predictions from the model.\n            *args: Unused positional arguments.\n            **kwargs: Must include 'model' containing Bayesian\n                layers.\n\n        Returns:\n            tf.Tensor: Scalar ELBO loss averaged over samples.\n\n        Notes:\n            The loss is averaged over `num_samples` Monte Carlo\n            draws.\n        \"\"\"\n\n        model = kwargs.get(\"model\")\n        if model is None:\n            raise ValueError(\"Model must be provided as a keyword argument\")\n\n        loss_value: tf.Tensor = tf.constant(0.0, dtype=tf.float32)\n\n        for _ in range(self.num_samples):\n            current_loss = self.loss_function(y_true, y_pred) + self.kl_loss(model)\n            loss_value += current_loss\n\n        # Average the loss across samples\n        loss_value = tf.divide(loss_value, tf.cast(self.num_samples, tf.float32))\n\n        return loss_value\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/losses.html#illia.losses.tf.elbo.ELBOLoss.__call__","title":"<code>__call__(y_true, y_pred, *args, **kwargs)</code>","text":"<p>Compute the ELBO loss with Monte Carlo sampling and KL regularization.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Tensor</code> <p>Ground truth targets.</p> required <code>y_pred</code> <code>Tensor</code> <p>Predictions from the model.</p> required <code>*args</code> <code>Any</code> <p>Unused positional arguments.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Must include 'model' containing Bayesian layers.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>tf.Tensor: Scalar ELBO loss averaged over samples.</p> Notes <p>The loss is averaged over <code>num_samples</code> Monte Carlo draws.</p> Source code in <code>illia/losses/tf/elbo.py</code> <pre><code>def __call__(\n    self, y_true: tf.Tensor, y_pred: tf.Tensor, *args: Any, **kwargs: Any\n) -&gt; tf.Tensor:\n    \"\"\"\n    Compute the ELBO loss with Monte Carlo sampling and KL\n    regularization.\n\n    Args:\n        y_true: Ground truth targets.\n        y_pred: Predictions from the model.\n        *args: Unused positional arguments.\n        **kwargs: Must include 'model' containing Bayesian\n            layers.\n\n    Returns:\n        tf.Tensor: Scalar ELBO loss averaged over samples.\n\n    Notes:\n        The loss is averaged over `num_samples` Monte Carlo\n        draws.\n    \"\"\"\n\n    model = kwargs.get(\"model\")\n    if model is None:\n        raise ValueError(\"Model must be provided as a keyword argument\")\n\n    loss_value: tf.Tensor = tf.constant(0.0, dtype=tf.float32)\n\n    for _ in range(self.num_samples):\n        current_loss = self.loss_function(y_true, y_pred) + self.kl_loss(model)\n        loss_value += current_loss\n\n    # Average the loss across samples\n    loss_value = tf.divide(loss_value, tf.cast(self.num_samples, tf.float32))\n\n    return loss_value\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/losses.html#illia.losses.tf.elbo.ELBOLoss.__init__","title":"<code>__init__(loss_function, num_samples=1, kl_weight=0.001, **kwargs)</code>","text":"<p>Initialize the ELBO loss with reconstruction and KL components.</p> <p>Parameters:</p> Name Type Description Default <code>loss_function</code> <code>Callable[[Tensor, Tensor], Tensor]</code> <p>Function used to compute reconstruction loss.</p> required <code>num_samples</code> <code>int</code> <p>Number of Monte Carlo samples used for estimation.</p> <code>1</code> <code>kl_weight</code> <code>float</code> <p>Weight applied to the KL divergence term.</p> <code>0.001</code> <code>**kwargs</code> <code>Any</code> <p>Extra arguments passed to the base class.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>illia/losses/tf/elbo.py</code> <pre><code>def __init__(\n    self,\n    loss_function: Callable[[tf.Tensor, tf.Tensor], tf.Tensor],\n    num_samples: int = 1,\n    kl_weight: float = 1e-3,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialize the ELBO loss with reconstruction and KL\n    components.\n\n    Args:\n        loss_function: Function used to compute reconstruction\n            loss.\n        num_samples: Number of Monte Carlo samples used for\n            estimation.\n        kl_weight: Weight applied to the KL divergence term.\n        **kwargs: Extra arguments passed to the base class.\n\n    Returns:\n        None\n    \"\"\"\n\n    # Call super class constructor\n    super().__init__(**kwargs)\n\n    # Set attributes\n    self.loss_function = loss_function\n    self.num_samples = num_samples\n    self.kl_weight = kl_weight\n    self.kl_loss = KLDivergenceLoss(weight=kl_weight)\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/nn.html","title":"Neural Network Layers","text":""},{"location":"api/Deep%20Learning/Tensorflow/nn.html#illia.nn.tf.base.BayesianModule","title":"<code>BayesianModule</code>","text":"<p>Abstract base for Bayesian-aware modules in Tensorflow. Provides mechanisms to track if a module is Bayesian and control parameter updates through freezing/unfreezing.</p> Notes <p>All derived classes must implement <code>freeze</code> and <code>kl_cost</code> to handle parameter management and compute the KL divergence cost.</p> Source code in <code>illia/nn/tf/base.py</code> <pre><code>@saving.register_keras_serializable(package=\"illia\", name=\"BayesianModule\")\nclass BayesianModule(layers.Layer, ABC):\n    \"\"\"\n    Abstract base for Bayesian-aware modules in Tensorflow.\n    Provides mechanisms to track if a module is Bayesian and control\n    parameter updates through freezing/unfreezing.\n\n    Notes:\n        All derived classes must implement `freeze` and `kl_cost` to\n        handle parameter management and compute the KL divergence cost.\n    \"\"\"\n\n    def __init__(self, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Initialize the Bayesian module with default flags.\n        Sets `frozen` to False and `is_bayesian` to True.\n\n        Args:\n            **kwargs: Extra arguments passed to the base class.\n\n        Returns:\n            None.\n        \"\"\"\n\n        # Call super class constructor\n        super().__init__(**kwargs)\n\n        # Set attributes\n        self.frozen: bool = False\n        self.is_bayesian: bool = True\n\n    @abstractmethod\n    def freeze(self) -&gt; None:\n        \"\"\"\n        Freeze the module's parameters to stop gradient computation.\n        If weights or biases are not sampled yet, they are sampled first.\n        Once frozen, parameters are not resampled or updated.\n\n        Returns:\n            None.\n\n        Notes:\n            Must be implemented by all subclasses.\n        \"\"\"\n\n    def unfreeze(self) -&gt; None:\n        \"\"\"\n        Unfreeze the module by setting its `frozen` flag to False.\n        Allows parameters to be sampled and updated again.\n\n        Returns:\n            None.\n        \"\"\"\n\n        self.frozen = False\n\n    @abstractmethod\n    def kl_cost(self) -&gt; tuple[tf.Tensor, int]:\n        \"\"\"\n        Compute the KL divergence cost for all Bayesian parameters.\n\n        Returns:\n            tuple[tf.Tensor, int]: A tuple containing the KL divergence\n                cost and the total number of parameters in the layer.\n\n        Notes:\n            Must be implemented by all subclasses.\n        \"\"\"\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/nn.html#illia.nn.tf.base.BayesianModule.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the Bayesian module with default flags. Sets <code>frozen</code> to False and <code>is_bayesian</code> to True.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Extra arguments passed to the base class.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>illia/nn/tf/base.py</code> <pre><code>def __init__(self, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Initialize the Bayesian module with default flags.\n    Sets `frozen` to False and `is_bayesian` to True.\n\n    Args:\n        **kwargs: Extra arguments passed to the base class.\n\n    Returns:\n        None.\n    \"\"\"\n\n    # Call super class constructor\n    super().__init__(**kwargs)\n\n    # Set attributes\n    self.frozen: bool = False\n    self.is_bayesian: bool = True\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/nn.html#illia.nn.tf.base.BayesianModule.freeze","title":"<code>freeze()</code>  <code>abstractmethod</code>","text":"<p>Freeze the module's parameters to stop gradient computation. If weights or biases are not sampled yet, they are sampled first. Once frozen, parameters are not resampled or updated.</p> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Notes <p>Must be implemented by all subclasses.</p> Source code in <code>illia/nn/tf/base.py</code> <pre><code>@abstractmethod\ndef freeze(self) -&gt; None:\n    \"\"\"\n    Freeze the module's parameters to stop gradient computation.\n    If weights or biases are not sampled yet, they are sampled first.\n    Once frozen, parameters are not resampled or updated.\n\n    Returns:\n        None.\n\n    Notes:\n        Must be implemented by all subclasses.\n    \"\"\"\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/nn.html#illia.nn.tf.base.BayesianModule.kl_cost","title":"<code>kl_cost()</code>  <code>abstractmethod</code>","text":"<p>Compute the KL divergence cost for all Bayesian parameters.</p> <p>Returns:</p> Type Description <code>tuple[Tensor, int]</code> <p>tuple[tf.Tensor, int]: A tuple containing the KL divergence cost and the total number of parameters in the layer.</p> Notes <p>Must be implemented by all subclasses.</p> Source code in <code>illia/nn/tf/base.py</code> <pre><code>@abstractmethod\ndef kl_cost(self) -&gt; tuple[tf.Tensor, int]:\n    \"\"\"\n    Compute the KL divergence cost for all Bayesian parameters.\n\n    Returns:\n        tuple[tf.Tensor, int]: A tuple containing the KL divergence\n            cost and the total number of parameters in the layer.\n\n    Notes:\n        Must be implemented by all subclasses.\n    \"\"\"\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/nn.html#illia.nn.tf.base.BayesianModule.unfreeze","title":"<code>unfreeze()</code>","text":"<p>Unfreeze the module by setting its <code>frozen</code> flag to False. Allows parameters to be sampled and updated again.</p> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>illia/nn/tf/base.py</code> <pre><code>def unfreeze(self) -&gt; None:\n    \"\"\"\n    Unfreeze the module by setting its `frozen` flag to False.\n    Allows parameters to be sampled and updated again.\n\n    Returns:\n        None.\n    \"\"\"\n\n    self.frozen = False\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/nn.html#illia.nn.tf.conv1d.Conv1d","title":"<code>Conv1d</code>","text":"<p>Bayesian 1D convolutional layer with optional weight and bias priors. Behaves like a standard Conv1d but treats weights and bias as random variables sampled from specified distributions. Parameters become fixed when the layer is frozen.</p> Source code in <code>illia/nn/tf/conv1d.py</code> <pre><code>@saving.register_keras_serializable(package=\"illia\", name=\"Conv1d\")\nclass Conv1d(BayesianModule):\n    \"\"\"\n    Bayesian 1D convolutional layer with optional weight and bias priors.\n    Behaves like a standard Conv1d but treats weights and bias as random\n    variables sampled from specified distributions. Parameters become fixed\n    when the layer is frozen.\n    \"\"\"\n\n    bias_distribution: Optional[GaussianDistribution] = None\n\n    def __init__(\n        self,\n        input_channels: int,\n        output_channels: int,\n        kernel_size: int,\n        stride: int = 1,\n        padding: str = \"VALID\",\n        dilation: int = 1,\n        groups: int = 1,\n        data_format: Optional[str] = \"NWC\",\n        weights_distribution: Optional[GaussianDistribution] = None,\n        bias_distribution: Optional[GaussianDistribution] = None,\n        use_bias: bool = True,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes a Bayesian 1D convolutional layer.\n\n        Args:\n            input_channels: Number of channels in the input.\n            output_channels: Number of channels produced by the conv.\n            kernel_size: Size of the convolution kernel.\n            stride: Stride of the convolution.\n            padding: Padding type, 'VALID' or 'SAME'.\n            dilation: Spacing between kernel elements.\n            groups: Number of blocked connections between input/output.\n            data_format: 'NWC' or 'NCW' format for input data.\n            weights_distribution: Distribution for weights sampling.\n            bias_distribution: Distribution for bias sampling.\n            use_bias: Whether to include a bias term.\n            **kwargs: Extra arguments passed to the base class.\n\n        Returns:\n            None.\n\n        Notes:\n            Gaussian distributions are used by default if none are\n            provided.\n        \"\"\"\n\n        # Call super class constructor\n        super().__init__(**kwargs)\n\n        # Check data format\n        self._check_params(kernel_size, groups, stride, dilation, data_format)\n\n        # Set attributes\n        self.input_channels = input_channels\n        self.output_channels = output_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n        self.groups = groups\n        self.use_bias = use_bias\n\n        # Adjust the weights distribution based on the channel format\n        self.data_format = (\n            \"NWC\" if data_format is None or data_format == \"NWC\" else \"NCW\"\n        )\n\n        # Get the weights distribution shape, needs to be channel last\n        self._weights_distribution_shape = (\n            input_channels // groups,\n            kernel_size,\n            output_channels,\n        )\n\n        # Set weights distribution\n        if weights_distribution is None:\n            self.weights_distribution = GaussianDistribution(\n                self._weights_distribution_shape\n            )\n        else:\n            self.weights_distribution = weights_distribution\n\n        # Set bias distribution\n        if self.use_bias:\n            if bias_distribution is None:\n                self.bias_distribution = GaussianDistribution((output_channels,))\n            else:\n                self.bias_distribution = bias_distribution\n        else:\n            self.bias_distribution = None\n\n    def _check_params(\n        self,\n        kernel_size: int,\n        groups: int,\n        stride: int,\n        dilation: int,\n        data_format: Optional[str],\n    ) -&gt; None:\n        \"\"\"\n        Validates convolution parameters for correctness.\n\n        Args:\n            kernel_size: Convolution kernel size.\n            groups: Number of blocked connections.\n            stride: Convolution stride.\n            dilation: Spacing between kernel elements.\n            data_format: 'NWC' or 'NCW' for input tensor.\n\n        Raises:\n            ValueError: If any parameter is invalid.\n        \"\"\"\n\n        if kernel_size is not None and (kernel_size &lt;= 0 or kernel_size % groups != 0):\n            raise ValueError(\n                f\"Invalid `kernel_size`: {kernel_size}. Must be &gt; 0 \"\n                f\"and divisible by `groups` {groups}.\"\n            )\n        if groups &lt;= 0:\n            raise ValueError(f\"Invalid `groups`: {groups}. Must be &gt; 0.\")\n        if isinstance(stride, list):\n            if any(s == 0 for s in stride):\n                raise ValueError(f\"`stride` {stride} cannot contain 0.\")\n            if max(stride) &gt; 1 and isinstance(dilation, list) and max(dilation) &gt; 1:\n                raise ValueError(\n                    f\"`stride` {stride} &gt; 1 not allowed with `dilation` {dilation} &gt; 1.\"\n                )\n        if data_format not in {\"NWC\", \"NCW\"}:\n            raise ValueError(\n                f\"Invalid `data_format`: {data_format}. Must be 'NWC' or 'NCW'.\"\n            )\n\n    def build(self, input_shape: tf.TensorShape) -&gt; None:\n        \"\"\"\n        Build trainable and non-trainable parameters.\n\n        Args:\n            input_shape: Input shape used to trigger layer build.\n\n        Returns:\n            None\n        \"\"\"\n\n        # Register non-trainable variables\n        self.w = self.add_weight(\n            name=\"weights\",\n            initializer=tf.constant_initializer(\n                self.weights_distribution.sample().numpy()\n            ),\n            shape=self._weights_distribution_shape,\n            trainable=False,\n        )\n\n        if self.use_bias and self.bias_distribution is not None:\n            self.b = self.add_weight(\n                name=\"bias\",\n                initializer=tf.constant_initializer(\n                    self.bias_distribution.sample().numpy()\n                ),\n                shape=(self.output_channels,),\n                trainable=False,\n            )\n\n        # Call super-class build method\n        super().build(input_shape)\n\n    def get_config(self) -&gt; dict:\n        \"\"\"\n        Return the configuration dictionary for serialization.\n\n        Returns:\n            dict: Dictionary with the layer configuration.\n        \"\"\"\n\n        # Get the base configuration\n        base_config = super().get_config()\n\n        # Add the custom configurations\n        custom_config = {\n            \"input_channels\": self.input_channels,\n            \"output_channels\": self.output_channels,\n            \"kernel_size\": self.kernel_size,\n            \"stride\": self.stride,\n            \"padding\": self.padding,\n            \"dilation\": self.dilation,\n            \"groups\": self.groups,\n            \"data_format\": self.data_format,\n        }\n\n        # Combine both configurations\n        return {**base_config, **custom_config}\n\n    def _conv1d(\n        self,\n        inputs: tf.Tensor,\n        weight: tf.Tensor,\n        stride: int | list[int],\n        padding: str,\n        data_format: Optional[str] = \"NWC\",\n        dilation: Optional[int | list[int]] = None,\n    ) -&gt; tf.Tensor:\n        \"\"\"\n        Performs a 1D convolution using provided weights.\n\n        Args:\n            inputs: Input tensor.\n            weight: Convolutional kernel tensor.\n            stride: Convolution stride.\n            padding: Padding strategy 'VALID' or 'SAME'.\n            data_format: 'NWC' or 'NCW' input format.\n            dilation: Spacing between kernel elements.\n\n        Returns:\n            Tensor after 1D convolution.\n        \"\"\"\n\n        output: tf.Tensor = tf.nn.conv1d(\n            input=inputs,\n            filters=weight,\n            stride=stride,\n            padding=padding,\n            data_format=data_format,\n            dilations=dilation,\n        )\n\n        return output\n\n    def freeze(self) -&gt; None:\n        \"\"\"\n        Freeze the module's parameters to stop gradient computation.\n        If weights or biases are not sampled yet, they are sampled first.\n        Once frozen, parameters are not resampled or updated.\n\n        Returns:\n            None.\n        \"\"\"\n\n        # Set indicator\n        self.frozen = True\n\n        # Sample weights if they are undefined\n        if self.w is None:\n            self.w = self.weights_distribution.sample()\n\n        # Sample bias is they are undefined\n        if self.use_bias and self.b is None and self.bias_distribution is not None:\n            self.b = self.bias_distribution.sample()\n\n        # Stop gradient computation\n        self.w = tf.stop_gradient(self.w)\n        if self.use_bias:\n            self.b = tf.stop_gradient(self.b)\n\n    def kl_cost(self) -&gt; tuple[tf.Tensor, int]:\n        \"\"\"\n        Compute the KL divergence cost for all Bayesian parameters.\n\n        Returns:\n            tuple[tf.Tensor, int]: A tuple containing the KL divergence\n                cost and the total number of parameters in the layer.\n        \"\"\"\n\n        # Compute log probs\n        log_probs: tf.Tensor = self.weights_distribution.log_prob(self.w)\n\n        # Add bias log probs only if using bias\n        if self.use_bias and self.b is not None and self.bias_distribution is not None:\n            log_probs += self.bias_distribution.log_prob(self.b)\n\n        # Compute number of parameters\n        num_params: int = self.weights_distribution.num_params\n        if self.use_bias and self.bias_distribution is not None:\n            num_params += self.bias_distribution.num_params\n\n        return log_probs, num_params\n\n    def call(self, inputs: tf.Tensor) -&gt; tf.Tensor:\n        \"\"\"\n        Performs a forward pass through the Bayesian Convolution 1D\n        layer. If the layer is not frozen, it samples weights and bias\n        from their respective distributions. If the layer is frozen\n        and the weights or bias are not initialized, it also performs\n        sampling.\n\n        Args:\n            inputs: Input tensor to the layer with shape\n                (batch, length, output_channels) if 'data_format' is\n                'NWC' or (batch, output_channels, length) if\n                'data_format' is 'NCW'\n\n        Returns:\n            Output tensor after convolution with optional bias added.\n\n        Raises:\n            ValueError: If the layer is frozen but weights or bias are\n                undefined.\n        \"\"\"\n\n        # Check if layer is frozen\n        if not self.frozen:\n            self.w = self.weights_distribution.sample()\n\n            # Sample bias only if using bias\n            if self.use_bias and self.bias_distribution is not None:\n                self.b = self.bias_distribution.sample()\n        elif self.w is None or (self.use_bias and self.b is None):\n            raise ValueError(\n                \"Module has been frozen with undefined weights and/or bias.\"\n            )\n\n        # Compute outputs\n        outputs: tf.Tensor = self._conv1d(\n            inputs=inputs,\n            weight=self.w,\n            stride=self.stride,\n            padding=self.padding,\n            data_format=self.data_format,\n            dilation=self.dilation,\n        )\n\n        # Add bias only if using bias\n        if self.use_bias and self.b is not None:\n            outputs = tf.nn.bias_add(\n                value=outputs,\n                bias=self.b,\n                data_format=\"N..C\" if self.data_format == \"NWC\" else \"NC..\",\n            )\n\n        return outputs\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/nn.html#illia.nn.tf.conv1d.Conv1d.__init__","title":"<code>__init__(input_channels, output_channels, kernel_size, stride=1, padding='VALID', dilation=1, groups=1, data_format='NWC', weights_distribution=None, bias_distribution=None, use_bias=True, **kwargs)</code>","text":"<p>Initializes a Bayesian 1D convolutional layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_channels</code> <code>int</code> <p>Number of channels in the input.</p> required <code>output_channels</code> <code>int</code> <p>Number of channels produced by the conv.</p> required <code>kernel_size</code> <code>int</code> <p>Size of the convolution kernel.</p> required <code>stride</code> <code>int</code> <p>Stride of the convolution.</p> <code>1</code> <code>padding</code> <code>str</code> <p>Padding type, 'VALID' or 'SAME'.</p> <code>'VALID'</code> <code>dilation</code> <code>int</code> <p>Spacing between kernel elements.</p> <code>1</code> <code>groups</code> <code>int</code> <p>Number of blocked connections between input/output.</p> <code>1</code> <code>data_format</code> <code>Optional[str]</code> <p>'NWC' or 'NCW' format for input data.</p> <code>'NWC'</code> <code>weights_distribution</code> <code>Optional[GaussianDistribution]</code> <p>Distribution for weights sampling.</p> <code>None</code> <code>bias_distribution</code> <code>Optional[GaussianDistribution]</code> <p>Distribution for bias sampling.</p> <code>None</code> <code>use_bias</code> <code>bool</code> <p>Whether to include a bias term.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Extra arguments passed to the base class.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Notes <p>Gaussian distributions are used by default if none are provided.</p> Source code in <code>illia/nn/tf/conv1d.py</code> <pre><code>def __init__(\n    self,\n    input_channels: int,\n    output_channels: int,\n    kernel_size: int,\n    stride: int = 1,\n    padding: str = \"VALID\",\n    dilation: int = 1,\n    groups: int = 1,\n    data_format: Optional[str] = \"NWC\",\n    weights_distribution: Optional[GaussianDistribution] = None,\n    bias_distribution: Optional[GaussianDistribution] = None,\n    use_bias: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes a Bayesian 1D convolutional layer.\n\n    Args:\n        input_channels: Number of channels in the input.\n        output_channels: Number of channels produced by the conv.\n        kernel_size: Size of the convolution kernel.\n        stride: Stride of the convolution.\n        padding: Padding type, 'VALID' or 'SAME'.\n        dilation: Spacing between kernel elements.\n        groups: Number of blocked connections between input/output.\n        data_format: 'NWC' or 'NCW' format for input data.\n        weights_distribution: Distribution for weights sampling.\n        bias_distribution: Distribution for bias sampling.\n        use_bias: Whether to include a bias term.\n        **kwargs: Extra arguments passed to the base class.\n\n    Returns:\n        None.\n\n    Notes:\n        Gaussian distributions are used by default if none are\n        provided.\n    \"\"\"\n\n    # Call super class constructor\n    super().__init__(**kwargs)\n\n    # Check data format\n    self._check_params(kernel_size, groups, stride, dilation, data_format)\n\n    # Set attributes\n    self.input_channels = input_channels\n    self.output_channels = output_channels\n    self.kernel_size = kernel_size\n    self.stride = stride\n    self.padding = padding\n    self.dilation = dilation\n    self.groups = groups\n    self.use_bias = use_bias\n\n    # Adjust the weights distribution based on the channel format\n    self.data_format = (\n        \"NWC\" if data_format is None or data_format == \"NWC\" else \"NCW\"\n    )\n\n    # Get the weights distribution shape, needs to be channel last\n    self._weights_distribution_shape = (\n        input_channels // groups,\n        kernel_size,\n        output_channels,\n    )\n\n    # Set weights distribution\n    if weights_distribution is None:\n        self.weights_distribution = GaussianDistribution(\n            self._weights_distribution_shape\n        )\n    else:\n        self.weights_distribution = weights_distribution\n\n    # Set bias distribution\n    if self.use_bias:\n        if bias_distribution is None:\n            self.bias_distribution = GaussianDistribution((output_channels,))\n        else:\n            self.bias_distribution = bias_distribution\n    else:\n        self.bias_distribution = None\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/nn.html#illia.nn.tf.conv1d.Conv1d.call","title":"<code>call(inputs)</code>","text":"<p>Performs a forward pass through the Bayesian Convolution 1D layer. If the layer is not frozen, it samples weights and bias from their respective distributions. If the layer is frozen and the weights or bias are not initialized, it also performs sampling.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Tensor</code> <p>Input tensor to the layer with shape (batch, length, output_channels) if 'data_format' is 'NWC' or (batch, output_channels, length) if 'data_format' is 'NCW'</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor after convolution with optional bias added.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the layer is frozen but weights or bias are undefined.</p> Source code in <code>illia/nn/tf/conv1d.py</code> <pre><code>def call(self, inputs: tf.Tensor) -&gt; tf.Tensor:\n    \"\"\"\n    Performs a forward pass through the Bayesian Convolution 1D\n    layer. If the layer is not frozen, it samples weights and bias\n    from their respective distributions. If the layer is frozen\n    and the weights or bias are not initialized, it also performs\n    sampling.\n\n    Args:\n        inputs: Input tensor to the layer with shape\n            (batch, length, output_channels) if 'data_format' is\n            'NWC' or (batch, output_channels, length) if\n            'data_format' is 'NCW'\n\n    Returns:\n        Output tensor after convolution with optional bias added.\n\n    Raises:\n        ValueError: If the layer is frozen but weights or bias are\n            undefined.\n    \"\"\"\n\n    # Check if layer is frozen\n    if not self.frozen:\n        self.w = self.weights_distribution.sample()\n\n        # Sample bias only if using bias\n        if self.use_bias and self.bias_distribution is not None:\n            self.b = self.bias_distribution.sample()\n    elif self.w is None or (self.use_bias and self.b is None):\n        raise ValueError(\n            \"Module has been frozen with undefined weights and/or bias.\"\n        )\n\n    # Compute outputs\n    outputs: tf.Tensor = self._conv1d(\n        inputs=inputs,\n        weight=self.w,\n        stride=self.stride,\n        padding=self.padding,\n        data_format=self.data_format,\n        dilation=self.dilation,\n    )\n\n    # Add bias only if using bias\n    if self.use_bias and self.b is not None:\n        outputs = tf.nn.bias_add(\n            value=outputs,\n            bias=self.b,\n            data_format=\"N..C\" if self.data_format == \"NWC\" else \"NC..\",\n        )\n\n    return outputs\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/nn.html#illia.nn.tf.conv1d.Conv1d.freeze","title":"<code>freeze()</code>","text":"<p>Freeze the module's parameters to stop gradient computation. If weights or biases are not sampled yet, they are sampled first. Once frozen, parameters are not resampled or updated.</p> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>illia/nn/tf/conv1d.py</code> <pre><code>def freeze(self) -&gt; None:\n    \"\"\"\n    Freeze the module's parameters to stop gradient computation.\n    If weights or biases are not sampled yet, they are sampled first.\n    Once frozen, parameters are not resampled or updated.\n\n    Returns:\n        None.\n    \"\"\"\n\n    # Set indicator\n    self.frozen = True\n\n    # Sample weights if they are undefined\n    if self.w is None:\n        self.w = self.weights_distribution.sample()\n\n    # Sample bias is they are undefined\n    if self.use_bias and self.b is None and self.bias_distribution is not None:\n        self.b = self.bias_distribution.sample()\n\n    # Stop gradient computation\n    self.w = tf.stop_gradient(self.w)\n    if self.use_bias:\n        self.b = tf.stop_gradient(self.b)\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/nn.html#illia.nn.tf.conv1d.Conv1d.kl_cost","title":"<code>kl_cost()</code>","text":"<p>Compute the KL divergence cost for all Bayesian parameters.</p> <p>Returns:</p> Type Description <code>tuple[Tensor, int]</code> <p>tuple[tf.Tensor, int]: A tuple containing the KL divergence cost and the total number of parameters in the layer.</p> Source code in <code>illia/nn/tf/conv1d.py</code> <pre><code>def kl_cost(self) -&gt; tuple[tf.Tensor, int]:\n    \"\"\"\n    Compute the KL divergence cost for all Bayesian parameters.\n\n    Returns:\n        tuple[tf.Tensor, int]: A tuple containing the KL divergence\n            cost and the total number of parameters in the layer.\n    \"\"\"\n\n    # Compute log probs\n    log_probs: tf.Tensor = self.weights_distribution.log_prob(self.w)\n\n    # Add bias log probs only if using bias\n    if self.use_bias and self.b is not None and self.bias_distribution is not None:\n        log_probs += self.bias_distribution.log_prob(self.b)\n\n    # Compute number of parameters\n    num_params: int = self.weights_distribution.num_params\n    if self.use_bias and self.bias_distribution is not None:\n        num_params += self.bias_distribution.num_params\n\n    return log_probs, num_params\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/nn.html#illia.nn.tf.conv2d.Conv2d","title":"<code>Conv2d</code>","text":"<p>Bayesian 2D convolutional layer with optional weight and bias priors. Behaves like a standard Conv2d but treats weights and bias as random variables sampled from specified distributions. Parameters become fixed when the layer is frozen.</p> Source code in <code>illia/nn/tf/conv2d.py</code> <pre><code>@saving.register_keras_serializable(package=\"illia\", name=\"Conv2d\")\nclass Conv2d(BayesianModule):\n    \"\"\"\n    Bayesian 2D convolutional layer with optional weight and bias priors.\n    Behaves like a standard Conv2d but treats weights and bias as random\n    variables sampled from specified distributions. Parameters become fixed\n    when the layer is frozen.\n    \"\"\"\n\n    bias_distribution: Optional[GaussianDistribution] = None\n\n    def __init__(\n        self,\n        input_channels: int,\n        output_channels: int,\n        kernel_size: int | list[int],\n        stride: int | list[int] = 1,\n        padding: str | list[int] = \"VALID\",\n        dilation: Optional[int | list[int]] = None,\n        groups: int = 1,\n        data_format: Optional[str] = \"NHWC\",\n        weights_distribution: Optional[GaussianDistribution] = None,\n        bias_distribution: Optional[GaussianDistribution] = None,\n        use_bias: bool = True,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes a Bayesian 2D convolutional layer.\n\n        Args:\n            input_channels: Number of channels in the input image.\n            output_channels: Number of channels produced by the conv.\n            kernel_size: Convolution kernel size as int or list.\n            stride: Convolution stride as int or list.\n            padding: Padding type 'VALID', 'SAME', or list of ints.\n            dilation: Spacing between kernel elements as int or list.\n            groups: Number of blocked connections between input/output.\n            data_format: 'NHWC' or 'NCHW' format for input data.\n            weights_distribution: Distribution for weights sampling.\n            bias_distribution: Distribution for bias sampling.\n            use_bias: Whether to include a bias term.\n            **kwargs: Extra arguments passed to the base class.\n\n        Returns:\n            None.\n\n        Notes:\n            Gaussian distributions are used by default if none are\n            provided.\n        \"\"\"\n\n        # Call super class constructor\n        super().__init__(**kwargs)\n\n        # Check data format\n        self._check_params(kernel_size, groups, stride, dilation, data_format)\n\n        # Set attributes\n        self.input_channels = input_channels\n        self.output_channels = output_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n        self.groups = groups\n        self.use_bias = use_bias\n\n        # Check if kernel_size is a list and unpack it if necessary\n        kernel_shape = (\n            kernel_size if isinstance(kernel_size, list) else [kernel_size, kernel_size]\n        )\n\n        # Adjust the weights distribution based on the channel format\n        self.data_format = (\n            \"NHWC\" if data_format is None or data_format == \"NHWC\" else \"NCHW\"\n        )\n\n        # Set the weights distribution shape\n        self._weights_distribution_shape = (\n            input_channels // groups,\n            *kernel_shape,\n            output_channels,\n        )\n\n        # Set weights distribution\n        if weights_distribution is None:\n            self.weights_distribution = GaussianDistribution(\n                shape=self._weights_distribution_shape\n            )\n        else:\n            self.weights_distribution = weights_distribution\n\n        # Set bias distribution\n        if self.use_bias:\n            if bias_distribution is None:\n                self.bias_distribution = GaussianDistribution((output_channels,))\n            else:\n                self.bias_distribution = bias_distribution\n        else:\n            self.bias_distribution = None\n\n    def _check_params(\n        self,\n        kernel_size: int | list[int],\n        groups: int,\n        stride: int | list[int],\n        dilation: Optional[int | list[int]],\n        data_format: Optional[str],\n    ) -&gt; None:\n        \"\"\"\n        Validates parameters for the 2D convolution operation.\n\n        Args:\n            kernel_size: Convolution kernel size.\n            groups: Number of blocked connections.\n            stride: Convolution stride as int or list.\n            dilation: Kernel spacing as int or list.\n            data_format: 'NHWC' or 'NCHW' for input tensor.\n\n        Raises:\n            ValueError: If any parameter is invalid.\n        \"\"\"\n\n        if kernel_size is not None and isinstance(kernel_size, int):\n            if kernel_size &lt;= 0 or kernel_size % groups != 0:\n                raise ValueError(\n                    f\"Invalid `kernel_size`: {kernel_size}. Must \"\n                    f\"be &gt; 0 and divisible by `groups` {groups}.\"\n                )\n        if groups &lt;= 0:\n            raise ValueError(f\"Invalid `groups`: {groups}. Must be &gt; 0.\")\n        if isinstance(stride, list):\n            if any(s == 0 for s in stride):\n                raise ValueError(f\"`stride` {stride} cannot contain 0.\")\n            if max(stride) &gt; 1 and isinstance(dilation, list) and max(dilation) &gt; 1:\n                raise ValueError(\n                    f\"`stride` {stride} &gt; 1 not allowed with \"\n                    f\"`dilation` {dilation} &gt; 1.\"\n                )\n        if data_format not in {\"NHWC\", \"NCHW\"}:\n            raise ValueError(\n                f\"Invalid `data_format`: {data_format}. Must be 'NHWC' or 'NCHW'.\"\n            )\n\n    def build(self, input_shape: tf.TensorShape) -&gt; None:\n        \"\"\"\n        Build trainable and non-trainable parameters.\n\n        Args:\n            input_shape: Input shape used to trigger layer build.\n\n        Returns:\n            None\n        \"\"\"\n\n        # Register non-trainable variables\n        self.w = self.add_weight(\n            name=\"weights\",\n            initializer=tf.constant_initializer(\n                self.weights_distribution.sample().numpy()\n            ),\n            shape=self._weights_distribution_shape,\n            trainable=False,\n        )\n\n        if self.use_bias and self.bias_distribution is not None:\n            self.b = self.add_weight(\n                name=\"bias\",\n                initializer=tf.constant_initializer(\n                    self.bias_distribution.sample().numpy()\n                ),\n                shape=(self.output_channels,),\n                trainable=False,\n            )\n\n        # Call super-class build method\n        super().build(input_shape)\n\n    def get_config(self) -&gt; dict:\n        \"\"\"\n        Return the configuration dictionary for serialization.\n\n        Returns:\n            dict: Dictionary with the layer configuration.\n        \"\"\"\n\n        # Get the base configuration\n        base_config = super().get_config()\n\n        # Add the custom configurations\n        custom_config = {\n            \"input_channels\": self.input_channels,\n            \"output_channels\": self.output_channels,\n            \"kernel_size\": self.kernel_size,\n            \"stride\": self.stride,\n            \"padding\": self.padding,\n            \"dilation\": self.dilation,\n            \"groups\": self.groups,\n            \"data_format\": self.data_format,\n        }\n\n        # Combine both configurations\n        return {**base_config, **custom_config}\n\n    def _conv2d(\n        self,\n        inputs: tf.Tensor,\n        weight: tf.Tensor,\n        stride: int | list[int],\n        padding: str | list[int],\n        data_format: Optional[str] = \"NHWC\",\n        dilation: Optional[int | list[int]] = None,\n    ) -&gt; tf.Tensor:\n        \"\"\"\n        Performs a 2D convolution using provided weights.\n\n        Args:\n            inputs: Input tensor.\n            weight: Convolutional kernel tensor.\n            stride: Convolution stride as int or list.\n            padding: Padding type 'VALID', 'SAME', or list of ints.\n            data_format: 'NHWC' or 'NCHW' input format.\n            dilation: Kernel spacing as int or list.\n\n        Returns:\n            Tensor after 2D convolution.\n        \"\"\"\n\n        output: tf.Tensor = tf.nn.conv2d(\n            input=inputs,\n            filters=weight,\n            strides=stride,\n            padding=padding,\n            data_format=data_format,\n            dilations=dilation,\n        )\n\n        return output\n\n    def freeze(self) -&gt; None:\n        \"\"\"\n        Freeze the module's parameters to stop gradient computation.\n        If weights or biases are not sampled yet, they are sampled first.\n        Once frozen, parameters are not resampled or updated.\n\n        Returns:\n            None.\n        \"\"\"\n\n        # Set indicator\n        self.frozen = True\n\n        # Sample weights if they are undefined\n        if self.w is None:\n            self.w = self.weights_distribution.sample()\n\n        # Sample bias is they are undefined\n        if self.use_bias and self.b is None and self.bias_distribution is not None:\n            self.b = self.bias_distribution.sample()\n\n        # Stop gradient computation\n        self.w = tf.stop_gradient(self.w)\n        if self.use_bias:\n            self.b = tf.stop_gradient(self.b)\n\n    def kl_cost(self) -&gt; tuple[tf.Tensor, int]:\n        \"\"\"\n        Compute the KL divergence cost for all Bayesian parameters.\n\n        Returns:\n            tuple[tf.Tensor, int]: A tuple containing the KL divergence\n                cost and the total number of parameters in the layer.\n        \"\"\"\n\n        # Compute log probs\n        log_probs: tf.Tensor = self.weights_distribution.log_prob(self.w)\n\n        # Add bias log probs only if using bias\n        if self.use_bias and self.b is not None and self.bias_distribution is not None:\n            log_probs += self.bias_distribution.log_prob(self.b)\n\n        # Compute number of parameters\n        num_params: int = self.weights_distribution.num_params\n        if self.use_bias and self.bias_distribution is not None:\n            num_params += self.bias_distribution.num_params\n\n        return log_probs, num_params\n\n    def call(self, inputs: tf.Tensor) -&gt; tf.Tensor:\n        \"\"\"\n        Performs a forward pass through the Bayesian Convolution 2D\n        layer. If the layer is not frozen, it samples weights and bias\n        from their respective distributions. If the layer is frozen\n        and the weights or bias are not initialized, it also performs\n        sampling.\n\n        Args:\n            inputs: Input tensor with shape [batch, height, width,\n                channels] if NHWC or [batch, channels, height, width] if NCHW.\n\n        Returns:\n            Output tensor after convolution with optional bias added.\n\n        Raises:\n            ValueError: If the layer is frozen but weights or bias are\n                undefined.\n        \"\"\"\n\n        # Check if layer is frozen\n        if not self.frozen:\n            self.w = self.weights_distribution.sample()\n\n            # Sample bias only if using bias\n            if self.use_bias and self.bias_distribution is not None:\n                self.b = self.bias_distribution.sample()\n        elif self.w is None or (self.use_bias and self.b is None):\n            raise ValueError(\n                \"Module has been frozen with undefined weights and/or bias.\"\n            )\n\n        # Compute outputs\n        outputs: tf.Tensor = self._conv2d(\n            inputs=inputs,\n            weight=self.w,\n            stride=self.stride,\n            padding=self.padding,\n            data_format=self.data_format,\n            dilation=self.dilation,\n        )\n\n        # Add bias only if using bias\n        if self.use_bias and self.b is not None:\n            outputs = tf.nn.bias_add(\n                value=outputs,\n                bias=self.b,\n                data_format=\"N..C\" if self.data_format == \"NHWC\" else \"NC..\",\n            )\n\n        return outputs\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/nn.html#illia.nn.tf.conv2d.Conv2d.__init__","title":"<code>__init__(input_channels, output_channels, kernel_size, stride=1, padding='VALID', dilation=None, groups=1, data_format='NHWC', weights_distribution=None, bias_distribution=None, use_bias=True, **kwargs)</code>","text":"<p>Initializes a Bayesian 2D convolutional layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_channels</code> <code>int</code> <p>Number of channels in the input image.</p> required <code>output_channels</code> <code>int</code> <p>Number of channels produced by the conv.</p> required <code>kernel_size</code> <code>int | list[int]</code> <p>Convolution kernel size as int or list.</p> required <code>stride</code> <code>int | list[int]</code> <p>Convolution stride as int or list.</p> <code>1</code> <code>padding</code> <code>str | list[int]</code> <p>Padding type 'VALID', 'SAME', or list of ints.</p> <code>'VALID'</code> <code>dilation</code> <code>Optional[int | list[int]]</code> <p>Spacing between kernel elements as int or list.</p> <code>None</code> <code>groups</code> <code>int</code> <p>Number of blocked connections between input/output.</p> <code>1</code> <code>data_format</code> <code>Optional[str]</code> <p>'NHWC' or 'NCHW' format for input data.</p> <code>'NHWC'</code> <code>weights_distribution</code> <code>Optional[GaussianDistribution]</code> <p>Distribution for weights sampling.</p> <code>None</code> <code>bias_distribution</code> <code>Optional[GaussianDistribution]</code> <p>Distribution for bias sampling.</p> <code>None</code> <code>use_bias</code> <code>bool</code> <p>Whether to include a bias term.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Extra arguments passed to the base class.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Notes <p>Gaussian distributions are used by default if none are provided.</p> Source code in <code>illia/nn/tf/conv2d.py</code> <pre><code>def __init__(\n    self,\n    input_channels: int,\n    output_channels: int,\n    kernel_size: int | list[int],\n    stride: int | list[int] = 1,\n    padding: str | list[int] = \"VALID\",\n    dilation: Optional[int | list[int]] = None,\n    groups: int = 1,\n    data_format: Optional[str] = \"NHWC\",\n    weights_distribution: Optional[GaussianDistribution] = None,\n    bias_distribution: Optional[GaussianDistribution] = None,\n    use_bias: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes a Bayesian 2D convolutional layer.\n\n    Args:\n        input_channels: Number of channels in the input image.\n        output_channels: Number of channels produced by the conv.\n        kernel_size: Convolution kernel size as int or list.\n        stride: Convolution stride as int or list.\n        padding: Padding type 'VALID', 'SAME', or list of ints.\n        dilation: Spacing between kernel elements as int or list.\n        groups: Number of blocked connections between input/output.\n        data_format: 'NHWC' or 'NCHW' format for input data.\n        weights_distribution: Distribution for weights sampling.\n        bias_distribution: Distribution for bias sampling.\n        use_bias: Whether to include a bias term.\n        **kwargs: Extra arguments passed to the base class.\n\n    Returns:\n        None.\n\n    Notes:\n        Gaussian distributions are used by default if none are\n        provided.\n    \"\"\"\n\n    # Call super class constructor\n    super().__init__(**kwargs)\n\n    # Check data format\n    self._check_params(kernel_size, groups, stride, dilation, data_format)\n\n    # Set attributes\n    self.input_channels = input_channels\n    self.output_channels = output_channels\n    self.kernel_size = kernel_size\n    self.stride = stride\n    self.padding = padding\n    self.dilation = dilation\n    self.groups = groups\n    self.use_bias = use_bias\n\n    # Check if kernel_size is a list and unpack it if necessary\n    kernel_shape = (\n        kernel_size if isinstance(kernel_size, list) else [kernel_size, kernel_size]\n    )\n\n    # Adjust the weights distribution based on the channel format\n    self.data_format = (\n        \"NHWC\" if data_format is None or data_format == \"NHWC\" else \"NCHW\"\n    )\n\n    # Set the weights distribution shape\n    self._weights_distribution_shape = (\n        input_channels // groups,\n        *kernel_shape,\n        output_channels,\n    )\n\n    # Set weights distribution\n    if weights_distribution is None:\n        self.weights_distribution = GaussianDistribution(\n            shape=self._weights_distribution_shape\n        )\n    else:\n        self.weights_distribution = weights_distribution\n\n    # Set bias distribution\n    if self.use_bias:\n        if bias_distribution is None:\n            self.bias_distribution = GaussianDistribution((output_channels,))\n        else:\n            self.bias_distribution = bias_distribution\n    else:\n        self.bias_distribution = None\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/nn.html#illia.nn.tf.conv2d.Conv2d.call","title":"<code>call(inputs)</code>","text":"<p>Performs a forward pass through the Bayesian Convolution 2D layer. If the layer is not frozen, it samples weights and bias from their respective distributions. If the layer is frozen and the weights or bias are not initialized, it also performs sampling.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Tensor</code> <p>Input tensor with shape [batch, height, width, channels] if NHWC or [batch, channels, height, width] if NCHW.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor after convolution with optional bias added.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the layer is frozen but weights or bias are undefined.</p> Source code in <code>illia/nn/tf/conv2d.py</code> <pre><code>def call(self, inputs: tf.Tensor) -&gt; tf.Tensor:\n    \"\"\"\n    Performs a forward pass through the Bayesian Convolution 2D\n    layer. If the layer is not frozen, it samples weights and bias\n    from their respective distributions. If the layer is frozen\n    and the weights or bias are not initialized, it also performs\n    sampling.\n\n    Args:\n        inputs: Input tensor with shape [batch, height, width,\n            channels] if NHWC or [batch, channels, height, width] if NCHW.\n\n    Returns:\n        Output tensor after convolution with optional bias added.\n\n    Raises:\n        ValueError: If the layer is frozen but weights or bias are\n            undefined.\n    \"\"\"\n\n    # Check if layer is frozen\n    if not self.frozen:\n        self.w = self.weights_distribution.sample()\n\n        # Sample bias only if using bias\n        if self.use_bias and self.bias_distribution is not None:\n            self.b = self.bias_distribution.sample()\n    elif self.w is None or (self.use_bias and self.b is None):\n        raise ValueError(\n            \"Module has been frozen with undefined weights and/or bias.\"\n        )\n\n    # Compute outputs\n    outputs: tf.Tensor = self._conv2d(\n        inputs=inputs,\n        weight=self.w,\n        stride=self.stride,\n        padding=self.padding,\n        data_format=self.data_format,\n        dilation=self.dilation,\n    )\n\n    # Add bias only if using bias\n    if self.use_bias and self.b is not None:\n        outputs = tf.nn.bias_add(\n            value=outputs,\n            bias=self.b,\n            data_format=\"N..C\" if self.data_format == \"NHWC\" else \"NC..\",\n        )\n\n    return outputs\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/nn.html#illia.nn.tf.conv2d.Conv2d.freeze","title":"<code>freeze()</code>","text":"<p>Freeze the module's parameters to stop gradient computation. If weights or biases are not sampled yet, they are sampled first. Once frozen, parameters are not resampled or updated.</p> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>illia/nn/tf/conv2d.py</code> <pre><code>def freeze(self) -&gt; None:\n    \"\"\"\n    Freeze the module's parameters to stop gradient computation.\n    If weights or biases are not sampled yet, they are sampled first.\n    Once frozen, parameters are not resampled or updated.\n\n    Returns:\n        None.\n    \"\"\"\n\n    # Set indicator\n    self.frozen = True\n\n    # Sample weights if they are undefined\n    if self.w is None:\n        self.w = self.weights_distribution.sample()\n\n    # Sample bias is they are undefined\n    if self.use_bias and self.b is None and self.bias_distribution is not None:\n        self.b = self.bias_distribution.sample()\n\n    # Stop gradient computation\n    self.w = tf.stop_gradient(self.w)\n    if self.use_bias:\n        self.b = tf.stop_gradient(self.b)\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/nn.html#illia.nn.tf.conv2d.Conv2d.kl_cost","title":"<code>kl_cost()</code>","text":"<p>Compute the KL divergence cost for all Bayesian parameters.</p> <p>Returns:</p> Type Description <code>tuple[Tensor, int]</code> <p>tuple[tf.Tensor, int]: A tuple containing the KL divergence cost and the total number of parameters in the layer.</p> Source code in <code>illia/nn/tf/conv2d.py</code> <pre><code>def kl_cost(self) -&gt; tuple[tf.Tensor, int]:\n    \"\"\"\n    Compute the KL divergence cost for all Bayesian parameters.\n\n    Returns:\n        tuple[tf.Tensor, int]: A tuple containing the KL divergence\n            cost and the total number of parameters in the layer.\n    \"\"\"\n\n    # Compute log probs\n    log_probs: tf.Tensor = self.weights_distribution.log_prob(self.w)\n\n    # Add bias log probs only if using bias\n    if self.use_bias and self.b is not None and self.bias_distribution is not None:\n        log_probs += self.bias_distribution.log_prob(self.b)\n\n    # Compute number of parameters\n    num_params: int = self.weights_distribution.num_params\n    if self.use_bias and self.bias_distribution is not None:\n        num_params += self.bias_distribution.num_params\n\n    return log_probs, num_params\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/nn.html#illia.nn.tf.embedding.Embedding","title":"<code>Embedding</code>","text":"<p>Bayesian embedding layer with optional padding and max-norm. Each embedding vector is sampled from a specified distribution. Can be frozen to fix embeddings and stop gradients.</p> Source code in <code>illia/nn/tf/embedding.py</code> <pre><code>@saving.register_keras_serializable(package=\"illia\", name=\"Embedding\")\nclass Embedding(BayesianModule):\n    \"\"\"\n    Bayesian embedding layer with optional padding and max-norm. Each\n    embedding vector is sampled from a specified distribution. Can be\n    frozen to fix embeddings and stop gradients.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_embeddings: int,\n        embeddings_dim: int,\n        padding_idx: Optional[int] = None,\n        max_norm: Optional[float] = None,\n        norm_type: float = 2.0,\n        scale_grad_by_freq: bool = False,\n        sparse: bool = False,\n        weights_distribution: Optional[GaussianDistribution] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes a Bayesian Embedding layer.\n\n        Args:\n            num_embeddings: Size of the embedding dictionary.\n            embeddings_dim: Dimensionality of each embedding vector.\n            padding_idx: Index to exclude from gradient computation.\n            max_norm: Maximum norm for embedding vectors.\n            norm_type: p of the p-norm for max_norm.\n            scale_grad_by_freq: Scale gradient by inverse frequency.\n            sparse: Use sparse gradient updates.\n            weights_distribution: Distribution for embedding weights.\n            **kwargs: Extra arguments passed to the base class.\n\n        Returns:\n            None.\n\n        Notes:\n            Gaussian distributions are used by default if none are\n            provided.\n        \"\"\"\n\n        # Call super class constructor\n        super().__init__(**kwargs)\n\n        # Set atributtes\n        self.num_embeddings = num_embeddings\n        self.embeddings_dim = embeddings_dim\n        self.padding_idx = padding_idx\n        self.max_norm = max_norm\n        self.norm_type = norm_type\n        self.scale_grad_by_freq = scale_grad_by_freq\n        self.sparse = sparse\n\n        # Set weights distribution\n        if weights_distribution is None:\n            self.weights_distribution = GaussianDistribution(\n                (num_embeddings, embeddings_dim)\n            )\n        else:\n            self.weights_distribution = weights_distribution\n\n    def build(self, input_shape: tf.TensorShape) -&gt; None:\n        \"\"\"\n        Build trainable and non-trainable parameters.\n\n        Args:\n            input_shape: Input shape used to trigger layer build.\n\n        Returns:\n            None\n        \"\"\"\n\n        # Create a variable for weights\n        self.w = self.add_weight(\n            name=\"weights\",\n            initializer=tf.constant_initializer(\n                self.weights_distribution.sample().numpy()\n            ),\n            shape=(self.num_embeddings, self.embeddings_dim),\n            trainable=False,\n        )\n\n        # Call super-class build method\n        super().build(input_shape)\n\n    def get_config(self) -&gt; dict:\n        \"\"\"\n        Return the configuration dictionary for serialization.\n\n        Returns:\n            dict: Dictionary with the layer configuration.\n        \"\"\"\n\n        # Get the base configuration\n        base_config = super().get_config()\n\n        # Add the custom configurations\n        config = {\n            \"num_embeddings\": self.num_embeddings,\n            \"embeddings_dim\": self.embeddings_dim,\n            \"padding_idx\": self.padding_idx,\n            \"max_norm\": self.max_norm,\n            \"norm_type\": self.norm_type,\n            \"scale_grad_by_freq\": self.scale_grad_by_freq,\n            \"sparse\": self.sparse,\n        }\n\n        # Combine both configurations\n        return {**base_config, **config}\n\n    def _embedding(\n        self,\n        inputs: tf.Tensor,\n        weight: tf.Tensor,\n        padding_idx: Optional[int] = None,\n        max_norm: Optional[float] = None,\n        norm_type: Optional[float] = 2.0,\n        sparse: bool = False,\n    ) -&gt; tf.Tensor:\n        \"\"\"\n        Computes embedding lookup with optional padding and normalization.\n\n        Args:\n            inputs: Input tensor of indices.\n            weight: Embedding weight tensor.\n            padding_idx: Index to mask out.\n            max_norm: Maximum norm for embeddings.\n            norm_type: Norm type for max_norm.\n            sparse: Use sparse lookup if True.\n\n        Returns:\n            Tensor of embeddings.\n        \"\"\"\n\n        inputs = tf.cast(inputs, tf.int32)\n        if sparse is not None:\n            embeddings = tf.nn.embedding_lookup(weight, inputs)\n        else:\n            embeddings = tf.nn.embedding_lookup_sparse(weight, inputs, sp_weights=None)\n\n        if padding_idx is not None:\n            padding_mask = tf.not_equal(inputs, padding_idx)\n            embeddings = tf.where(\n                tf.expand_dims(padding_mask, -1), embeddings, tf.zeros_like(embeddings)\n            )\n\n        if max_norm is not None:\n            norms = tf.norm(embeddings, ord=norm_type, axis=-1, keepdims=True)\n            desired = tf.clip_by_value(norms, clip_value_min=0, clip_value_max=max_norm)\n            scale = desired / (tf.maximum(norms, 1e-7))\n            embeddings = embeddings * scale\n\n        return embeddings\n\n    def freeze(self) -&gt; None:\n        \"\"\"\n        Freeze the module's parameters to stop gradient computation.\n        If weights or biases are not sampled yet, they are sampled first.\n        Once frozen, parameters are not resampled or updated.\n\n        Returns:\n            None.\n        \"\"\"\n\n        # Set indicator\n        self.frozen = True\n\n        # Sample weights if they are undefined\n        if self.w is None:\n            self.w = self.weights_distribution.sample()\n\n        # Stop gradient computation\n        self.w = tf.stop_gradient(self.w)\n\n    def kl_cost(self) -&gt; tuple[tf.Tensor, int]:\n        \"\"\"\n        Compute the KL divergence cost for all Bayesian parameters.\n\n        Returns:\n            tuple[tf.Tensor, int]: A tuple containing the KL divergence\n                cost and the total number of parameters in the layer.\n        \"\"\"\n\n        # Get log probs\n        log_probs: tf.Tensor = self.weights_distribution.log_prob(self.w)\n\n        # Get number of parameters\n        num_params: int = self.weights_distribution.num_params\n\n        return log_probs, num_params\n\n    def call(self, inputs: tf.Tensor) -&gt; tf.Tensor:\n        \"\"\"\n        Performs embedding lookup using current weights.\n\n        Args:\n            inputs: Input tensor of indices with shape [batch, *].\n\n        Returns:\n            Tensor of embeddings.\n\n        Raises:\n            ValueError: If the layer is frozen but weights are\n                undefined.\n        \"\"\"\n\n        # Check if layer is frozen\n        if not self.frozen:\n            self.w = self.weights_distribution.sample()\n        elif self.w is None:\n            raise ValueError(\"Module has been frozen with undefined weights.\")\n\n        # Compute outputs\n        outputs: tf.Tensor = self._embedding(\n            inputs,\n            self.w,\n            self.padding_idx,\n            self.max_norm,\n            self.norm_type,\n            self.sparse,\n        )\n\n        return outputs\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/nn.html#illia.nn.tf.embedding.Embedding.__init__","title":"<code>__init__(num_embeddings, embeddings_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, weights_distribution=None, **kwargs)</code>","text":"<p>Initializes a Bayesian Embedding layer.</p> <p>Parameters:</p> Name Type Description Default <code>num_embeddings</code> <code>int</code> <p>Size of the embedding dictionary.</p> required <code>embeddings_dim</code> <code>int</code> <p>Dimensionality of each embedding vector.</p> required <code>padding_idx</code> <code>Optional[int]</code> <p>Index to exclude from gradient computation.</p> <code>None</code> <code>max_norm</code> <code>Optional[float]</code> <p>Maximum norm for embedding vectors.</p> <code>None</code> <code>norm_type</code> <code>float</code> <p>p of the p-norm for max_norm.</p> <code>2.0</code> <code>scale_grad_by_freq</code> <code>bool</code> <p>Scale gradient by inverse frequency.</p> <code>False</code> <code>sparse</code> <code>bool</code> <p>Use sparse gradient updates.</p> <code>False</code> <code>weights_distribution</code> <code>Optional[GaussianDistribution]</code> <p>Distribution for embedding weights.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Extra arguments passed to the base class.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Notes <p>Gaussian distributions are used by default if none are provided.</p> Source code in <code>illia/nn/tf/embedding.py</code> <pre><code>def __init__(\n    self,\n    num_embeddings: int,\n    embeddings_dim: int,\n    padding_idx: Optional[int] = None,\n    max_norm: Optional[float] = None,\n    norm_type: float = 2.0,\n    scale_grad_by_freq: bool = False,\n    sparse: bool = False,\n    weights_distribution: Optional[GaussianDistribution] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes a Bayesian Embedding layer.\n\n    Args:\n        num_embeddings: Size of the embedding dictionary.\n        embeddings_dim: Dimensionality of each embedding vector.\n        padding_idx: Index to exclude from gradient computation.\n        max_norm: Maximum norm for embedding vectors.\n        norm_type: p of the p-norm for max_norm.\n        scale_grad_by_freq: Scale gradient by inverse frequency.\n        sparse: Use sparse gradient updates.\n        weights_distribution: Distribution for embedding weights.\n        **kwargs: Extra arguments passed to the base class.\n\n    Returns:\n        None.\n\n    Notes:\n        Gaussian distributions are used by default if none are\n        provided.\n    \"\"\"\n\n    # Call super class constructor\n    super().__init__(**kwargs)\n\n    # Set atributtes\n    self.num_embeddings = num_embeddings\n    self.embeddings_dim = embeddings_dim\n    self.padding_idx = padding_idx\n    self.max_norm = max_norm\n    self.norm_type = norm_type\n    self.scale_grad_by_freq = scale_grad_by_freq\n    self.sparse = sparse\n\n    # Set weights distribution\n    if weights_distribution is None:\n        self.weights_distribution = GaussianDistribution(\n            (num_embeddings, embeddings_dim)\n        )\n    else:\n        self.weights_distribution = weights_distribution\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/nn.html#illia.nn.tf.embedding.Embedding.call","title":"<code>call(inputs)</code>","text":"<p>Performs embedding lookup using current weights.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Tensor</code> <p>Input tensor of indices with shape [batch, *].</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of embeddings.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the layer is frozen but weights are undefined.</p> Source code in <code>illia/nn/tf/embedding.py</code> <pre><code>def call(self, inputs: tf.Tensor) -&gt; tf.Tensor:\n    \"\"\"\n    Performs embedding lookup using current weights.\n\n    Args:\n        inputs: Input tensor of indices with shape [batch, *].\n\n    Returns:\n        Tensor of embeddings.\n\n    Raises:\n        ValueError: If the layer is frozen but weights are\n            undefined.\n    \"\"\"\n\n    # Check if layer is frozen\n    if not self.frozen:\n        self.w = self.weights_distribution.sample()\n    elif self.w is None:\n        raise ValueError(\"Module has been frozen with undefined weights.\")\n\n    # Compute outputs\n    outputs: tf.Tensor = self._embedding(\n        inputs,\n        self.w,\n        self.padding_idx,\n        self.max_norm,\n        self.norm_type,\n        self.sparse,\n    )\n\n    return outputs\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/nn.html#illia.nn.tf.embedding.Embedding.freeze","title":"<code>freeze()</code>","text":"<p>Freeze the module's parameters to stop gradient computation. If weights or biases are not sampled yet, they are sampled first. Once frozen, parameters are not resampled or updated.</p> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>illia/nn/tf/embedding.py</code> <pre><code>def freeze(self) -&gt; None:\n    \"\"\"\n    Freeze the module's parameters to stop gradient computation.\n    If weights or biases are not sampled yet, they are sampled first.\n    Once frozen, parameters are not resampled or updated.\n\n    Returns:\n        None.\n    \"\"\"\n\n    # Set indicator\n    self.frozen = True\n\n    # Sample weights if they are undefined\n    if self.w is None:\n        self.w = self.weights_distribution.sample()\n\n    # Stop gradient computation\n    self.w = tf.stop_gradient(self.w)\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/nn.html#illia.nn.tf.embedding.Embedding.kl_cost","title":"<code>kl_cost()</code>","text":"<p>Compute the KL divergence cost for all Bayesian parameters.</p> <p>Returns:</p> Type Description <code>tuple[Tensor, int]</code> <p>tuple[tf.Tensor, int]: A tuple containing the KL divergence cost and the total number of parameters in the layer.</p> Source code in <code>illia/nn/tf/embedding.py</code> <pre><code>def kl_cost(self) -&gt; tuple[tf.Tensor, int]:\n    \"\"\"\n    Compute the KL divergence cost for all Bayesian parameters.\n\n    Returns:\n        tuple[tf.Tensor, int]: A tuple containing the KL divergence\n            cost and the total number of parameters in the layer.\n    \"\"\"\n\n    # Get log probs\n    log_probs: tf.Tensor = self.weights_distribution.log_prob(self.w)\n\n    # Get number of parameters\n    num_params: int = self.weights_distribution.num_params\n\n    return log_probs, num_params\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/nn.html#illia.nn.tf.linear.Linear","title":"<code>Linear</code>","text":"<p>Bayesian linear layer (fully connected) with optional weight and bias distributions. Can be frozen to stop gradient updates and fix parameters.</p> Source code in <code>illia/nn/tf/linear.py</code> <pre><code>@saving.register_keras_serializable(package=\"illia\", name=\"Linear\")\nclass Linear(BayesianModule):\n    \"\"\"\n    Bayesian linear layer (fully connected) with optional weight and bias\n    distributions. Can be frozen to stop gradient updates and fix\n    parameters.\n    \"\"\"\n\n    bias_distribution: Optional[GaussianDistribution] = None\n\n    def __init__(\n        self,\n        input_size: int,\n        output_size: int,\n        weights_distribution: Optional[GaussianDistribution] = None,\n        bias_distribution: Optional[GaussianDistribution] = None,\n        use_bias: bool = True,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes a Bayesian Linear layer.\n\n        Args:\n            input_size: Number of input features.\n            output_size: Number of output features.\n            weights_distribution: Distribution for the weights.\n            bias_distribution: Distribution for the bias.\n            use_bias: Whether to include a bias term.\n            **kwargs: Extra arguments passed to the base class.\n\n        Returns:\n            None.\n\n        Notes:\n            Gaussian distributions are used by default if none are\n            provided.\n        \"\"\"\n\n        # Call super-class constructor\n        super().__init__(**kwargs)\n\n        # Set parameters\n        self.input_size = input_size\n        self.output_size = output_size\n        self.use_bias = use_bias\n\n        # Set weights distribution\n        if weights_distribution is None:\n            self.weights_distribution = GaussianDistribution((output_size, input_size))\n        else:\n            self.weights_distribution = weights_distribution\n\n        # Set bias distribution\n        if self.use_bias:\n            if bias_distribution is None:\n                self.bias_distribution = GaussianDistribution((output_size,))\n            else:\n                self.bias_distribution = bias_distribution\n        else:\n            self.bias_distribution = None\n\n    def build(self, input_shape: tf.TensorShape) -&gt; None:\n        \"\"\"\n        Build trainable and non-trainable parameters.\n\n        Args:\n            input_shape: Input shape used to trigger layer build.\n\n        Returns:\n            None\n        \"\"\"\n\n        # Register non-trainable variables\n        self.w = self.add_weight(\n            name=\"weights\",\n            initializer=tf.constant_initializer(\n                self.weights_distribution.sample().numpy()\n            ),\n            shape=(self.output_size, self.input_size),\n            trainable=False,\n        )\n\n        if self.use_bias and self.bias_distribution is not None:\n            self.b = self.add_weight(\n                name=\"bias\",\n                initializer=tf.constant_initializer(\n                    self.bias_distribution.sample().numpy()\n                ),\n                shape=(self.output_size,),\n                trainable=False,\n            )\n\n        # Call super-class build method\n        super().build(input_shape)\n\n    def get_config(self) -&gt; dict:\n        \"\"\"\n        Return the configuration dictionary for serialization.\n\n        Returns:\n            dict: Dictionary with the layer configuration.\n        \"\"\"\n\n        # Get the base configuration\n        base_config = super().get_config()\n\n        # Add the custom configurations\n        custom_config = {\n            \"input_size\": self.input_size,\n            \"output_size\": self.output_size,\n        }\n\n        # Combine both configurations\n        return {**base_config, **custom_config}\n\n    def freeze(self) -&gt; None:\n        \"\"\"\n        Freeze the module's parameters to stop gradient computation.\n        If weights or biases are not sampled yet, they are sampled first.\n        Once frozen, parameters are not resampled or updated.\n\n        Returns:\n            None.\n        \"\"\"\n\n        # Set indicator\n        self.frozen = True\n\n        # Sample weights if they are undefined\n        if self.w is None:\n            self.w = self.weights_distribution.sample()\n\n        # Sample bias is they are undefined\n        if self.use_bias and self.b is None and self.bias_distribution is not None:\n            self.b = self.bias_distribution.sample()\n\n        # Stop gradient computation\n        self.w = tf.stop_gradient(self.w)\n        if self.use_bias:\n            self.b = tf.stop_gradient(self.b)\n\n    def kl_cost(self) -&gt; tuple[tf.Tensor, int]:\n        \"\"\"\n        Compute the KL divergence cost for all Bayesian parameters.\n\n        Returns:\n            tuple[tf.Tensor, int]: A tuple containing the KL divergence\n                cost and the total number of parameters in the layer.\n        \"\"\"\n\n        # Compute log probs\n        log_probs: tf.Tensor = self.weights_distribution.log_prob(self.w)\n\n        # Add bias log probs only if using bias\n        if self.use_bias and self.b is not None and self.bias_distribution is not None:\n            log_probs += self.bias_distribution.log_prob(self.b)\n\n        # Compute number of parameters\n        num_params: int = self.weights_distribution.num_params\n        if self.use_bias and self.bias_distribution is not None:\n            num_params += self.bias_distribution.num_params\n\n        return log_probs, num_params\n\n    def call(self, inputs: tf.Tensor) -&gt; tf.Tensor:\n        \"\"\"\n        Performs forward pass using current weights and bias.\n\n        Samples parameters if layer is not frozen. Raises an error if\n        frozen weights are undefined.\n\n        Args:\n            inputs: Input tensor of shape [batch, features].\n\n        Returns:\n            Output tensor after linear transformation.\n\n        Raises:\n            ValueError: If the layer is frozen but weights or bias are\n                undefined.\n        \"\"\"\n\n        # Check if layer is frozen\n        if not self.frozen:\n            self.w = self.weights_distribution.sample()\n\n            # Sample bias only if using bias\n            if self.use_bias and self.bias_distribution is not None:\n                self.b = self.bias_distribution.sample()\n        elif self.w is None or (self.use_bias and self.b is None):\n            raise ValueError(\n                \"Module has been frozen with undefined weights and/or bias.\"\n            )\n\n        # Compute outputs\n        outputs: tf.Tensor = tf.linalg.matmul(inputs, self.w, transpose_b=True)\n\n        # Add bias only if using bias\n        if self.use_bias and self.b is not None:\n            outputs = tf.nn.bias_add(outputs, self.b)\n\n        return outputs\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/nn.html#illia.nn.tf.linear.Linear.__init__","title":"<code>__init__(input_size, output_size, weights_distribution=None, bias_distribution=None, use_bias=True, **kwargs)</code>","text":"<p>Initializes a Bayesian Linear layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>int</code> <p>Number of input features.</p> required <code>output_size</code> <code>int</code> <p>Number of output features.</p> required <code>weights_distribution</code> <code>Optional[GaussianDistribution]</code> <p>Distribution for the weights.</p> <code>None</code> <code>bias_distribution</code> <code>Optional[GaussianDistribution]</code> <p>Distribution for the bias.</p> <code>None</code> <code>use_bias</code> <code>bool</code> <p>Whether to include a bias term.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Extra arguments passed to the base class.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Notes <p>Gaussian distributions are used by default if none are provided.</p> Source code in <code>illia/nn/tf/linear.py</code> <pre><code>def __init__(\n    self,\n    input_size: int,\n    output_size: int,\n    weights_distribution: Optional[GaussianDistribution] = None,\n    bias_distribution: Optional[GaussianDistribution] = None,\n    use_bias: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes a Bayesian Linear layer.\n\n    Args:\n        input_size: Number of input features.\n        output_size: Number of output features.\n        weights_distribution: Distribution for the weights.\n        bias_distribution: Distribution for the bias.\n        use_bias: Whether to include a bias term.\n        **kwargs: Extra arguments passed to the base class.\n\n    Returns:\n        None.\n\n    Notes:\n        Gaussian distributions are used by default if none are\n        provided.\n    \"\"\"\n\n    # Call super-class constructor\n    super().__init__(**kwargs)\n\n    # Set parameters\n    self.input_size = input_size\n    self.output_size = output_size\n    self.use_bias = use_bias\n\n    # Set weights distribution\n    if weights_distribution is None:\n        self.weights_distribution = GaussianDistribution((output_size, input_size))\n    else:\n        self.weights_distribution = weights_distribution\n\n    # Set bias distribution\n    if self.use_bias:\n        if bias_distribution is None:\n            self.bias_distribution = GaussianDistribution((output_size,))\n        else:\n            self.bias_distribution = bias_distribution\n    else:\n        self.bias_distribution = None\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/nn.html#illia.nn.tf.linear.Linear.call","title":"<code>call(inputs)</code>","text":"<p>Performs forward pass using current weights and bias.</p> <p>Samples parameters if layer is not frozen. Raises an error if frozen weights are undefined.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Tensor</code> <p>Input tensor of shape [batch, features].</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor after linear transformation.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the layer is frozen but weights or bias are undefined.</p> Source code in <code>illia/nn/tf/linear.py</code> <pre><code>def call(self, inputs: tf.Tensor) -&gt; tf.Tensor:\n    \"\"\"\n    Performs forward pass using current weights and bias.\n\n    Samples parameters if layer is not frozen. Raises an error if\n    frozen weights are undefined.\n\n    Args:\n        inputs: Input tensor of shape [batch, features].\n\n    Returns:\n        Output tensor after linear transformation.\n\n    Raises:\n        ValueError: If the layer is frozen but weights or bias are\n            undefined.\n    \"\"\"\n\n    # Check if layer is frozen\n    if not self.frozen:\n        self.w = self.weights_distribution.sample()\n\n        # Sample bias only if using bias\n        if self.use_bias and self.bias_distribution is not None:\n            self.b = self.bias_distribution.sample()\n    elif self.w is None or (self.use_bias and self.b is None):\n        raise ValueError(\n            \"Module has been frozen with undefined weights and/or bias.\"\n        )\n\n    # Compute outputs\n    outputs: tf.Tensor = tf.linalg.matmul(inputs, self.w, transpose_b=True)\n\n    # Add bias only if using bias\n    if self.use_bias and self.b is not None:\n        outputs = tf.nn.bias_add(outputs, self.b)\n\n    return outputs\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/nn.html#illia.nn.tf.linear.Linear.freeze","title":"<code>freeze()</code>","text":"<p>Freeze the module's parameters to stop gradient computation. If weights or biases are not sampled yet, they are sampled first. Once frozen, parameters are not resampled or updated.</p> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>illia/nn/tf/linear.py</code> <pre><code>def freeze(self) -&gt; None:\n    \"\"\"\n    Freeze the module's parameters to stop gradient computation.\n    If weights or biases are not sampled yet, they are sampled first.\n    Once frozen, parameters are not resampled or updated.\n\n    Returns:\n        None.\n    \"\"\"\n\n    # Set indicator\n    self.frozen = True\n\n    # Sample weights if they are undefined\n    if self.w is None:\n        self.w = self.weights_distribution.sample()\n\n    # Sample bias is they are undefined\n    if self.use_bias and self.b is None and self.bias_distribution is not None:\n        self.b = self.bias_distribution.sample()\n\n    # Stop gradient computation\n    self.w = tf.stop_gradient(self.w)\n    if self.use_bias:\n        self.b = tf.stop_gradient(self.b)\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/nn.html#illia.nn.tf.linear.Linear.kl_cost","title":"<code>kl_cost()</code>","text":"<p>Compute the KL divergence cost for all Bayesian parameters.</p> <p>Returns:</p> Type Description <code>tuple[Tensor, int]</code> <p>tuple[tf.Tensor, int]: A tuple containing the KL divergence cost and the total number of parameters in the layer.</p> Source code in <code>illia/nn/tf/linear.py</code> <pre><code>def kl_cost(self) -&gt; tuple[tf.Tensor, int]:\n    \"\"\"\n    Compute the KL divergence cost for all Bayesian parameters.\n\n    Returns:\n        tuple[tf.Tensor, int]: A tuple containing the KL divergence\n            cost and the total number of parameters in the layer.\n    \"\"\"\n\n    # Compute log probs\n    log_probs: tf.Tensor = self.weights_distribution.log_prob(self.w)\n\n    # Add bias log probs only if using bias\n    if self.use_bias and self.b is not None and self.bias_distribution is not None:\n        log_probs += self.bias_distribution.log_prob(self.b)\n\n    # Compute number of parameters\n    num_params: int = self.weights_distribution.num_params\n    if self.use_bias and self.bias_distribution is not None:\n        num_params += self.bias_distribution.num_params\n\n    return log_probs, num_params\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/nn.html#illia.nn.tf.lstm.LSTM","title":"<code>LSTM</code>","text":"<p>Bayesian LSTM layer with embedding and probabilistic weights. All weights and biases are sampled from Gaussian distributions. Freezing the layer fixes parameters and stops gradient computation.</p> Source code in <code>illia/nn/tf/lstm.py</code> <pre><code>@saving.register_keras_serializable(package=\"illia\", name=\"LSTM\")\nclass LSTM(BayesianModule):\n    \"\"\"\n    Bayesian LSTM layer with embedding and probabilistic weights.\n    All weights and biases are sampled from Gaussian distributions.\n    Freezing the layer fixes parameters and stops gradient computation.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_embeddings: int,\n        embeddings_dim: int,\n        hidden_size: int,\n        output_size: int,\n        padding_idx: Optional[int] = None,\n        max_norm: Optional[float] = None,\n        norm_type: float = 2.0,\n        scale_grad_by_freq: bool = False,\n        sparse: bool = False,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the Bayesian LSTM layer.\n\n        Args:\n            num_embeddings: Size of the embedding dictionary.\n            embeddings_dim: Dimensionality of each embedding vector.\n            hidden_size: Number of hidden units in the LSTM.\n            output_size: Size of the final output.\n            padding_idx: Index to ignore in embeddings.\n            max_norm: Maximum norm for embedding vectors.\n            norm_type: Norm type used for max_norm.\n            scale_grad_by_freq: Scale gradient by inverse frequency.\n            sparse: Use sparse embedding updates.\n            **kwargs: Extra arguments passed to the base class.\n\n        Returns:\n            None.\n\n        Notes:\n            Gaussian distributions are used by default if none are\n            provided.\n        \"\"\"\n\n        # Call super-class constructor\n        super().__init__(**kwargs)\n\n        # Set attributes\n        self.num_embeddings = num_embeddings\n        self.embeddings_dim = embeddings_dim\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.padding_idx = padding_idx\n        self.max_norm = max_norm\n        self.norm_type = norm_type\n        self.scale_grad_by_freq = scale_grad_by_freq\n        self.sparse = sparse\n\n        # Define the Embedding layer\n        self.embedding = Embedding(\n            num_embeddings=self.num_embeddings,\n            embeddings_dim=self.embeddings_dim,\n            padding_idx=self.padding_idx,\n            max_norm=self.max_norm,\n            norm_type=self.norm_type,\n            scale_grad_by_freq=self.scale_grad_by_freq,\n            sparse=self.sparse,\n        )\n\n        # Initialize weight distributions\n        # Forget gate\n        self.wf_distribution = GaussianDistribution(\n            (self.embeddings_dim + self.hidden_size, self.hidden_size)\n        )\n        self.bf_distribution = GaussianDistribution((self.hidden_size,))\n\n        # Input gate\n        self.wi_distribution = GaussianDistribution(\n            (self.embeddings_dim + self.hidden_size, self.hidden_size)\n        )\n        self.bi_distribution = GaussianDistribution((self.hidden_size,))\n\n        # Candidate gate\n        self.wc_distribution = GaussianDistribution(\n            (self.embeddings_dim + self.hidden_size, self.hidden_size)\n        )\n        self.bc_distribution = GaussianDistribution((self.hidden_size,))\n\n        # Output gate\n        self.wo_distribution = GaussianDistribution(\n            (self.embeddings_dim + self.hidden_size, self.hidden_size)\n        )\n        self.bo_distribution = GaussianDistribution((self.hidden_size,))\n\n        # Final output layer\n        self.wv_distribution = GaussianDistribution(\n            (self.hidden_size, self.output_size)\n        )\n        self.bv_distribution = GaussianDistribution((self.output_size,))\n\n    def build(self, input_shape: tf.TensorShape) -&gt; None:\n        \"\"\"\n        Build trainable and non-trainable parameters.\n\n        Args:\n            input_shape: Input shape used to trigger layer build.\n\n        Returns:\n            None\n        \"\"\"\n\n        # Forget gate weights and bias\n        self.wf = self.add_weight(\n            name=\"forget_gate_weights\",\n            initializer=tf.constant_initializer(self.wf_distribution.sample().numpy()),\n            shape=(self.embeddings_dim + self.hidden_size, self.hidden_size),\n            trainable=False,\n        )\n\n        self.bf = self.add_weight(\n            name=\"forget_gate_bias\",\n            initializer=tf.constant_initializer(self.bf_distribution.sample().numpy()),\n            shape=(self.hidden_size,),\n            trainable=False,\n        )\n\n        # Input gate weights and bias\n        self.wi = self.add_weight(\n            name=\"input_gate_weights\",\n            initializer=tf.constant_initializer(self.wi_distribution.sample().numpy()),\n            shape=(self.embeddings_dim + self.hidden_size, self.hidden_size),\n            trainable=False,\n        )\n\n        self.bi = self.add_weight(\n            name=\"input_gate_bias\",\n            initializer=tf.constant_initializer(self.bi_distribution.sample().numpy()),\n            shape=(self.hidden_size,),\n            trainable=False,\n        )\n\n        # Candidate gate weights and bias\n        self.wc = self.add_weight(\n            name=\"candidate_gate_weights\",\n            initializer=tf.constant_initializer(self.wc_distribution.sample().numpy()),\n            shape=(self.embeddings_dim + self.hidden_size, self.hidden_size),\n            trainable=False,\n        )\n\n        self.bc = self.add_weight(\n            name=\"candidate_gate_bias\",\n            initializer=tf.constant_initializer(self.bc_distribution.sample().numpy()),\n            shape=(self.hidden_size,),\n            trainable=False,\n        )\n\n        # Output gate weights and bias\n        self.wo = self.add_weight(\n            name=\"output_gate_weights\",\n            initializer=tf.constant_initializer(self.wo_distribution.sample().numpy()),\n            shape=(self.embeddings_dim + self.hidden_size, self.hidden_size),\n            trainable=False,\n        )\n\n        self.bo = self.add_weight(\n            name=\"output_gate_bias\",\n            initializer=tf.constant_initializer(self.bo_distribution.sample().numpy()),\n            shape=(self.hidden_size,),\n            trainable=False,\n        )\n\n        # Final output layer weights and bias\n        self.wv = self.add_weight(\n            name=\"final_output_weights\",\n            initializer=tf.constant_initializer(self.wv_distribution.sample().numpy()),\n            shape=(self.hidden_size, self.output_size),\n            trainable=False,\n        )\n\n        self.bv = self.add_weight(\n            name=\"final_output_bias\",\n            initializer=tf.constant_initializer(self.bv_distribution.sample().numpy()),\n            shape=(self.output_size,),\n            trainable=False,\n        )\n\n        # Call super-class build method\n        super().build(input_shape)\n\n    def get_config(self) -&gt; dict:\n        \"\"\"\n        Return the configuration dictionary for serialization.\n\n        Returns:\n            dict: Dictionary with the layer configuration.\n        \"\"\"\n\n        # Get the base configuration\n        base_config = super().get_config()\n\n        # Add the custom configurations\n        custom_config = {\n            \"num_embeddings\": self.num_embeddings,\n            \"embeddings_dim\": self.embeddings_dim,\n            \"hidden_size\": self.hidden_size,\n            \"output_size\": self.output_size,\n            \"padding_idx\": self.padding_idx,\n            \"max_norm\": self.max_norm,\n            \"norm_type\": self.norm_type,\n            \"scale_grad_by_freq\": self.scale_grad_by_freq,\n            \"sparse\": self.sparse,\n        }\n\n        # Combine both configurations\n        return {**base_config, **custom_config}\n\n    def freeze(self) -&gt; None:\n        \"\"\"\n        Freeze the module's parameters to stop gradient computation.\n        If weights or biases are not sampled yet, they are sampled first.\n        Once frozen, parameters are not resampled or updated.\n\n        Returns:\n            None.\n        \"\"\"\n\n        # Set indicator\n        self.frozen = True\n\n        # Freeze embedding layer\n        self.embedding.freeze()\n\n        # Forget gate\n        if self.wf is None:\n            self.wf = self.wf_distribution.sample()\n        if self.bf is None:\n            self.bf = self.bf_distribution.sample()\n        self.wf = tf.stop_gradient(self.wf)\n        self.bf = tf.stop_gradient(self.bf)\n\n        # Input gate\n        if self.wi is None:\n            self.wi = self.wi_distribution.sample()\n        if self.bi is None:\n            self.bi = self.bi_distribution.sample()\n        self.wi = tf.stop_gradient(self.wi)\n        self.bi = tf.stop_gradient(self.bi)\n\n        # Candidate gate\n        if self.wc is None:\n            self.wc = self.wc_distribution.sample()\n        if self.bc is None:\n            self.bc = self.bc_distribution.sample()\n        self.wc = tf.stop_gradient(self.wc)\n        self.bc = tf.stop_gradient(self.bc)\n\n        # Output gate\n        if self.wo is None:\n            self.wo = self.wo_distribution.sample()\n        if self.bo is None:\n            self.bo = self.bo_distribution.sample()\n        self.wo = tf.stop_gradient(self.wo)\n        self.bo = tf.stop_gradient(self.bo)\n\n        # Final output layer\n        if self.wv is None:\n            self.wv = self.wv_distribution.sample()\n        if self.bv is None:\n            self.bv = self.bv_distribution.sample()\n        self.wv = tf.stop_gradient(self.wv)\n        self.bv = tf.stop_gradient(self.bv)\n\n    def kl_cost(self) -&gt; tuple[tf.Tensor, int]:\n        \"\"\"\n        Compute the KL divergence cost for all Bayesian parameters.\n\n        Returns:\n            tuple[tf.Tensor, int]: A tuple containing the KL divergence\n                cost and the total number of parameters in the layer.\n        \"\"\"\n\n        # Compute log probs for each pair of weights and bias\n        log_probs_f = self.wf_distribution.log_prob(\n            self.wf\n        ) + self.bf_distribution.log_prob(self.bf)\n\n        log_probs_i = self.wi_distribution.log_prob(\n            self.wi\n        ) + self.bi_distribution.log_prob(self.bi)\n\n        log_probs_c = self.wc_distribution.log_prob(\n            self.wc\n        ) + self.bc_distribution.log_prob(self.bc)\n\n        log_probs_o = self.wo_distribution.log_prob(\n            self.wo\n        ) + self.bo_distribution.log_prob(self.bo)\n\n        log_probs_v = self.wv_distribution.log_prob(\n            self.wv\n        ) + self.bv_distribution.log_prob(self.bv)\n\n        # Compute the total loss\n        log_probs = log_probs_f + log_probs_i + log_probs_c + log_probs_o + log_probs_v\n\n        # Compute number of parameters\n        num_params = (\n            self.wf_distribution.num_params\n            + self.bf_distribution.num_params\n            + self.wi_distribution.num_params\n            + self.bi_distribution.num_params\n            + self.wc_distribution.num_params\n            + self.bc_distribution.num_params\n            + self.wo_distribution.num_params\n            + self.bo_distribution.num_params\n            + self.wv_distribution.num_params\n            + self.bv_distribution.num_params\n        )\n\n        return log_probs, num_params\n\n    def call(\n        self,\n        inputs: tf.Tensor,\n        init_states: Optional[tuple[tf.Tensor, tf.Tensor]] = None,\n    ) -&gt; tuple[tf.Tensor, tuple[tf.Tensor, tf.Tensor]]:\n        \"\"\"\n        Performs a forward pass through the Bayesian LSTM.\n\n        Args:\n            inputs: Input tensor of token indices. Shape: [batch, seq_len, 1].\n            init_states: Optional tuple of initial (hidden, cell) states.\n\n        Returns:\n            Tuple containing:\n                - Output tensor after final linear transformation.\n                - Tuple of final hidden and cell states.\n\n        Raises:\n            ValueError: If the layer is frozen but weights are\n                undefined.\n        \"\"\"\n\n        # Sample weights if not frozen\n        if not self.frozen:\n            self.wf = self.wf_distribution.sample()\n            self.bf = self.bf_distribution.sample()\n            self.wi = self.wi_distribution.sample()\n            self.bi = self.bi_distribution.sample()\n            self.wc = self.wc_distribution.sample()\n            self.bc = self.bc_distribution.sample()\n            self.wo = self.wo_distribution.sample()\n            self.bo = self.bo_distribution.sample()\n            self.wv = self.wv_distribution.sample()\n            self.bv = self.bv_distribution.sample()\n        elif any(\n            p is None\n            for p in [\n                self.wf,\n                self.bf,\n                self.wi,\n                self.bi,\n                self.wc,\n                self.bc,\n                self.wo,\n                self.bo,\n                self.wv,\n                self.bv,\n            ]\n        ):\n            raise ValueError(\n                \"Module has been frozen with undefined weights and/or bias.\"\n            )\n\n        # Apply embedding layer to input indices\n        inputs = tf.squeeze(inputs, axis=-1)\n        inputs = self.embedding(inputs)\n        batch_size = tf.shape(inputs)[0]\n        seq_len = tf.shape(inputs)[1]\n\n        # Initialize h_t and c_t if init_states is None\n        if init_states is None:\n            h_t = tf.zeros([batch_size, self.hidden_size], dtype=inputs.dtype)\n            c_t = tf.zeros([batch_size, self.hidden_size], dtype=inputs.dtype)\n        else:\n            h_t, c_t = init_states[0], init_states[1]\n\n        # Process sequence\n        for t in range(seq_len):\n            # Shape: (batch_size, embedding_dim)\n            x_t = inputs[:, t, :]\n\n            # Concatenate input and hidden state\n            # Shape: (batch_size, embedding_dim + hidden_size)\n            z_t = tf.concat([x_t, h_t], axis=1)\n\n            # Forget gate\n            ft = tf.sigmoid(tf.matmul(z_t, self.wf) + self.bf)\n\n            # Input gate\n            it = tf.sigmoid(tf.matmul(z_t, self.wi) + self.bi)\n\n            # Candidate cell state\n            can = tf.tanh(tf.matmul(z_t, self.wc) + self.bc)\n\n            # Output gate\n            ot = tf.sigmoid(tf.matmul(z_t, self.wo) + self.bo)\n\n            # Update cell state\n            c_t = c_t * ft + can * it\n\n            # Update hidden state\n            h_t = ot * tf.tanh(c_t)\n\n        # Compute final output\n        y_t = tf.matmul(h_t, self.wv) + self.bv\n\n        return y_t, (h_t, c_t)\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/nn.html#illia.nn.tf.lstm.LSTM.__init__","title":"<code>__init__(num_embeddings, embeddings_dim, hidden_size, output_size, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, **kwargs)</code>","text":"<p>Initializes the Bayesian LSTM layer.</p> <p>Parameters:</p> Name Type Description Default <code>num_embeddings</code> <code>int</code> <p>Size of the embedding dictionary.</p> required <code>embeddings_dim</code> <code>int</code> <p>Dimensionality of each embedding vector.</p> required <code>hidden_size</code> <code>int</code> <p>Number of hidden units in the LSTM.</p> required <code>output_size</code> <code>int</code> <p>Size of the final output.</p> required <code>padding_idx</code> <code>Optional[int]</code> <p>Index to ignore in embeddings.</p> <code>None</code> <code>max_norm</code> <code>Optional[float]</code> <p>Maximum norm for embedding vectors.</p> <code>None</code> <code>norm_type</code> <code>float</code> <p>Norm type used for max_norm.</p> <code>2.0</code> <code>scale_grad_by_freq</code> <code>bool</code> <p>Scale gradient by inverse frequency.</p> <code>False</code> <code>sparse</code> <code>bool</code> <p>Use sparse embedding updates.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Extra arguments passed to the base class.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Notes <p>Gaussian distributions are used by default if none are provided.</p> Source code in <code>illia/nn/tf/lstm.py</code> <pre><code>def __init__(\n    self,\n    num_embeddings: int,\n    embeddings_dim: int,\n    hidden_size: int,\n    output_size: int,\n    padding_idx: Optional[int] = None,\n    max_norm: Optional[float] = None,\n    norm_type: float = 2.0,\n    scale_grad_by_freq: bool = False,\n    sparse: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the Bayesian LSTM layer.\n\n    Args:\n        num_embeddings: Size of the embedding dictionary.\n        embeddings_dim: Dimensionality of each embedding vector.\n        hidden_size: Number of hidden units in the LSTM.\n        output_size: Size of the final output.\n        padding_idx: Index to ignore in embeddings.\n        max_norm: Maximum norm for embedding vectors.\n        norm_type: Norm type used for max_norm.\n        scale_grad_by_freq: Scale gradient by inverse frequency.\n        sparse: Use sparse embedding updates.\n        **kwargs: Extra arguments passed to the base class.\n\n    Returns:\n        None.\n\n    Notes:\n        Gaussian distributions are used by default if none are\n        provided.\n    \"\"\"\n\n    # Call super-class constructor\n    super().__init__(**kwargs)\n\n    # Set attributes\n    self.num_embeddings = num_embeddings\n    self.embeddings_dim = embeddings_dim\n    self.hidden_size = hidden_size\n    self.output_size = output_size\n    self.padding_idx = padding_idx\n    self.max_norm = max_norm\n    self.norm_type = norm_type\n    self.scale_grad_by_freq = scale_grad_by_freq\n    self.sparse = sparse\n\n    # Define the Embedding layer\n    self.embedding = Embedding(\n        num_embeddings=self.num_embeddings,\n        embeddings_dim=self.embeddings_dim,\n        padding_idx=self.padding_idx,\n        max_norm=self.max_norm,\n        norm_type=self.norm_type,\n        scale_grad_by_freq=self.scale_grad_by_freq,\n        sparse=self.sparse,\n    )\n\n    # Initialize weight distributions\n    # Forget gate\n    self.wf_distribution = GaussianDistribution(\n        (self.embeddings_dim + self.hidden_size, self.hidden_size)\n    )\n    self.bf_distribution = GaussianDistribution((self.hidden_size,))\n\n    # Input gate\n    self.wi_distribution = GaussianDistribution(\n        (self.embeddings_dim + self.hidden_size, self.hidden_size)\n    )\n    self.bi_distribution = GaussianDistribution((self.hidden_size,))\n\n    # Candidate gate\n    self.wc_distribution = GaussianDistribution(\n        (self.embeddings_dim + self.hidden_size, self.hidden_size)\n    )\n    self.bc_distribution = GaussianDistribution((self.hidden_size,))\n\n    # Output gate\n    self.wo_distribution = GaussianDistribution(\n        (self.embeddings_dim + self.hidden_size, self.hidden_size)\n    )\n    self.bo_distribution = GaussianDistribution((self.hidden_size,))\n\n    # Final output layer\n    self.wv_distribution = GaussianDistribution(\n        (self.hidden_size, self.output_size)\n    )\n    self.bv_distribution = GaussianDistribution((self.output_size,))\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/nn.html#illia.nn.tf.lstm.LSTM.call","title":"<code>call(inputs, init_states=None)</code>","text":"<p>Performs a forward pass through the Bayesian LSTM.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Tensor</code> <p>Input tensor of token indices. Shape: [batch, seq_len, 1].</p> required <code>init_states</code> <code>Optional[tuple[Tensor, Tensor]]</code> <p>Optional tuple of initial (hidden, cell) states.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Tensor, tuple[Tensor, Tensor]]</code> <p>Tuple containing: - Output tensor after final linear transformation. - Tuple of final hidden and cell states.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the layer is frozen but weights are undefined.</p> Source code in <code>illia/nn/tf/lstm.py</code> <pre><code>def call(\n    self,\n    inputs: tf.Tensor,\n    init_states: Optional[tuple[tf.Tensor, tf.Tensor]] = None,\n) -&gt; tuple[tf.Tensor, tuple[tf.Tensor, tf.Tensor]]:\n    \"\"\"\n    Performs a forward pass through the Bayesian LSTM.\n\n    Args:\n        inputs: Input tensor of token indices. Shape: [batch, seq_len, 1].\n        init_states: Optional tuple of initial (hidden, cell) states.\n\n    Returns:\n        Tuple containing:\n            - Output tensor after final linear transformation.\n            - Tuple of final hidden and cell states.\n\n    Raises:\n        ValueError: If the layer is frozen but weights are\n            undefined.\n    \"\"\"\n\n    # Sample weights if not frozen\n    if not self.frozen:\n        self.wf = self.wf_distribution.sample()\n        self.bf = self.bf_distribution.sample()\n        self.wi = self.wi_distribution.sample()\n        self.bi = self.bi_distribution.sample()\n        self.wc = self.wc_distribution.sample()\n        self.bc = self.bc_distribution.sample()\n        self.wo = self.wo_distribution.sample()\n        self.bo = self.bo_distribution.sample()\n        self.wv = self.wv_distribution.sample()\n        self.bv = self.bv_distribution.sample()\n    elif any(\n        p is None\n        for p in [\n            self.wf,\n            self.bf,\n            self.wi,\n            self.bi,\n            self.wc,\n            self.bc,\n            self.wo,\n            self.bo,\n            self.wv,\n            self.bv,\n        ]\n    ):\n        raise ValueError(\n            \"Module has been frozen with undefined weights and/or bias.\"\n        )\n\n    # Apply embedding layer to input indices\n    inputs = tf.squeeze(inputs, axis=-1)\n    inputs = self.embedding(inputs)\n    batch_size = tf.shape(inputs)[0]\n    seq_len = tf.shape(inputs)[1]\n\n    # Initialize h_t and c_t if init_states is None\n    if init_states is None:\n        h_t = tf.zeros([batch_size, self.hidden_size], dtype=inputs.dtype)\n        c_t = tf.zeros([batch_size, self.hidden_size], dtype=inputs.dtype)\n    else:\n        h_t, c_t = init_states[0], init_states[1]\n\n    # Process sequence\n    for t in range(seq_len):\n        # Shape: (batch_size, embedding_dim)\n        x_t = inputs[:, t, :]\n\n        # Concatenate input and hidden state\n        # Shape: (batch_size, embedding_dim + hidden_size)\n        z_t = tf.concat([x_t, h_t], axis=1)\n\n        # Forget gate\n        ft = tf.sigmoid(tf.matmul(z_t, self.wf) + self.bf)\n\n        # Input gate\n        it = tf.sigmoid(tf.matmul(z_t, self.wi) + self.bi)\n\n        # Candidate cell state\n        can = tf.tanh(tf.matmul(z_t, self.wc) + self.bc)\n\n        # Output gate\n        ot = tf.sigmoid(tf.matmul(z_t, self.wo) + self.bo)\n\n        # Update cell state\n        c_t = c_t * ft + can * it\n\n        # Update hidden state\n        h_t = ot * tf.tanh(c_t)\n\n    # Compute final output\n    y_t = tf.matmul(h_t, self.wv) + self.bv\n\n    return y_t, (h_t, c_t)\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/nn.html#illia.nn.tf.lstm.LSTM.freeze","title":"<code>freeze()</code>","text":"<p>Freeze the module's parameters to stop gradient computation. If weights or biases are not sampled yet, they are sampled first. Once frozen, parameters are not resampled or updated.</p> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>illia/nn/tf/lstm.py</code> <pre><code>def freeze(self) -&gt; None:\n    \"\"\"\n    Freeze the module's parameters to stop gradient computation.\n    If weights or biases are not sampled yet, they are sampled first.\n    Once frozen, parameters are not resampled or updated.\n\n    Returns:\n        None.\n    \"\"\"\n\n    # Set indicator\n    self.frozen = True\n\n    # Freeze embedding layer\n    self.embedding.freeze()\n\n    # Forget gate\n    if self.wf is None:\n        self.wf = self.wf_distribution.sample()\n    if self.bf is None:\n        self.bf = self.bf_distribution.sample()\n    self.wf = tf.stop_gradient(self.wf)\n    self.bf = tf.stop_gradient(self.bf)\n\n    # Input gate\n    if self.wi is None:\n        self.wi = self.wi_distribution.sample()\n    if self.bi is None:\n        self.bi = self.bi_distribution.sample()\n    self.wi = tf.stop_gradient(self.wi)\n    self.bi = tf.stop_gradient(self.bi)\n\n    # Candidate gate\n    if self.wc is None:\n        self.wc = self.wc_distribution.sample()\n    if self.bc is None:\n        self.bc = self.bc_distribution.sample()\n    self.wc = tf.stop_gradient(self.wc)\n    self.bc = tf.stop_gradient(self.bc)\n\n    # Output gate\n    if self.wo is None:\n        self.wo = self.wo_distribution.sample()\n    if self.bo is None:\n        self.bo = self.bo_distribution.sample()\n    self.wo = tf.stop_gradient(self.wo)\n    self.bo = tf.stop_gradient(self.bo)\n\n    # Final output layer\n    if self.wv is None:\n        self.wv = self.wv_distribution.sample()\n    if self.bv is None:\n        self.bv = self.bv_distribution.sample()\n    self.wv = tf.stop_gradient(self.wv)\n    self.bv = tf.stop_gradient(self.bv)\n</code></pre>"},{"location":"api/Deep%20Learning/Tensorflow/nn.html#illia.nn.tf.lstm.LSTM.kl_cost","title":"<code>kl_cost()</code>","text":"<p>Compute the KL divergence cost for all Bayesian parameters.</p> <p>Returns:</p> Type Description <code>tuple[Tensor, int]</code> <p>tuple[tf.Tensor, int]: A tuple containing the KL divergence cost and the total number of parameters in the layer.</p> Source code in <code>illia/nn/tf/lstm.py</code> <pre><code>def kl_cost(self) -&gt; tuple[tf.Tensor, int]:\n    \"\"\"\n    Compute the KL divergence cost for all Bayesian parameters.\n\n    Returns:\n        tuple[tf.Tensor, int]: A tuple containing the KL divergence\n            cost and the total number of parameters in the layer.\n    \"\"\"\n\n    # Compute log probs for each pair of weights and bias\n    log_probs_f = self.wf_distribution.log_prob(\n        self.wf\n    ) + self.bf_distribution.log_prob(self.bf)\n\n    log_probs_i = self.wi_distribution.log_prob(\n        self.wi\n    ) + self.bi_distribution.log_prob(self.bi)\n\n    log_probs_c = self.wc_distribution.log_prob(\n        self.wc\n    ) + self.bc_distribution.log_prob(self.bc)\n\n    log_probs_o = self.wo_distribution.log_prob(\n        self.wo\n    ) + self.bo_distribution.log_prob(self.bo)\n\n    log_probs_v = self.wv_distribution.log_prob(\n        self.wv\n    ) + self.bv_distribution.log_prob(self.bv)\n\n    # Compute the total loss\n    log_probs = log_probs_f + log_probs_i + log_probs_c + log_probs_o + log_probs_v\n\n    # Compute number of parameters\n    num_params = (\n        self.wf_distribution.num_params\n        + self.bf_distribution.num_params\n        + self.wi_distribution.num_params\n        + self.bi_distribution.num_params\n        + self.wc_distribution.num_params\n        + self.bc_distribution.num_params\n        + self.wo_distribution.num_params\n        + self.bo_distribution.num_params\n        + self.wv_distribution.num_params\n        + self.bv_distribution.num_params\n    )\n\n    return log_probs, num_params\n</code></pre>"},{"location":"api/Geometric%20Deep%20Learning/PyG/nn.html","title":"Graph Neural Network Layers","text":""},{"location":"api/Geometric%20Deep%20Learning/PyG/nn.html#illia.nn.pyg.cg_conv.CGConv","title":"<code>CGConv</code>","text":"<p>Crystal Graph Convolutional operator for material property prediction.</p> <p>Updates node features using neighboring nodes and edge features as:</p> <pre><code>x'_i = x_i + sum_{j in N(i)} sigmoid(z_ij W_f + b_f) *\n       softplus(z_ij W_s + b_s)\n</code></pre> <p>where z_ij is the concatenation of central node features, neighbor features, and edge features. Applies element-wise sigmoid and softplus functions.</p> <p>Parameters:</p> Name Type Description Default <code>channels</code> <code>int | tuple[int, int]</code> <p>Size of input features. If tuple, represents source and target feature dimensions.</p> required <code>dim</code> <code>int</code> <p>Dimensionality of edge features.</p> <code>0</code> <code>aggr</code> <code>str</code> <p>Aggregation method (\"add\", \"mean\", \"max\").</p> <code>'add'</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for MessagePassing.</p> <code>{}</code> <p>Returns:</p> Type Description <p>None.</p> Shapes <ul> <li>input: node features (|V|, F) or ((|Vs|, Fs), (|Vt|, Ft)) if   bipartite, edge indices (2, |E|), edge features (|E|, D) optional.</li> <li>output: node features (|V|, F) or (|Vt|, Ft) if bipartite.</li> </ul> Notes <p>Based on \"Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties\" (https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.120.145301)</p> Source code in <code>illia/nn/pyg/cg_conv.py</code> <pre><code>class CGConv(MessagePassing):\n    r\"\"\"\n    Crystal Graph Convolutional operator for material property prediction.\n\n    Updates node features using neighboring nodes and edge features as:\n\n        x'_i = x_i + sum_{j in N(i)} sigmoid(z_ij W_f + b_f) *\n               softplus(z_ij W_s + b_s)\n\n    where z_ij is the concatenation of central node features, neighbor\n    features, and edge features. Applies element-wise sigmoid and\n    softplus functions.\n\n    Args:\n        channels: Size of input features. If tuple, represents source and\n            target feature dimensions.\n        dim: Dimensionality of edge features.\n        aggr: Aggregation method (\"add\", \"mean\", \"max\").\n        **kwargs: Additional arguments for MessagePassing.\n\n    Returns:\n        None.\n\n    Shapes:\n        - input: node features (|V|, F) or ((|Vs|, Fs), (|Vt|, Ft)) if\n          bipartite, edge indices (2, |E|), edge features (|E|, D) optional.\n        - output: node features (|V|, F) or (|Vt|, Ft) if bipartite.\n\n    Notes:\n        Based on \"Crystal Graph Convolutional Neural Networks for an\n        Accurate and Interpretable Prediction of Material Properties\"\n        (https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.120.145301)\n    \"\"\"\n\n    def __init__(\n        self,\n        channels: int | tuple[int, int],\n        dim: int = 0,\n        aggr: str = \"add\",\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the CGConv layer with linear transformations.\n\n        Args:\n            channels: Size of input features. Tuple for source and target.\n            dim: Dimensionality of edge features.\n            aggr: Aggregation operator (\"add\", \"mean\", \"max\").\n            **kwargs: Extra arguments passed to the base class.\n\n        Returns:\n            None.\n        \"\"\"\n\n        # Call super class constructor\n        super().__init__(aggr=aggr, **kwargs)\n\n        # Set attributes\n        self.channels = channels\n        self.dim = dim\n\n        if isinstance(channels, int):\n            channels = (channels, channels)\n\n        # Define linear layers\n        self.lin_f = Linear(sum(channels) + dim, channels[1])\n        self.lin_s = Linear(sum(channels) + dim, channels[1])\n\n    def reset_parameters(self) -&gt; None:\n        \"\"\"\n        Resets parameters of the linear layers and optional batch norm.\n\n        Returns:\n            None.\n        \"\"\"\n\n        self.lin_f.reset_parameters()\n        self.lin_s.reset_parameters()\n        if self.bn is not None:\n            self.bn.reset_parameters()\n\n    def forward(\n        self, x: Tensor | PairTensor, edge_index: Adj, edge_attr: OptTensor = None\n    ) -&gt; Tensor:\n        \"\"\"\n        Performs a forward pass of the convolutional layer.\n\n        Args:\n            x: Input node features, as a single tensor or a pair if bipartite.\n            edge_index: Edge indices.\n            edge_attr: Optional edge features.\n\n        Returns:\n            Node features after applying the convolution.\n        \"\"\"\n\n        if isinstance(x, Tensor):\n            x = (x, x)\n\n        # Propagate_type: (x: PairTensor, edge_attr: OptTensor)\n        out = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=None)\n        out = out + x[1]\n        return out\n\n    def message(self, x_i: Tensor, x_j: Tensor, edge_attr: OptTensor) -&gt; Tensor:\n        \"\"\"\n        Constructs messages passed to neighboring nodes.\n\n        Args:\n            x_i: Central node features.\n            x_j: Neighboring node features.\n            edge_attr: Optional edge features.\n\n        Returns:\n            Aggregated messages for neighbors.\n        \"\"\"\n\n        if edge_attr is None:\n            z = torch.cat([x_i, x_j], dim=-1)\n        else:\n            z = torch.cat([x_i, x_j, edge_attr], dim=-1)\n\n        # pylint: disable=E1102\n        return self.lin_f(z).sigmoid() * F.softplus(input=self.lin_s(z))\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        Returns a string representation of the module.\n\n        Returns:\n            String with class name, channels, and edge feature dimension.\n        \"\"\"\n\n        return f\"{self.__class__.__name__}({self.channels}, dim={self.dim})\"\n</code></pre>"},{"location":"api/Geometric%20Deep%20Learning/PyG/nn.html#illia.nn.pyg.cg_conv.CGConv.__init__","title":"<code>__init__(channels, dim=0, aggr='add', **kwargs)</code>","text":"<p>Initializes the CGConv layer with linear transformations.</p> <p>Parameters:</p> Name Type Description Default <code>channels</code> <code>int | tuple[int, int]</code> <p>Size of input features. Tuple for source and target.</p> required <code>dim</code> <code>int</code> <p>Dimensionality of edge features.</p> <code>0</code> <code>aggr</code> <code>str</code> <p>Aggregation operator (\"add\", \"mean\", \"max\").</p> <code>'add'</code> <code>**kwargs</code> <code>Any</code> <p>Extra arguments passed to the base class.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>illia/nn/pyg/cg_conv.py</code> <pre><code>def __init__(\n    self,\n    channels: int | tuple[int, int],\n    dim: int = 0,\n    aggr: str = \"add\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the CGConv layer with linear transformations.\n\n    Args:\n        channels: Size of input features. Tuple for source and target.\n        dim: Dimensionality of edge features.\n        aggr: Aggregation operator (\"add\", \"mean\", \"max\").\n        **kwargs: Extra arguments passed to the base class.\n\n    Returns:\n        None.\n    \"\"\"\n\n    # Call super class constructor\n    super().__init__(aggr=aggr, **kwargs)\n\n    # Set attributes\n    self.channels = channels\n    self.dim = dim\n\n    if isinstance(channels, int):\n        channels = (channels, channels)\n\n    # Define linear layers\n    self.lin_f = Linear(sum(channels) + dim, channels[1])\n    self.lin_s = Linear(sum(channels) + dim, channels[1])\n</code></pre>"},{"location":"api/Geometric%20Deep%20Learning/PyG/nn.html#illia.nn.pyg.cg_conv.CGConv.__repr__","title":"<code>__repr__()</code>","text":"<p>Returns a string representation of the module.</p> <p>Returns:</p> Type Description <code>str</code> <p>String with class name, channels, and edge feature dimension.</p> Source code in <code>illia/nn/pyg/cg_conv.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    Returns a string representation of the module.\n\n    Returns:\n        String with class name, channels, and edge feature dimension.\n    \"\"\"\n\n    return f\"{self.__class__.__name__}({self.channels}, dim={self.dim})\"\n</code></pre>"},{"location":"api/Geometric%20Deep%20Learning/PyG/nn.html#illia.nn.pyg.cg_conv.CGConv.forward","title":"<code>forward(x, edge_index, edge_attr=None)</code>","text":"<p>Performs a forward pass of the convolutional layer.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor | PairTensor</code> <p>Input node features, as a single tensor or a pair if bipartite.</p> required <code>edge_index</code> <code>Adj</code> <p>Edge indices.</p> required <code>edge_attr</code> <code>OptTensor</code> <p>Optional edge features.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Node features after applying the convolution.</p> Source code in <code>illia/nn/pyg/cg_conv.py</code> <pre><code>def forward(\n    self, x: Tensor | PairTensor, edge_index: Adj, edge_attr: OptTensor = None\n) -&gt; Tensor:\n    \"\"\"\n    Performs a forward pass of the convolutional layer.\n\n    Args:\n        x: Input node features, as a single tensor or a pair if bipartite.\n        edge_index: Edge indices.\n        edge_attr: Optional edge features.\n\n    Returns:\n        Node features after applying the convolution.\n    \"\"\"\n\n    if isinstance(x, Tensor):\n        x = (x, x)\n\n    # Propagate_type: (x: PairTensor, edge_attr: OptTensor)\n    out = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=None)\n    out = out + x[1]\n    return out\n</code></pre>"},{"location":"api/Geometric%20Deep%20Learning/PyG/nn.html#illia.nn.pyg.cg_conv.CGConv.message","title":"<code>message(x_i, x_j, edge_attr)</code>","text":"<p>Constructs messages passed to neighboring nodes.</p> <p>Parameters:</p> Name Type Description Default <code>x_i</code> <code>Tensor</code> <p>Central node features.</p> required <code>x_j</code> <code>Tensor</code> <p>Neighboring node features.</p> required <code>edge_attr</code> <code>OptTensor</code> <p>Optional edge features.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Aggregated messages for neighbors.</p> Source code in <code>illia/nn/pyg/cg_conv.py</code> <pre><code>def message(self, x_i: Tensor, x_j: Tensor, edge_attr: OptTensor) -&gt; Tensor:\n    \"\"\"\n    Constructs messages passed to neighboring nodes.\n\n    Args:\n        x_i: Central node features.\n        x_j: Neighboring node features.\n        edge_attr: Optional edge features.\n\n    Returns:\n        Aggregated messages for neighbors.\n    \"\"\"\n\n    if edge_attr is None:\n        z = torch.cat([x_i, x_j], dim=-1)\n    else:\n        z = torch.cat([x_i, x_j, edge_attr], dim=-1)\n\n    # pylint: disable=E1102\n    return self.lin_f(z).sigmoid() * F.softplus(input=self.lin_s(z))\n</code></pre>"},{"location":"api/Geometric%20Deep%20Learning/PyG/nn.html#illia.nn.pyg.cg_conv.CGConv.reset_parameters","title":"<code>reset_parameters()</code>","text":"<p>Resets parameters of the linear layers and optional batch norm.</p> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>illia/nn/pyg/cg_conv.py</code> <pre><code>def reset_parameters(self) -&gt; None:\n    \"\"\"\n    Resets parameters of the linear layers and optional batch norm.\n\n    Returns:\n        None.\n    \"\"\"\n\n    self.lin_f.reset_parameters()\n    self.lin_s.reset_parameters()\n    if self.bn is not None:\n        self.bn.reset_parameters()\n</code></pre>"},{"location":"examples/Computer%20Vision/MNIST%20Bayesian%20CNN.html","title":"MNIST Bayesian CNN","text":"<p>This code sets up the environment for training a neural network with illia on top of PyTorch. First, it makes sure illia uses the PyTorch backend. Then it imports the necessary illia layers, PyTorch modules for models and optimization, dataset utilities, and Matplotlib for plotting. With this setup, you can define a model, load data, train, and visualize results.</p> In\u00a0[1]: Copied! <pre>import sys\nimport os\n\n# Configure Illia to use PyTorch as backend\nos.environ[\"ILLIA_BACKEND\"] = \"torch\"\n\nimport illia\nfrom illia.nn import Conv2d, Linear\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nimport matplotlib.pyplot as plt\n\nprint(illia.__version__)\nprint(illia.__get_backend__)\n</pre> import sys import os  # Configure Illia to use PyTorch as backend os.environ[\"ILLIA_BACKEND\"] = \"torch\"  import illia from illia.nn import Conv2d, Linear  import numpy as np import torch import torch.nn as nn import torch.optim as optim import torch.nn.functional as F from torch.utils.data import DataLoader from torchvision import datasets, transforms  import matplotlib.pyplot as plt  print(illia.__version__) print(illia.__get_backend__) <pre>0.2.1\ntorch\n</pre> <p>Here we choose whether to run the model on a GPU (CUDA) if available, or fall back to the CPU. This ensures the code adapts automatically to the hardware. We also print the selected device so it\u2019s clear where computations will happen.</p> In\u00a0[2]: Copied! <pre>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n</pre> device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") print(f\"Using device: {device}\") <pre>Using device: cpu\n</pre> <p>Here we prepare the MNIST dataset for training. First, we define transformations to convert images to tensors and normalize them. Then we load the training and test sets, downloading them if needed. Finally, we create data loaders that batch and shuffle the data for efficient training and evaluation.</p> In\u00a0[3]: Copied! <pre># Define preprocessing: convert to tensor and normalize MNIST images\ntransform = transforms.Compose(\n    [\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,)),  # Mean and std of MNIST\n    ]\n)\n\n# Load MNIST datasets (download if not already present)\ntrain_dataset = datasets.MNIST(\"data\", train=True, download=True, transform=transform)\ntest_dataset = datasets.MNIST(\"data\", train=False, transform=transform)\n\n# Create data loaders for training and testing\nbatch_size = 64\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n</pre> # Define preprocessing: convert to tensor and normalize MNIST images transform = transforms.Compose(     [         transforms.ToTensor(),         transforms.Normalize((0.1307,), (0.3081,)),  # Mean and std of MNIST     ] )  # Load MNIST datasets (download if not already present) train_dataset = datasets.MNIST(\"data\", train=True, download=True, transform=transform) test_dataset = datasets.MNIST(\"data\", train=False, transform=transform)  # Create data loaders for training and testing batch_size = 64 train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) <p>This defines a simple Convolutional Neural Network (CNN) for classifying MNIST digits. It has two bayesian convolutional layers, followed by two bayesian fully connected layers. Dropout is used to reduce overfitting. The forward pass applies convolution, ReLU, pooling, flattening, and finally outputs class probabilities with log-softmax. After defining the class, we create the model, move it to the selected device (CPU/GPU), and print the number of parameters.</p> In\u00a0[4]: Copied! <pre># Define the CNN architecture\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        # Convolutional layers\n        self.conv1 = Conv2d(1, 32, kernel_size=3, padding=1)\n        self.conv2 = Conv2d(32, 64, kernel_size=3, padding=1)\n\n        # Fully connected layers\n        self.fc1 = Linear(64 * 7 * 7, 128)\n        self.fc2 = Linear(128, 10)\n\n        # Dropout for regularization\n        self.dropout = nn.Dropout(0.25)\n\n    def forward(self, x):\n        # First conv + ReLU + MaxPool\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2)\n\n        # Second conv + ReLU + MaxPool\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2)\n\n        # Flatten before fully connected layers\n        x = x.view(x.size(0), -1)\n\n        # Fully connected + dropout\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n\n        # Output log-probabilities\n        return F.log_softmax(x, dim=1)\n\n\n# Initialize the model and move to device\nmodel = SimpleCNN().to(device)\nprint(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")\n</pre> # Define the CNN architecture class SimpleCNN(nn.Module):     def __init__(self):         super(SimpleCNN, self).__init__()         # Convolutional layers         self.conv1 = Conv2d(1, 32, kernel_size=3, padding=1)         self.conv2 = Conv2d(32, 64, kernel_size=3, padding=1)          # Fully connected layers         self.fc1 = Linear(64 * 7 * 7, 128)         self.fc2 = Linear(128, 10)          # Dropout for regularization         self.dropout = nn.Dropout(0.25)      def forward(self, x):         # First conv + ReLU + MaxPool         x = F.relu(self.conv1(x))         x = F.max_pool2d(x, 2)          # Second conv + ReLU + MaxPool         x = F.relu(self.conv2(x))         x = F.max_pool2d(x, 2)          # Flatten before fully connected layers         x = x.view(x.size(0), -1)          # Fully connected + dropout         x = F.relu(self.fc1(x))         x = self.dropout(x)         x = self.fc2(x)          # Output log-probabilities         return F.log_softmax(x, dim=1)   # Initialize the model and move to device model = SimpleCNN().to(device) print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\") <pre>Model created with 843284 parameters\n</pre> <p>Now we configure how the model will learn. The Adam optimizer updates the network\u2019s parameters during training with a learning rate of 0.001. For the loss, we use Negative Log Likelihood (NLLLoss), which pairs with the log_softmax output from the model to measure prediction accuracy against true labels.</p> In\u00a0[5]: Copied! <pre>optimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.NLLLoss()\n</pre> optimizer = optim.Adam(model.parameters(), lr=0.001) criterion = nn.NLLLoss() <p>We define two functions: one for training (<code>train_epoch</code>) and one for evaluation (<code>test</code>).</p> <p>During training, the model is set to training mode, gradients are reset, and each batch goes through forward pass, loss calculation, backward pass, and optimizer update. Loss and accuracy are tracked and periodically printed.</p> <p>For evaluation, the model switches to evaluation mode and gradients are disabled for efficiency. Each batch is processed, loss is accumulated, and accuracy is computed on the test set. At the end, average loss and accuracy are reported.</p> In\u00a0[6]: Copied! <pre># Training function for one epoch\ndef train_epoch(model, train_loader, optimizer, criterion, device):\n    model.train()\n    total_loss, correct, total = 0, 0, 0\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n\n        # Reset gradients\n        optimizer.zero_grad()\n\n        # Forward and loss\n        output = model(data)\n        loss = criterion(output, target)\n\n        # Backpropagation and update\n        loss.backward()\n        optimizer.step()\n\n        # Track statistics\n        total_loss += loss.item()\n        pred = output.argmax(dim=1)\n        correct += pred.eq(target).sum().item()\n        total += target.size(0)\n\n        # Print progress every 200 batches\n        if batch_idx % 200 == 0:\n            print(\n                f\"Batch {batch_idx}/{len(train_loader)}, \"\n                f\"Loss: {loss.item():.6f}, \"\n                f\"Accuracy: {100. * correct / total:.2f}%\"\n            )\n\n    return total_loss / len(train_loader), 100.0 * correct / total\n\n\n# Evaluation function\ndef test(model, test_loader, criterion, device):\n    model.eval()\n    test_loss, correct, total = 0, 0, 0\n\n    with torch.no_grad():  # No gradients needed for evaluation\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += criterion(output, target).item()\n\n            pred = output.argmax(dim=1)\n            correct += pred.eq(target).sum().item()\n            total += target.size(0)\n\n    test_loss /= len(test_loader)\n    accuracy = 100.0 * correct / total\n\n    print(f\"\\nTest Loss: {test_loss:.6f}, Test Accuracy: {accuracy:.2f}%\\n\")\n    return test_loss, accuracy\n</pre> # Training function for one epoch def train_epoch(model, train_loader, optimizer, criterion, device):     model.train()     total_loss, correct, total = 0, 0, 0      for batch_idx, (data, target) in enumerate(train_loader):         data, target = data.to(device), target.to(device)          # Reset gradients         optimizer.zero_grad()          # Forward and loss         output = model(data)         loss = criterion(output, target)          # Backpropagation and update         loss.backward()         optimizer.step()          # Track statistics         total_loss += loss.item()         pred = output.argmax(dim=1)         correct += pred.eq(target).sum().item()         total += target.size(0)          # Print progress every 200 batches         if batch_idx % 200 == 0:             print(                 f\"Batch {batch_idx}/{len(train_loader)}, \"                 f\"Loss: {loss.item():.6f}, \"                 f\"Accuracy: {100. * correct / total:.2f}%\"             )      return total_loss / len(train_loader), 100.0 * correct / total   # Evaluation function def test(model, test_loader, criterion, device):     model.eval()     test_loss, correct, total = 0, 0, 0      with torch.no_grad():  # No gradients needed for evaluation         for data, target in test_loader:             data, target = data.to(device), target.to(device)             output = model(data)             test_loss += criterion(output, target).item()              pred = output.argmax(dim=1)             correct += pred.eq(target).sum().item()             total += target.size(0)      test_loss /= len(test_loader)     accuracy = 100.0 * correct / total      print(f\"\\nTest Loss: {test_loss:.6f}, Test Accuracy: {accuracy:.2f}%\\n\")     return test_loss, accuracy <p>We now run the training loop. For each epoch, the model is trained on the training set and then evaluated on the test set. Loss and accuracy are stored for later visualization. Progress is printed after every epoch, and at the end we confirm training is complete. In this example, training runs for 2 epochs.</p> In\u00a0[7]: Copied! <pre># Training setup\nnum_epochs = 2\ntrain_losses, train_accuracies = [], []\ntest_losses, test_accuracies = [], []\n\nprint(\"Starting training...\\n\")\n\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n    print(\"-\" * 50)\n\n    # Train for one epoch\n    train_loss, train_acc = train_epoch(\n        model, train_loader, optimizer, criterion, device\n    )\n    train_losses.append(train_loss)\n    train_accuracies.append(train_acc)\n\n    # Evaluate on test set\n    test_loss, test_acc = test(model, test_loader, criterion, device)\n    test_losses.append(test_loss)\n    test_accuracies.append(test_acc)\n\n    # Report epoch results\n    print(\n        f\"Epoch {epoch + 1} - \"\n        f\"Train Loss: {train_loss:.6f}, Train Acc: {train_acc:.2f}%, \"\n        f\"Test Acc: {test_acc:.2f}%\"\n    )\n\nprint(\"Training completed!\")\n</pre> # Training setup num_epochs = 2 train_losses, train_accuracies = [], [] test_losses, test_accuracies = [], []  print(\"Starting training...\\n\")  for epoch in range(num_epochs):     print(f\"Epoch {epoch + 1}/{num_epochs}\")     print(\"-\" * 50)      # Train for one epoch     train_loss, train_acc = train_epoch(         model, train_loader, optimizer, criterion, device     )     train_losses.append(train_loss)     train_accuracies.append(train_acc)      # Evaluate on test set     test_loss, test_acc = test(model, test_loader, criterion, device)     test_losses.append(test_loss)     test_accuracies.append(test_acc)      # Report epoch results     print(         f\"Epoch {epoch + 1} - \"         f\"Train Loss: {train_loss:.6f}, Train Acc: {train_acc:.2f}%, \"         f\"Test Acc: {test_acc:.2f}%\"     )  print(\"Training completed!\") <pre>Starting training...\n\nEpoch 1/2\n--------------------------------------------------\nBatch 0/938, Loss: 3.542201, Accuracy: 9.38%\nBatch 200/938, Loss: 0.405245, Accuracy: 83.38%\nBatch 400/938, Loss: 0.096597, Accuracy: 89.36%\nBatch 600/938, Loss: 0.062778, Accuracy: 91.73%\nBatch 800/938, Loss: 0.162707, Accuracy: 93.07%\n\nTest Loss: 0.043868, Test Accuracy: 98.52%\n\nEpoch 1 - Train Loss: 0.206314, Train Acc: 93.69%, Test Acc: 98.52%\nEpoch 2/2\n--------------------------------------------------\nBatch 0/938, Loss: 0.052432, Accuracy: 98.44%\nBatch 200/938, Loss: 0.013136, Accuracy: 98.00%\nBatch 400/938, Loss: 0.008689, Accuracy: 98.06%\nBatch 600/938, Loss: 0.035820, Accuracy: 98.03%\nBatch 800/938, Loss: 0.024955, Accuracy: 98.05%\n\nTest Loss: 0.033828, Test Accuracy: 98.91%\n\nEpoch 2 - Train Loss: 0.060348, Train Acc: 98.10%, Test Acc: 98.91%\nTraining completed!\n</pre> <p>After training, we can visualize predictions by passing a batch of test images through the model and showing the true vs. predicted labels. Next, we plot the training and test losses and accuracies over epochs to analyze performance. We then save the trained model to a file for later use and provide a helper function to load it. Finally, the last test accuracy is displayed as a summary of the model\u2019s performance.</p> In\u00a0[8]: Copied! <pre># Function to visualize predictions on test images\ndef visualize_predictions(model, test_loader, device, num_images=8):\n    model.eval()\n    data_iter = iter(test_loader)\n    data, target = next(data_iter)\n    data, target = data.to(device), target.to(device)\n\n    with torch.no_grad():\n        output = model(data)\n        predictions = output.argmax(dim=1)\n\n    fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n    axes = axes.ravel()\n\n    for i in range(num_images):\n        img = data[i].cpu().squeeze()\n        true_label = target[i].cpu().item()\n        pred_label = predictions[i].cpu().item()\n\n        axes[i].imshow(img, cmap=\"gray\")\n        axes[i].set_title(f\"True: {true_label}, Pred: {pred_label}\")\n        axes[i].axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n\n\n# Plot training and test loss/accuracy curves\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.plot(range(1, num_epochs + 1), train_losses, \"b-\", label=\"Train Loss\")\nplt.plot(range(1, num_epochs + 1), test_losses, \"r-\", label=\"Test Loss\")\nplt.title(\"Training Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(range(1, num_epochs + 1), train_accuracies, \"b-\", label=\"Train Accuracy\")\nplt.plot(range(1, num_epochs + 1), test_accuracies, \"r-\", label=\"Test Accuracy\")\nplt.title(\"Training Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy (%)\")\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Show some predictions\nprint(\"Visualizing some predictions...\")\nvisualize_predictions(model, test_loader, device)\n\n# Save the trained model\ntorch.save(model.state_dict(), \"mnist_cnn_model.pth\")\nprint(\"Model saved as 'mnist_cnn_model.pth'\")\n\n\n# Function to load the saved model\ndef load_model():\n    model = SimpleCNN().to(device)\n    model.load_state_dict(torch.load(\"mnist_cnn_model.pth\"))\n    model.eval()\n    return model\n\n\n# Display final test accuracy\nprint(f\"\\nFinal result: {test_accuracies[-1]:.2f}% test accuracy\")\n</pre> # Function to visualize predictions on test images def visualize_predictions(model, test_loader, device, num_images=8):     model.eval()     data_iter = iter(test_loader)     data, target = next(data_iter)     data, target = data.to(device), target.to(device)      with torch.no_grad():         output = model(data)         predictions = output.argmax(dim=1)      fig, axes = plt.subplots(2, 4, figsize=(12, 6))     axes = axes.ravel()      for i in range(num_images):         img = data[i].cpu().squeeze()         true_label = target[i].cpu().item()         pred_label = predictions[i].cpu().item()          axes[i].imshow(img, cmap=\"gray\")         axes[i].set_title(f\"True: {true_label}, Pred: {pred_label}\")         axes[i].axis(\"off\")      plt.tight_layout()     plt.show()   # Plot training and test loss/accuracy curves plt.figure(figsize=(12, 4))  plt.subplot(1, 2, 1) plt.plot(range(1, num_epochs + 1), train_losses, \"b-\", label=\"Train Loss\") plt.plot(range(1, num_epochs + 1), test_losses, \"r-\", label=\"Test Loss\") plt.title(\"Training Loss\") plt.xlabel(\"Epoch\") plt.ylabel(\"Loss\") plt.legend()  plt.subplot(1, 2, 2) plt.plot(range(1, num_epochs + 1), train_accuracies, \"b-\", label=\"Train Accuracy\") plt.plot(range(1, num_epochs + 1), test_accuracies, \"r-\", label=\"Test Accuracy\") plt.title(\"Training Accuracy\") plt.xlabel(\"Epoch\") plt.ylabel(\"Accuracy (%)\") plt.legend()  plt.tight_layout() plt.show()  # Show some predictions print(\"Visualizing some predictions...\") visualize_predictions(model, test_loader, device)  # Save the trained model torch.save(model.state_dict(), \"mnist_cnn_model.pth\") print(\"Model saved as 'mnist_cnn_model.pth'\")   # Function to load the saved model def load_model():     model = SimpleCNN().to(device)     model.load_state_dict(torch.load(\"mnist_cnn_model.pth\"))     model.eval()     return model   # Display final test accuracy print(f\"\\nFinal result: {test_accuracies[-1]:.2f}% test accuracy\") <pre>Visualizing some predictions...\n</pre> <pre>Model saved as 'mnist_cnn_model.pth'\n\nFinal result: 98.91% test accuracy\n</pre> <p>This function loads a previously trained model from a file. It reconstructs the <code>SimpleCNN</code>, loads the saved weights, moves the model to the appropriate device (CPU or GPU), sets it to evaluation mode, and prints a confirmation. This allows you to use the trained model later without retraining.</p> In\u00a0[9]: Copied! <pre># Load a trained model from file\ndef load_trained_model(model_path=\"mnist_cnn_model.pth\"):\n    model = SimpleCNN().to(device)  # Recreate the model and move to device\n    model.load_state_dict(\n        torch.load(model_path, map_location=device)\n    )  # Load saved weights\n    model.eval()  # Set to evaluation mode\n    print(f\"Model loaded from {model_path}\")\n    return model\n</pre> # Load a trained model from file def load_trained_model(model_path=\"mnist_cnn_model.pth\"):     model = SimpleCNN().to(device)  # Recreate the model and move to device     model.load_state_dict(         torch.load(model_path, map_location=device)     )  # Load saved weights     model.eval()  # Set to evaluation mode     print(f\"Model loaded from {model_path}\")     return model <p>This function predicts the digit for a single MNIST image. It ensures the image tensor has the correct shape, moves it to the proper device, passes it through the model in evaluation mode, and returns the predicted class, the confidence of that prediction, and the full probability distribution over all classes. This is useful for testing individual images without using a DataLoader.</p> In\u00a0[10]: Copied! <pre># Predict the digit of a single image\ndef predict_single_image(model, image_tensor):\n    \"\"\"\n    image_tensor: tensor of shape (1, 28, 28) or (28, 28)\n    Returns: predicted class, confidence, and probabilities\n    \"\"\"\n    model.eval()\n\n    # Ensure tensor has shape (1, 1, 28, 28)\n    if image_tensor.dim() == 2:  # (28, 28)\n        image_tensor = image_tensor.unsqueeze(0).unsqueeze(0)\n    elif image_tensor.dim() == 3:  # (1, 28, 28)\n        image_tensor = image_tensor.unsqueeze(0)\n\n    image_tensor = image_tensor.to(device)\n\n    with torch.no_grad():\n        output = model(image_tensor)\n        probabilities = torch.exp(output)  # Convert log_softmax to probabilities\n        prediction = output.argmax(dim=1).item()\n        confidence = probabilities.max().item()\n\n    return prediction, confidence, probabilities.squeeze()\n</pre> # Predict the digit of a single image def predict_single_image(model, image_tensor):     \"\"\"     image_tensor: tensor of shape (1, 28, 28) or (28, 28)     Returns: predicted class, confidence, and probabilities     \"\"\"     model.eval()      # Ensure tensor has shape (1, 1, 28, 28)     if image_tensor.dim() == 2:  # (28, 28)         image_tensor = image_tensor.unsqueeze(0).unsqueeze(0)     elif image_tensor.dim() == 3:  # (1, 28, 28)         image_tensor = image_tensor.unsqueeze(0)      image_tensor = image_tensor.to(device)      with torch.no_grad():         output = model(image_tensor)         probabilities = torch.exp(output)  # Convert log_softmax to probabilities         prediction = output.argmax(dim=1).item()         confidence = probabilities.max().item()      return prediction, confidence, probabilities.squeeze() <p>We take a single batch from the test loader, select one image, and visualize it. Since MNIST images are grayscale, we need to remove the channel dimension before plotting. This approach avoids using iter in a way that could conflict with overwritten names.</p> In\u00a0[11]: Copied! <pre># Get one batch from the test loader\nfor data, target in test_loader:\n    break  # just take the first batch\n\n# Select an example image (index 8)\nimg_example = data[8]  # shape: (1, 28, 28)\n\n# Remove channel dimension for plotting\nimg_to_plot = img_example.squeeze().cpu()  # shape: (28, 28)\n\n# Display the image\nplt.imshow(img_to_plot, cmap=\"gray\")\nplt.title(f\"Label: {target[8].item()}\")\nplt.axis(\"off\")\nplt.show()\n</pre> # Get one batch from the test loader for data, target in test_loader:     break  # just take the first batch  # Select an example image (index 8) img_example = data[8]  # shape: (1, 28, 28)  # Remove channel dimension for plotting img_to_plot = img_example.squeeze().cpu()  # shape: (28, 28)  # Display the image plt.imshow(img_to_plot, cmap=\"gray\") plt.title(f\"Label: {target[8].item()}\") plt.axis(\"off\") plt.show() <p>First, we load the previously trained model from file. Then, we run multiple predictions on a single MNIST image to collect probability distributions. This can be useful for analyzing model confidence or estimating uncertainty. Each prediction returns probabilities over all classes, and stacking them results in a matrix where each row is an iteration and each column is a class probability.</p> In\u00a0[12]: Copied! <pre># 1. Load trained model\nmodel = load_trained_model(\"mnist_cnn_model.pth\")\n\n# 2. Make multiple predictions on a single image\nnum_iter = 100\nall_probabilities = []\n\nfor i in range(num_iter):  # avoid overwriting built-in 'iter'\n    _, _, probabilities = predict_single_image(model, img_example)\n    all_probabilities.append(probabilities)\n\n# Stack results: each row = one iteration, each column = probability for a class\nall_probabilities = np.vstack(all_probabilities)\n</pre> # 1. Load trained model model = load_trained_model(\"mnist_cnn_model.pth\")  # 2. Make multiple predictions on a single image num_iter = 100 all_probabilities = []  for i in range(num_iter):  # avoid overwriting built-in 'iter'     _, _, probabilities = predict_single_image(model, img_example)     all_probabilities.append(probabilities)  # Stack results: each row = one iteration, each column = probability for a class all_probabilities = np.vstack(all_probabilities) <pre>Model loaded from mnist_cnn_model.pth\n</pre> <pre>/tmp/ipykernel_51255/3600316336.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path, map_location=device))  # Load saved weights\n</pre> <p>After collecting multiple predictions, we analyze the probability distributions. We compute the mean probability for each class and the 95% confidence interval using the 2.5th and 97.5th percentiles. A bar plot visualizes the mean probabilities, with error bars representing the confidence intervals. Printing the numerical values helps to inspect the model\u2019s certainty for each class. This approach is useful to understand prediction stability or uncertainty for a single image.</p> In\u00a0[13]: Copied! <pre># Stack predictions (each row = one iteration, each column = class probability)\nall_probabilities = np.vstack(all_probabilities)\n\n# Compute statistics: mean and 95% confidence interval\nmean_probs = np.mean(all_probabilities, axis=0)\nlower_ci = np.percentile(all_probabilities, 2.5, axis=0)  # 2.5th percentile\nupper_ci = np.percentile(all_probabilities, 97.5, axis=0)  # 97.5th percentile\n\n# Plot mean probabilities with error bars\nplt.figure(figsize=(10, 6))\nclasses = range(len(mean_probs))\n\nplt.bar(classes, mean_probs, alpha=0.7, color=\"steelblue\", label=\"Mean\")\n\n# Compute error bars\nerror_lower = np.abs(mean_probs - lower_ci)\nerror_upper = np.abs(upper_ci - mean_probs)\n\nplt.errorbar(\n    classes,\n    mean_probs,\n    yerr=[error_lower, error_upper],\n    fmt=\"none\",\n    color=\"red\",\n    capsize=5,\n    capthick=2,\n    label=\"95% CI\",\n)\n\nplt.xlabel(\"Class\")\nplt.ylabel(\"Probability\")\nplt.title(\"Class Probabilities with 95% Confidence Intervals\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.xticks(classes)\n\n# Print values\nfor i, (mean_val, lower_val, upper_val) in enumerate(\n    zip(mean_probs, lower_ci, upper_ci)\n):\n    print(f\"Class {i}: {mean_val:.3f} [{lower_val:.3f}, {upper_val:.3f}]\")\n\nplt.tight_layout()\nplt.show()\n</pre> # Stack predictions (each row = one iteration, each column = class probability) all_probabilities = np.vstack(all_probabilities)  # Compute statistics: mean and 95% confidence interval mean_probs = np.mean(all_probabilities, axis=0) lower_ci = np.percentile(all_probabilities, 2.5, axis=0)  # 2.5th percentile upper_ci = np.percentile(all_probabilities, 97.5, axis=0)  # 97.5th percentile  # Plot mean probabilities with error bars plt.figure(figsize=(10, 6)) classes = range(len(mean_probs))  plt.bar(classes, mean_probs, alpha=0.7, color=\"steelblue\", label=\"Mean\")  # Compute error bars error_lower = np.abs(mean_probs - lower_ci) error_upper = np.abs(upper_ci - mean_probs)  plt.errorbar(     classes,     mean_probs,     yerr=[error_lower, error_upper],     fmt=\"none\",     color=\"red\",     capsize=5,     capthick=2,     label=\"95% CI\", )  plt.xlabel(\"Class\") plt.ylabel(\"Probability\") plt.title(\"Class Probabilities with 95% Confidence Intervals\") plt.legend() plt.grid(True, alpha=0.3) plt.xticks(classes)  # Print values for i, (mean_val, lower_val, upper_val) in enumerate(     zip(mean_probs, lower_ci, upper_ci) ):     print(f\"Class {i}: {mean_val:.3f} [{lower_val:.3f}, {upper_val:.3f}]\")  plt.tight_layout() plt.show() <pre>Class 0: 0.000 [0.000, 0.000]\nClass 1: 0.000 [0.000, 0.000]\nClass 2: 0.000 [0.000, 0.000]\nClass 3: 0.000 [0.000, 0.000]\nClass 4: 0.000 [0.000, 0.000]\nClass 5: 0.830 [0.812, 0.843]\nClass 6: 0.011 [0.010, 0.012]\nClass 7: 0.000 [0.000, 0.000]\nClass 8: 0.157 [0.144, 0.174]\nClass 9: 0.001 [0.001, 0.002]\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/Computer%20Vision/MNIST%20Bayesian%20CNN.html#libraries","title":"Libraries\u00b6","text":""},{"location":"reference/bayesian_neural_networks.html","title":"Bayesian Neural Networks","text":"<p>Bayesian Neural Networks (BNNs) represent a paradigm that integrates Bayesian inference into deep learning models. Unlike traditional neural networks, where parameters (weights and biases) are fixed values determined through optimization algorithms like backpropagation and gradient descent, BNNs model these parameters as probability distributions. This conceptual shift allows capturing the inherent uncertainty in both the model's parameters and its predictions, offering a more comprehensive understanding of the model's limitations and reliability.</p> <p></p>"},{"location":"reference/bayesian_neural_networks.html#theoretical-foundations-of-bayesian-inference","title":"Theoretical Foundations of Bayesian Inference","text":"<p>Bayesian inference is based on Bayes' Theorem, which provides a mathematical framework for updating beliefs about a model when new observations become available. To understand this concept, it's helpful to consider the process of human learning: initially, we have prior knowledge about a phenomenon, and when we observe new data, we update that knowledge to gain a more accurate understanding.</p> <p>Bayes' Theorem is mathematically expressed as:</p> \\[ P(\\theta | D) = \\frac{P(D | \\theta) P(\\theta)}{P(D)}. \\] <p>This equation can be interpreted as a rule for updating knowledge, where each component represents a specific aspect of the learning process:</p> <ul> <li>\\(P(\\theta)\\) \u2013 Prior Knowledge (Prior Distribution): Represents the initial beliefs   about the model parameters before observing the data. For example, if we want to   predict a person's height, the prior might state that most heights lie between 1.50 and   2.00 meters, with an average around 1.70 meters.</li> <li>\\(P(D | \\theta)\\) \u2013 Data Compatibility (Likelihood): Measures how likely the observed   data are given a specific set of parameters. Continuing the previous example, if the   model parameters suggest an average height of 1.80 meters, the likelihood evaluates how   compatible the observed heights are with that prediction.</li> <li>\\(P(D)\\) \u2013 Normalization (Evidence): Acts as a normalization factor ensuring that the   posterior distribution sums to one (since probabilities must lie between 0 and 1),   satisfying the properties of a valid probability distribution. This term represents the   total probability of observing the data under all possible parameter values.</li> <li>\\(P(\\theta | D)\\) \u2013 Updated Knowledge (Posterior Distribution): This is the final   result of the Bayesian process: the updated beliefs about the parameters after   considering both the prior knowledge and the observed data. The posterior distribution   combines prior information with empirical evidence to provide a more informed estimate   of the parameters.</li> </ul>"},{"location":"reference/bayesian_neural_networks.html#probabilistic-parameter-modeling-in-bnns","title":"Probabilistic Parameter Modeling in BNNs","text":"<p>In a BNN, each weight and bias is represented by a probability distribution, typically a normal distribution with mean 0 and standard deviation 1, denoted as \\(\\mathcal{N}(0, 1)\\). The training process does not aim to estimate a single value for each parameter but rather to adjust the posterior distribution that best explains the observed data.</p> <p>This approach requires parameterizing the distributions through the mean and standard deviation, updating them iteratively during training. The goal is to learn a posterior distribution \\(P(\\theta | D)\\) over the parameters \\(\\theta\\) given the data \\(D\\), where:</p> <ul> <li>The prior distribution \\(P(\\theta)\\) typically assumes a standard Gaussian form,   representing prior knowledge about the parameters.</li> <li>The posterior distribution \\(P(\\theta | D)\\) is adjusted during training and can   differ significantly from the prior, shifting to reflect the knowledge gained from the   data.</li> </ul>"},{"location":"reference/bayesian_neural_networks.html#approximation-methods-for-the-posterior-distribution","title":"Approximation Methods for the Posterior Distribution","text":"<p>Since exact computation of the posterior distribution is computationally intractable in most practical cases, approximate inference techniques are employed:</p> <ul> <li> <p>Variational Inference: Approximates the posterior distribution with a simpler   distribution \\(q(\\theta)\\), optimizing the Kullback-Leibler (KL) divergence between   \\(q(\\theta)\\) and \\(P(\\theta | D)\\). This method offers computational efficiency and   scalability for large models, making it the most common choice in practical   applications.</p> </li> <li> <p>Markov Chain Monte Carlo (MCMC): Sampling-based methods that approximate the   posterior by generating multiple samples. Although computationally more expensive, they   provide more accurate approximations of the posterior and are useful when precision is   prioritized over efficiency.</p> </li> </ul>"},{"location":"reference/bayesian_neural_networks.html#elbo-loss-function","title":"ELBO Loss Function","text":"<p>Optimization in Bayesian Neural Networks is fundamentally based on maximizing the Evidence Lower Bound (ELBO):</p> \\[ \\mathcal{L} = \\mathbb{E}_{q(\\theta)}[\\log P(D | \\theta)] - KL(q(\\theta) || P(\\theta)). \\] <p>This objective function balances two critical components that are essential for Bayesian learning. The first component, known as the likelihood term \\(\\mathbb{E}_{q(\\theta)}[\\log P(D | \\theta)]\\), maximizes the probability of the observed data under the approximate distribution \\(q(\\theta)\\). This component ensures that the model maintains a good fit to the training data by encouraging the approximate posterior to assign high probability to parameter values that explain the observed data well.</p> <p>The second component, referred to as the regularization term \\(KL(q(\\theta) || P(\\theta))\\), minimizes the Kullback-Leibler divergence between the approximate posterior distribution \\(q(\\theta)\\) and the prior distribution \\(P(\\theta)\\). This component acts as a regularizing force that prevents overfitting by maintaining the posterior distribution close to the prior when data is insufficient or ambiguous.</p> <p>The KL divergence is formulated differently depending on the type of distribution. For discrete distributions, the divergence is calculated as:</p> \\[ KL(P || Q) = \\sum_{x} P(x) \\log \\frac{P(x)}{Q(x)}. \\] <p>For continuous distributions, the divergence is expressed as an integral over the parameter space:</p> \\[ KL(P || Q) = \\int_{-\\infty}^{\\infty} p(x) \\log \\frac{p(x)}{q(x)} dx. \\] <p>This duality in formulation allows the Bayesian framework to be applied in both discrete and continuous spaces, providing flexibility in modeling different types of parametric uncertainty. The continuous formulation is particularly relevant for BNNs, where the parameters typically follow continuous distributions such as Gaussians, enabling the framework to capture smooth variations in parameter uncertainty across the continuous parameter space.</p>"},{"location":"reference/bayesian_neural_networks.html#inference-and-uncertainty-quantification","title":"Inference and Uncertainty Quantification","text":"<p>During the inference phase, a BNN generates predictions by repeatedly sampling from the weight distribution. This process typically involves multiple independent inferences (commonly between 50 and 1000 repetitions) for the same input, producing a set of predictions that allows:</p> <ul> <li>Calculating the mean of the predictions as the final estimate.</li> <li>Determining the variance or standard deviation as a quantitative measure of the   associated uncertainty.</li> </ul> <p>This ability to quantify uncertainty is the main advantage of BNNs, providing insight into the reliability of each individual prediction.</p>"},{"location":"reference/bayesian_neural_networks.html#applications-and-comparative-advantages","title":"Applications and Comparative Advantages","text":""},{"location":"reference/bayesian_neural_networks.html#application-domains","title":"Application Domains","text":"<p>BNNs are particularly valuable in contexts where uncertainty quantification is critical:</p> <ul> <li>Biochemistry and drug discovery: Risk and reliability assessment of new molecules.</li> <li>Medical diagnosis: Probabilistic estimation of critical diagnoses where uncertainty   must be explicit.</li> <li>Finance: Risk assessment based on probabilistic predictions.</li> <li>Robotics and reinforcement learning: Adapting to dynamic environments under   uncertainty.</li> <li>Telecommunications: Dynamic adjustment of network parameters considering   environmental variability.</li> </ul>"},{"location":"reference/bayesian_neural_networks.html#advantages-over-deterministic-models","title":"Advantages over Deterministic Models","text":"<p>BNNs offer several advantages over traditional neural networks:</p> <ul> <li>Formal uncertainty quantification: Enables understanding of the model's limitations   on new inputs, providing crucial information for decision-making in critical domains.</li> <li>Effective regularization: Prior distributions and KL divergence terms act as   natural regularization mechanisms, significantly reducing the risk of overfitting.</li> <li>Improved performance with limited data: Prior knowledge acts as a guide when   available data is scarce, improving the model's generalization.</li> <li>Greater interpretability: Facilitates analysis of prediction reliability and   provides additional tools for informed decision-making, especially important in   high-risk applications.</li> </ul>"},{"location":"reference/bayesian_neural_networks.html#integration-with-probabilistic-programming","title":"Integration with Probabilistic Programming","text":"<p>BNNs naturally integrate with probabilistic programming, a paradigm that allows complex statistical models to be described using declarative code. This integration significantly broadens their applicability and facilitates implementation in systems where explicit modeling of uncertainty is essential.</p> <p>The combination provides a unified framework for developing applications that require both the representational power of neural networks and the uncertainty modeling capabilities of Bayesian inference.</p>"},{"location":"reference/bibliography.html","title":"Bibliography","text":"<p>The following references provide foundational and advanced insights into Bayesian Neural Network. These sources have been selected to support the theoretical and methodological concepts for this library.</p> <ul> <li>Weight Uncertainty in Neural Networks</li> <li>Eric J. Ma - An Attempt At Demystifying Bayesian Deep Learning</li> </ul>"}]}